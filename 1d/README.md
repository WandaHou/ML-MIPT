> **AI-Generated Documentation**  
> This README was generated by AI. For questions, clarifications, or detailed information about this project, please contact: **wandahou96@gmail.com**

# 1D Quantum Teleportation Machine Learning Project

A comprehensive quantum machine learning project that implements and compares three distinct approaches for quantum state reconstruction in quantum teleportation protocols. The project evaluates transformer-based machine learning, tensor network methods, and classical post-selection techniques across 1D quantum systems of varying sizes.

## Project Overview

This project develops and benchmarks **three methodologies** for quantum state reconstruction in quantum teleportation protocols:

### 1. GPT (Transformer-Based Machine Learning)
Uses modified Llama architecture to predict quantum density matrices from measurement sequences, leveraging the power of attention mechanisms for quantum state reconstruction.

### 2. MPS/MPO (Matrix Product State/Operator)
Employs tensor network methods with Matrix Product Operators for efficient representation and reconstruction of quantum states in 1D systems.

### 3. Post-Selection (Classical Statistical Method)
Utilizes classical post-processing and statistical averaging based on measurement outcome patterns to reconstruct quantum states.

## Key Features

- **Multi-method comparison**: Direct benchmarking of ML, tensor network, and classical approaches
- **Scalable analysis**: Support for system sizes from N=4 to N=34 qubits
- **Two experimental paradigms**: With and without ancilla qubit assistance
- **Quantum information metrics**: Comprehensive evaluation using Negativity, Coherent Information, and QC Entropy
- **Professional implementation**: Robust training pipelines, checkpointing, and reproducible results
- **Real quantum data**: Integration with quantum hardware/simulation platforms

## Project Structure

```
1d/
├── N=4-16 with ancilla/     # Smaller systems (N=4-16) with ancilla assistance
│   ├── GPT_together/        # Transformer-based ML training
│   ├── MPO/                 # Matrix Product Operator tensor networks
│   ├── data/                # Experimental quantum data
│   ├── simulator.py         # Core quantum simulation utilities
│   ├── utils.py             # Helper functions and data processing
│   └── test_result.ipynb    # Results analysis and evaluation
├── N=4-34 no ancilla/       # Larger systems (N=4-34) without ancilla
│   ├── GPT/                 # Transformer training (alternative setup)
│   ├── GPT_together/        # Joint transformer training
│   ├── MPO/                 # Tensor network methods
│   ├── data/                # Experimental quantum data
│   ├── post_select.ipynb    # Post-selection analysis
│   └── test_result.ipynb    # Results analysis and evaluation
├── 1d-result-figure/        # Comprehensive results and visualizations
│   ├── 1dfigure.ipynb      # Main analysis and plotting
│   └── *_values.pt         # Processed results for each method
└── README.md               # This file
```

## Experimental Paradigms

### With Ancilla (N=4-16)
- **System Size**: 4-16 qubits in 1D chains
- **Ancilla Assistance**: Uses additional qubits for error correction and post-selection
- **Focus**: High-fidelity reconstruction with error mitigation
- **Applications**: Proof-of-principle quantum teleportation with enhanced reliability

### No Ancilla (N=4-34)
- **System Size**: 4-34 qubits in 1D chains  
- **Direct Measurement**: No auxiliary qubits, direct protocol implementation
- **Focus**: Scalability and resource efficiency
- **Applications**: Large-scale quantum teleportation protocols

## Methodologies

### 1. GPT Method (`GPT_together/`, `GPT/`)

**Architecture**: Modified Llama transformer optimized for quantum data
- **Model**: 12-24 layers, 6 attention heads, 24 embedding dimensions
- **Input**: Sequential quantum measurement data from teleportation protocols
- **Output**: 4×4 complex quantum density matrices (two-qubit systems)
- **Training**: Adam optimizer with quantum-specific loss functions

**Key Files**:
- `train-gpt-tel_together.ipynb`: Main training pipeline
- `train-gpt-tel_fine_tune.ipynb`: Fine-tuning procedures
- `Llama2.py`: Custom transformer architecture
- `simulator.py`: Quantum simulation and metrics

**Training Features**:
- Shadow state tomography for supervision
- Batch processing with 500 samples per batch
- 20 epochs with comprehensive logging
- Quantum fidelity-based loss functions

### 2. MPS/MPO Method (`MPO/`)

**Architecture**: Matrix Product Operator tensor networks
- **Bond Dimension**: 10 (configurable)
- **System Support**: Up to N=34 qubits
- **Method**: Variational optimization of MPO representation
- **Training**: Adam optimizer with tensor network constraints

**Key Files**:
- `train-mpo-tel.ipynb`: Main tensor network training
- `mpo.py`: MPO implementation and utilities
- `simulator.py`: Quantum simulation framework

**Features**:
- Efficient 1D quantum state representation
- Scalable to large system sizes
- Inherent quantum structure preservation
- Transfer learning between system sizes

### 3. Post-Selection Method (`post_select.ipynb`)

**Methodology**: Classical statistical reconstruction
- **Approach**: Post-selection based on measurement parity
- **Classification**: Even/odd measurement outcome patterns
- **Reconstruction**: Statistical averaging of density matrices
- **Validation**: Cross-validation across different partitions

**Features**:
- No training required
- Direct statistical analysis
- Baseline comparison for ML methods
- Computationally efficient

## Quantum Metrics

### Negativity
- **Purpose**: Quantifies quantum entanglement
- **Range**: [0, ∞), 0 indicates separable states
- **Implementation**: Partial transpose eigenvalue analysis
- **Usage**: Primary entanglement measure for two-qubit systems

### Coherent Information (I)
- **Purpose**: Measures quantum channel capacity
- **Formula**: I = S(A) - S(AB) (conditional von Neumann entropy)
- **Range**: (-∞, ∞), positive values indicate quantum advantage
- **Usage**: Information-theoretic protocol performance

### QC Entropy (S)
- **Purpose**: Quantum-classical entropy difference
- **Implementation**: von Neumann entropy calculations
- **Usage**: Quantifies quantum information content

## Quick Start

### Prerequisites
```bash
pip install torch>=2.0.0 numpy>=1.21.0 matplotlib>=3.5.0 jupyter>=1.0.0 pandas>=1.3.0
```

### Basic Workflow

#### 1. GPT Training (With Ancilla)
```bash
cd N=4-16\ with\ ancilla/GPT_together/
jupyter notebook train-gpt-tel_together.ipynb
```

#### 2. MPO Training
```bash
cd ../MPO/
jupyter notebook train-mpo-tel.ipynb
```

#### 3. Post-Selection Analysis (No Ancilla)
```bash
cd ../../N=4-34\ no\ ancilla/
jupyter notebook post_select.ipynb
```

#### 4. Results Analysis
```bash
cd ../1d-result-figure/
jupyter notebook 1dfigure.ipynb
```

### Data Access

**Google Drive**: https://drive.google.com/drive/folders/1mW342CtuutjiGhPIPRSAz8-r8XKolCJk?usp=sharing

**Access Request**: Email wandahou96@gmail.com with your Gmail address

## Technical Details

### Quantum Teleportation Protocol
- **System**: 1D qubit chains implementing quantum teleportation
- **Protocol**: Two-qubit teleportation with measurement and reconstruction
- **Measurements**: X, Y, Z basis measurements in sequence
- **Reconstruction**: Density matrix prediction from measurement outcomes

### Machine Learning Architecture (GPT)
- **Base**: Modified Llama transformer
- **Modifications**: Custom output layer for quantum density matrices
- **Training**: Shadow state tomography supervision
- **Loss**: Quantum fidelity-based objective functions

### Tensor Networks (MPS/MPO)
- **Representation**: Matrix Product Operator for 2-qubit reduced states
- **Optimization**: Variational training with gradient descent
- **Scalability**: Efficient scaling to large system sizes
- **Transfer**: Progressive training from smaller to larger systems

### Performance Metrics
- **Reconstruction Fidelity**: Quantum state fidelity measures
- **Entanglement Preservation**: Negativity conservation analysis
- **Information Transfer**: Coherent information evaluation
- **Computational Efficiency**: Training time and resource usage

## Results Summary

The project demonstrates:

1. **GPT Method**: Superior performance on smaller systems (N≤16) with ancilla assistance
2. **MPS/MPO Method**: Excellent scalability to large systems (N≤34) with consistent performance
3. **Post-Selection**: Reliable baseline with computational efficiency
4. **System Size Scaling**: All methods show characteristic scaling behaviors
5. **Ancilla Advantage**: Clear improvement with ancilla-assisted protocols

## Research Applications

### Quantum Communication
- **Quantum Teleportation**: Protocol optimization and fidelity enhancement
- **Quantum Networks**: Scalable state transfer protocols
- **Error Correction**: Integration with quantum error correction schemes

### Machine Learning for Quantum Systems
- **Quantum State Tomography**: ML-enhanced state reconstruction
- **Protocol Optimization**: Data-driven quantum protocol improvement
- **Hybrid Classical-Quantum**: Integration of classical ML with quantum processing

### Tensor Network Methods
- **Efficient Simulation**: Large-scale quantum system simulation
- **Variational Algorithms**: Quantum state preparation and optimization
- **Quantum Many-Body Systems**: Study of 1D quantum systems

## Hardware Requirements

### Minimum
- 8GB GPU memory
- 16GB RAM
- 100GB storage
- CUDA-compatible GPU

### Recommended
- 16GB+ GPU memory
- 32GB+ RAM
- 500GB+ SSD storage
- High-speed internet (for data access)

## Troubleshooting

### Common Issues

**GPU Memory Error**:
```python
# Reduce batch size
batch = 250  # instead of 500

# Use gradient checkpointing
model.gradient_checkpointing_enable()
```

**Training Convergence**:
```python
# Adjust learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)

# Add regularization
loss = primary_loss + 1e-4 * regularization_term
```

**Large System Size**:
```python
# Use transfer learning for MPO
model.load_state_dict(torch.load(f'models/mpo_N={N-2}.pt'))

# Progressive training
for N in range(4, max_N+1, 2):
    train_system_size(N)
```

## File Format and Data Organization

### Training Data Structure
```
data/
├── post_selected/
│   ├── prepseq_train.pt      # Measurement sequences (training)
│   ├── shadow_state_train.pt # Shadow states (training)
│   ├── rhoS_train.pt         # Density matrices (training)
│   └── *_test.pt            # Corresponding test data
└── data_N{size}pa.pickle     # Raw data for system size N (with ancilla)
    data_N{size}na.pickle     # Raw data for system size N (no ancilla)
```

### Model Checkpoints
```
seed{X}/
├── models/
│   ├── gpt_pa.pt            # GPT model (with ancilla)
│   ├── mpo_N={N}_pa.pt      # MPO model for size N (with ancilla)
│   └── mpo_N={N}_na.pt      # MPO model for size N (no ancilla)
└── record/
    └── *_pa.pt              # Training logs and metrics
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/quantum-feature`)
3. Commit your changes (`git commit -m 'Add quantum feature'`)
4. Push to the branch (`git push origin feature/quantum-feature`)
5. Open a Pull Request

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{quantum_teleportation_1d_2025,
    title={1D Quantum Teleportation: Comparing ML, Tensor Networks, and Classical Methods},
    author={[Your Name]},
    year={2025},
    howpublished={\url{https://github.com/[your-repo]/ML-MIPT}}
}
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Quantum computing research community for foundational methods
- PyTorch and tensor network library developers
- Research institutions supporting quantum machine learning research
- Google Quantum AI for hardware access and development tools

---

*This project represents cutting-edge research in quantum teleportation protocols, combining multiple methodological approaches to advance our understanding of quantum state reconstruction and machine learning applications in quantum systems.* 