{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbe55be",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# LLaMA Fine-Tuning for Quantum State Reconstruction\n",
    "\n",
    "This notebook performs fine-tuning of pre-trained LLaMA transformer models for quantum state reconstruction across different system sizes (N=4 to N=34).\n",
    "\n",
    "**Training Configuration:**\n",
    "- **Data Split**: 8/9 training data, 1/9 test data (train_ratio = 8/9)\n",
    "- **Fine-Tuning Approach**: Loads pre-trained models for each system size and performs additional training/refinement\n",
    "- **Sequential Processing**: Processes each system size individually, loading the corresponding pre-trained model\n",
    "- **Metrics**: Quantum coherence (Sqc), negativity (Neg), and Sa measures for performance evaluation\n",
    "\n",
    "**Training Approach Comparison:**\n",
    "- **This notebook (Fine-Tuning)**: Loads pre-trained specialized models for each system size and performs additional refinement training on individual system data.\n",
    "- **Sequential GPT**: Trains models from scratch with transfer learning initialization.\n",
    "- **Unified GPT**: Trains a single model on all system sizes simultaneously.\n",
    "\n",
    "The fine-tuning approach allows for specialized refinement of already-trained models while maintaining the benefits of system-specific optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg, Sa\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49976"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "train, test = False, True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 8/9\n",
    "batch = 500\n",
    "\n",
    "# mdl = LlamaPredictor(L_max=35,\n",
    "#                      n_embd=12, \n",
    "#                      n_layer=6, \n",
    "#                      n_head=6, \n",
    "#                      vocab_size=4, \n",
    "#                      dropout_prob=0.0).to(device)\n",
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     n_embd=24, \n",
    "                     n_layer=12, \n",
    "                     n_head=6, \n",
    "                     vocab_size=4, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "#mdl.load_state_dict(torch.load(f'{file}/models/gpt_na.pt'))\n",
    "    \n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3762 | test Neg: 0.4104\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3720 | test Neg: 0.4133\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4154 | test Neg: 0.4030\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4094 | test Neg: 0.4043\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.4981 | test Neg: 0.3727\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.5023 | test Neg: 0.3720\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | test Sqc: 0.6339 | test Neg: 0.3277\n",
      "epoch:    0 | step:  199 | N:  10 | test Sqc: 0.6330 | test Neg: 0.3280\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  12 | test Sqc: 0.7257 | test Neg: 0.2898\n",
      "epoch:    0 | step:  199 | N:  12 | test Sqc: 0.7160 | test Neg: 0.2953\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  14 | test Sqc: 0.7497 | test Neg: 0.2795\n",
      "epoch:    0 | step:  199 | N:  14 | test Sqc: 0.7440 | test Neg: 0.2824\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  16 | test Sqc: 0.8113 | test Neg: 0.2559\n",
      "epoch:    0 | step:  199 | N:  16 | test Sqc: 0.8178 | test Neg: 0.2521\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  18 | test Sqc: 0.8416 | test Neg: 0.2407\n",
      "epoch:    0 | step:  199 | N:  18 | test Sqc: 0.8485 | test Neg: 0.2367\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  20 | test Sqc: 0.9158 | test Neg: 0.2009\n",
      "epoch:    0 | step:  199 | N:  20 | test Sqc: 0.9250 | test Neg: 0.1977\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  22 | test Sqc: 0.9684 | test Neg: 0.1754\n",
      "epoch:    0 | step:  199 | N:  22 | test Sqc: 0.9648 | test Neg: 0.1756\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  24 | test Sqc: 1.0318 | test Neg: 0.1402\n",
      "epoch:    0 | step:  199 | N:  24 | test Sqc: 1.0267 | test Neg: 0.1429\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  26 | test Sqc: 1.0596 | test Neg: 0.1259\n",
      "epoch:    0 | step:  199 | N:  26 | test Sqc: 1.0598 | test Neg: 0.1245\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  28 | test Sqc: 1.0985 | test Neg: 0.1017\n",
      "epoch:    0 | step:  199 | N:  28 | test Sqc: 1.0919 | test Neg: 0.1028\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  30 | test Sqc: 1.1168 | test Neg: 0.0844\n",
      "epoch:    0 | step:  199 | N:  30 | test Sqc: 1.1109 | test Neg: 0.0873\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  32 | test Sqc: 1.1550 | test Neg: 0.0581\n",
      "epoch:    0 | step:  199 | N:  32 | test Sqc: 1.1524 | test Neg: 0.0607\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  34 | test Sqc: 1.1699 | test Neg: 0.0391\n",
      "epoch:    0 | step:  199 | N:  34 | test Sqc: 1.1649 | test Neg: 0.0440\n"
     ]
    }
   ],
   "source": [
    "for N in range(4,36,2):\n",
    "    # if N > 4:\n",
    "    #     mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N-2}_na.pt'))\n",
    "    mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N}_na.pt'))\n",
    "    #mdl.load_state_dict(torch.load(f'{file}/models/gpt_na.pt'))\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/data_{N}na.pickle', shuffle=True)\n",
    "    prepseq = torch.cat([prepseq+2, torch.zeros(prepseq.shape[0], 32-prepseq.shape[1], dtype=prepseq.dtype), torch.ones(prepseq.shape[0], 1, dtype=prepseq.dtype)], 1)\n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "    \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, 33)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, 33)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'test Sa':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        # Train\n",
    "        if train:\n",
    "            print('='*50+'   Train   '+'='*50)\n",
    "            mdl.train()\n",
    "            for i in range(prepseq_train.shape[0]):\n",
    "                rhoC = mdl(prepseq_train[i])\n",
    "                l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "                optimizer.zero_grad()\n",
    "                probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "                loss = -probs.log().mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                l['loss'].append(loss.item())\n",
    "                if (i+1)%100 == 0:\n",
    "                    trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                    loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                    print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Sa'].append(Sa(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/gpt_N={N}_na.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/gpt_N={N}_na.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db531145-d589-4085-a3c8-3834e14b27a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
