{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6692"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "test = True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 8/9\n",
    "batch = 500\n",
    "\n",
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     L=4,\n",
    "                     n_embd=12, \n",
    "                     n_layer=6, \n",
    "                     n_head=6, \n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464552c-cd95-419d-ba59-67774ddfd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in range(4,36,2):\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/data_{N}na.pickle', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | train Sqc: 0.7487 | loss: 1.2846\n",
      "epoch:    0 | step:  199 | N:  4 | train Sqc: 0.6008 | loss: 1.2586\n",
      "epoch:    0 | step:  299 | N:  4 | train Sqc: 0.5432 | loss: 1.2494\n",
      "epoch:    0 | step:  399 | N:  4 | train Sqc: 0.5060 | loss: 1.2440\n",
      "epoch:    0 | step:  499 | N:  4 | train Sqc: 0.4850 | loss: 1.2408\n",
      "epoch:    0 | step:  599 | N:  4 | train Sqc: 0.4709 | loss: 1.2388\n",
      "epoch:    0 | step:  699 | N:  4 | train Sqc: 0.4579 | loss: 1.2371\n",
      "epoch:    0 | step:  799 | N:  4 | train Sqc: 0.4451 | loss: 1.2356\n",
      "epoch:    0 | step:  899 | N:  4 | train Sqc: 0.4356 | loss: 1.2344\n",
      "epoch:    0 | step:  999 | N:  4 | train Sqc: 0.4289 | loss: 1.2335\n",
      "epoch:    0 | step:  1099 | N:  4 | train Sqc: 0.4241 | loss: 1.2330\n",
      "epoch:    0 | step:  1199 | N:  4 | train Sqc: 0.4179 | loss: 1.2323\n",
      "epoch:    0 | step:  1299 | N:  4 | train Sqc: 0.4126 | loss: 1.2318\n",
      "epoch:    0 | step:  1399 | N:  4 | train Sqc: 0.4114 | loss: 1.2316\n",
      "epoch:    0 | step:  1499 | N:  4 | train Sqc: 0.4079 | loss: 1.2313\n",
      "epoch:    0 | step:  1599 | N:  4 | train Sqc: 0.4036 | loss: 1.2310\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3618 | test Neg: 0.4195\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3740 | test Neg: 0.4136\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | train Sqc: 0.3740 | loss: 1.2270\n",
      "epoch:    1 | step:  199 | N:  4 | train Sqc: 0.3733 | loss: 1.2277\n",
      "epoch:    1 | step:  299 | N:  4 | train Sqc: 0.3756 | loss: 1.2280\n",
      "epoch:    1 | step:  399 | N:  4 | train Sqc: 0.3715 | loss: 1.2275\n",
      "epoch:    1 | step:  499 | N:  4 | train Sqc: 0.3706 | loss: 1.2273\n",
      "epoch:    1 | step:  599 | N:  4 | train Sqc: 0.3719 | loss: 1.2274\n",
      "epoch:    1 | step:  699 | N:  4 | train Sqc: 0.3715 | loss: 1.2272\n",
      "epoch:    1 | step:  799 | N:  4 | train Sqc: 0.3681 | loss: 1.2268\n",
      "epoch:    1 | step:  899 | N:  4 | train Sqc: 0.3655 | loss: 1.2265\n",
      "epoch:    1 | step:  999 | N:  4 | train Sqc: 0.3638 | loss: 1.2263\n",
      "epoch:    1 | step:  1099 | N:  4 | train Sqc: 0.3644 | loss: 1.2264\n",
      "epoch:    1 | step:  1199 | N:  4 | train Sqc: 0.3624 | loss: 1.2263\n",
      "epoch:    1 | step:  1299 | N:  4 | train Sqc: 0.3611 | loss: 1.2262\n",
      "epoch:    1 | step:  1399 | N:  4 | train Sqc: 0.3632 | loss: 1.2264\n",
      "epoch:    1 | step:  1499 | N:  4 | train Sqc: 0.3625 | loss: 1.2264\n",
      "epoch:    1 | step:  1599 | N:  4 | train Sqc: 0.3608 | loss: 1.2263\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3576 | test Neg: 0.4200\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3699 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | train Sqc: 0.3712 | loss: 1.2266\n",
      "epoch:    2 | step:  199 | N:  4 | train Sqc: 0.3704 | loss: 1.2273\n",
      "epoch:    2 | step:  299 | N:  4 | train Sqc: 0.4336 | loss: 1.2350\n",
      "epoch:    2 | step:  399 | N:  4 | train Sqc: 0.4196 | loss: 1.2332\n",
      "epoch:    2 | step:  499 | N:  4 | train Sqc: 0.4097 | loss: 1.2319\n",
      "epoch:    2 | step:  599 | N:  4 | train Sqc: 0.4049 | loss: 1.2311\n",
      "epoch:    2 | step:  699 | N:  4 | train Sqc: 0.3995 | loss: 1.2304\n",
      "epoch:    2 | step:  799 | N:  4 | train Sqc: 0.3926 | loss: 1.2296\n",
      "epoch:    2 | step:  899 | N:  4 | train Sqc: 0.3867 | loss: 1.2289\n",
      "epoch:    2 | step:  999 | N:  4 | train Sqc: 0.3820 | loss: 1.2285\n",
      "epoch:    2 | step:  1099 | N:  4 | train Sqc: 0.3806 | loss: 1.2284\n",
      "epoch:    2 | step:  1199 | N:  4 | train Sqc: 0.3772 | loss: 1.2280\n",
      "epoch:    2 | step:  1299 | N:  4 | train Sqc: 0.3747 | loss: 1.2278\n",
      "epoch:    2 | step:  1399 | N:  4 | train Sqc: 0.3761 | loss: 1.2278\n",
      "epoch:    2 | step:  1499 | N:  4 | train Sqc: 0.3745 | loss: 1.2278\n",
      "epoch:    2 | step:  1599 | N:  4 | train Sqc: 0.3720 | loss: 1.2276\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3571 | test Neg: 0.4201\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3684 | test Neg: 0.4142\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | train Sqc: 0.3688 | loss: 1.2264\n",
      "epoch:    3 | step:  199 | N:  4 | train Sqc: 0.3673 | loss: 1.2272\n",
      "epoch:    3 | step:  299 | N:  4 | train Sqc: 0.3690 | loss: 1.2275\n",
      "epoch:    3 | step:  399 | N:  4 | train Sqc: 0.3656 | loss: 1.2271\n",
      "epoch:    3 | step:  499 | N:  4 | train Sqc: 0.3648 | loss: 1.2269\n",
      "epoch:    3 | step:  599 | N:  4 | train Sqc: 0.3666 | loss: 1.2270\n",
      "epoch:    3 | step:  699 | N:  4 | train Sqc: 0.3662 | loss: 1.2268\n",
      "epoch:    3 | step:  799 | N:  4 | train Sqc: 0.3637 | loss: 1.2264\n",
      "epoch:    3 | step:  899 | N:  4 | train Sqc: 0.3609 | loss: 1.2261\n",
      "epoch:    3 | step:  999 | N:  4 | train Sqc: 0.3589 | loss: 1.2259\n",
      "epoch:    3 | step:  1099 | N:  4 | train Sqc: 0.3596 | loss: 1.2260\n",
      "epoch:    3 | step:  1199 | N:  4 | train Sqc: 0.3580 | loss: 1.2259\n",
      "epoch:    3 | step:  1299 | N:  4 | train Sqc: 0.3569 | loss: 1.2258\n",
      "epoch:    3 | step:  1399 | N:  4 | train Sqc: 0.3592 | loss: 1.2260\n",
      "epoch:    3 | step:  1499 | N:  4 | train Sqc: 0.3586 | loss: 1.2260\n",
      "epoch:    3 | step:  1599 | N:  4 | train Sqc: 0.3569 | loss: 1.2260\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | test Sqc: 0.3557 | test Neg: 0.4202\n",
      "epoch:    3 | step:  199 | N:  4 | test Sqc: 0.3675 | test Neg: 0.4143\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | train Sqc: 0.3688 | loss: 1.2264\n",
      "epoch:    4 | step:  199 | N:  4 | train Sqc: 0.3679 | loss: 1.2271\n",
      "epoch:    4 | step:  299 | N:  4 | train Sqc: 0.3689 | loss: 1.2275\n",
      "epoch:    4 | step:  399 | N:  4 | train Sqc: 0.3654 | loss: 1.2270\n",
      "epoch:    4 | step:  499 | N:  4 | train Sqc: 0.3642 | loss: 1.2268\n",
      "epoch:    4 | step:  599 | N:  4 | train Sqc: 0.3660 | loss: 1.2269\n",
      "epoch:    4 | step:  699 | N:  4 | train Sqc: 0.3657 | loss: 1.2267\n",
      "epoch:    4 | step:  799 | N:  4 | train Sqc: 0.3631 | loss: 1.2263\n",
      "epoch:    4 | step:  899 | N:  4 | train Sqc: 0.3605 | loss: 1.2260\n",
      "epoch:    4 | step:  999 | N:  4 | train Sqc: 0.3585 | loss: 1.2259\n",
      "epoch:    4 | step:  1099 | N:  4 | train Sqc: 0.3592 | loss: 1.2260\n",
      "epoch:    4 | step:  1199 | N:  4 | train Sqc: 0.3576 | loss: 1.2258\n",
      "epoch:    4 | step:  1299 | N:  4 | train Sqc: 0.3564 | loss: 1.2257\n",
      "epoch:    4 | step:  1399 | N:  4 | train Sqc: 0.3586 | loss: 1.2259\n",
      "epoch:    4 | step:  1499 | N:  4 | train Sqc: 0.3580 | loss: 1.2260\n",
      "epoch:    4 | step:  1599 | N:  4 | train Sqc: 0.3563 | loss: 1.2259\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | test Sqc: 0.3554 | test Neg: 0.4202\n",
      "epoch:    4 | step:  199 | N:  4 | test Sqc: 0.3671 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | train Sqc: 0.3685 | loss: 1.2263\n",
      "epoch:    5 | step:  199 | N:  4 | train Sqc: 0.3679 | loss: 1.2270\n",
      "epoch:    5 | step:  299 | N:  4 | train Sqc: 0.3686 | loss: 1.2274\n",
      "epoch:    5 | step:  399 | N:  4 | train Sqc: 0.3650 | loss: 1.2269\n",
      "epoch:    5 | step:  499 | N:  4 | train Sqc: 0.3637 | loss: 1.2267\n",
      "epoch:    5 | step:  599 | N:  4 | train Sqc: 0.3654 | loss: 1.2268\n",
      "epoch:    5 | step:  699 | N:  4 | train Sqc: 0.3651 | loss: 1.2266\n",
      "epoch:    5 | step:  799 | N:  4 | train Sqc: 0.3626 | loss: 1.2263\n",
      "epoch:    5 | step:  899 | N:  4 | train Sqc: 0.3600 | loss: 1.2260\n",
      "epoch:    5 | step:  999 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:    5 | step:  1099 | N:  4 | train Sqc: 0.3589 | loss: 1.2259\n",
      "epoch:    5 | step:  1199 | N:  4 | train Sqc: 0.3573 | loss: 1.2258\n",
      "epoch:    5 | step:  1299 | N:  4 | train Sqc: 0.3561 | loss: 1.2257\n",
      "epoch:    5 | step:  1399 | N:  4 | train Sqc: 0.3583 | loss: 1.2259\n",
      "epoch:    5 | step:  1499 | N:  4 | train Sqc: 0.3577 | loss: 1.2259\n",
      "epoch:    5 | step:  1599 | N:  4 | train Sqc: 0.3559 | loss: 1.2258\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | test Sqc: 0.3549 | test Neg: 0.4203\n",
      "epoch:    5 | step:  199 | N:  4 | test Sqc: 0.3667 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | train Sqc: 0.3679 | loss: 1.2262\n",
      "epoch:    6 | step:  199 | N:  4 | train Sqc: 0.3674 | loss: 1.2270\n",
      "epoch:    6 | step:  299 | N:  4 | train Sqc: 0.3682 | loss: 1.2273\n",
      "epoch:    6 | step:  399 | N:  4 | train Sqc: 0.3645 | loss: 1.2269\n",
      "epoch:    6 | step:  499 | N:  4 | train Sqc: 0.3633 | loss: 1.2267\n",
      "epoch:    6 | step:  599 | N:  4 | train Sqc: 0.3650 | loss: 1.2268\n",
      "epoch:    6 | step:  699 | N:  4 | train Sqc: 0.3646 | loss: 1.2266\n",
      "epoch:    6 | step:  799 | N:  4 | train Sqc: 0.3621 | loss: 1.2262\n",
      "epoch:    6 | step:  899 | N:  4 | train Sqc: 0.3596 | loss: 1.2259\n",
      "epoch:    6 | step:  999 | N:  4 | train Sqc: 0.3577 | loss: 1.2257\n",
      "epoch:    6 | step:  1099 | N:  4 | train Sqc: 0.3584 | loss: 1.2258\n",
      "epoch:    6 | step:  1199 | N:  4 | train Sqc: 0.3568 | loss: 1.2257\n",
      "epoch:    6 | step:  1299 | N:  4 | train Sqc: 0.3558 | loss: 1.2256\n",
      "epoch:    6 | step:  1399 | N:  4 | train Sqc: 0.3579 | loss: 1.2258\n",
      "epoch:    6 | step:  1499 | N:  4 | train Sqc: 0.3573 | loss: 1.2259\n",
      "epoch:    6 | step:  1599 | N:  4 | train Sqc: 0.3555 | loss: 1.2258\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | test Sqc: 0.3549 | test Neg: 0.4203\n",
      "epoch:    6 | step:  199 | N:  4 | test Sqc: 0.3667 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | train Sqc: 0.3673 | loss: 1.2262\n",
      "epoch:    7 | step:  199 | N:  4 | train Sqc: 0.3670 | loss: 1.2269\n",
      "epoch:    7 | step:  299 | N:  4 | train Sqc: 0.3676 | loss: 1.2273\n",
      "epoch:    7 | step:  399 | N:  4 | train Sqc: 0.3641 | loss: 1.2268\n",
      "epoch:    7 | step:  499 | N:  4 | train Sqc: 0.3627 | loss: 1.2266\n",
      "epoch:    7 | step:  599 | N:  4 | train Sqc: 0.3645 | loss: 1.2267\n",
      "epoch:    7 | step:  699 | N:  4 | train Sqc: 0.3642 | loss: 1.2266\n",
      "epoch:    7 | step:  799 | N:  4 | train Sqc: 0.3617 | loss: 1.2262\n",
      "epoch:    7 | step:  899 | N:  4 | train Sqc: 0.3591 | loss: 1.2259\n",
      "epoch:    7 | step:  999 | N:  4 | train Sqc: 0.3574 | loss: 1.2257\n",
      "epoch:    7 | step:  1099 | N:  4 | train Sqc: 0.3581 | loss: 1.2258\n",
      "epoch:    7 | step:  1199 | N:  4 | train Sqc: 0.3567 | loss: 1.2257\n",
      "epoch:    7 | step:  1299 | N:  4 | train Sqc: 0.3556 | loss: 1.2256\n",
      "epoch:    7 | step:  1399 | N:  4 | train Sqc: 0.3577 | loss: 1.2258\n",
      "epoch:    7 | step:  1499 | N:  4 | train Sqc: 0.3570 | loss: 1.2258\n",
      "epoch:    7 | step:  1599 | N:  4 | train Sqc: 0.3553 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | test Sqc: 0.3546 | test Neg: 0.4203\n",
      "epoch:    7 | step:  199 | N:  4 | test Sqc: 0.3664 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | train Sqc: 0.3665 | loss: 1.2262\n",
      "epoch:    8 | step:  199 | N:  4 | train Sqc: 0.3661 | loss: 1.2269\n",
      "epoch:    8 | step:  299 | N:  4 | train Sqc: 0.3668 | loss: 1.2272\n",
      "epoch:    8 | step:  399 | N:  4 | train Sqc: 0.3635 | loss: 1.2268\n",
      "epoch:    8 | step:  499 | N:  4 | train Sqc: 0.3622 | loss: 1.2266\n",
      "epoch:    8 | step:  599 | N:  4 | train Sqc: 0.3640 | loss: 1.2267\n",
      "epoch:    8 | step:  699 | N:  4 | train Sqc: 0.3637 | loss: 1.2265\n",
      "epoch:    8 | step:  799 | N:  4 | train Sqc: 0.3612 | loss: 1.2261\n",
      "epoch:    8 | step:  899 | N:  4 | train Sqc: 0.3587 | loss: 1.2258\n",
      "epoch:    8 | step:  999 | N:  4 | train Sqc: 0.3570 | loss: 1.2257\n",
      "epoch:    8 | step:  1099 | N:  4 | train Sqc: 0.3577 | loss: 1.2258\n",
      "epoch:    8 | step:  1199 | N:  4 | train Sqc: 0.3563 | loss: 1.2256\n",
      "epoch:    8 | step:  1299 | N:  4 | train Sqc: 0.3552 | loss: 1.2255\n",
      "epoch:    8 | step:  1399 | N:  4 | train Sqc: 0.3575 | loss: 1.2257\n",
      "epoch:    8 | step:  1499 | N:  4 | train Sqc: 0.3567 | loss: 1.2258\n",
      "epoch:    8 | step:  1599 | N:  4 | train Sqc: 0.3551 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | test Sqc: 0.3555 | test Neg: 0.4203\n",
      "epoch:    8 | step:  199 | N:  4 | test Sqc: 0.3662 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | train Sqc: 0.3661 | loss: 1.2261\n",
      "epoch:    9 | step:  199 | N:  4 | train Sqc: 0.3654 | loss: 1.2269\n",
      "epoch:    9 | step:  299 | N:  4 | train Sqc: 0.3661 | loss: 1.2272\n",
      "epoch:    9 | step:  399 | N:  4 | train Sqc: 0.3630 | loss: 1.2268\n",
      "epoch:    9 | step:  499 | N:  4 | train Sqc: 0.3616 | loss: 1.2266\n",
      "epoch:    9 | step:  599 | N:  4 | train Sqc: 0.3635 | loss: 1.2267\n",
      "epoch:    9 | step:  699 | N:  4 | train Sqc: 0.3632 | loss: 1.2265\n",
      "epoch:    9 | step:  799 | N:  4 | train Sqc: 0.3608 | loss: 1.2261\n",
      "epoch:    9 | step:  899 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:    9 | step:  999 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:    9 | step:  1099 | N:  4 | train Sqc: 0.3573 | loss: 1.2257\n",
      "epoch:    9 | step:  1199 | N:  4 | train Sqc: 0.3559 | loss: 1.2256\n",
      "epoch:    9 | step:  1299 | N:  4 | train Sqc: 0.3549 | loss: 1.2255\n",
      "epoch:    9 | step:  1399 | N:  4 | train Sqc: 0.3569 | loss: 1.2257\n",
      "epoch:    9 | step:  1499 | N:  4 | train Sqc: 0.3562 | loss: 1.2257\n",
      "epoch:    9 | step:  1599 | N:  4 | train Sqc: 0.3545 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | test Sqc: 0.3545 | test Neg: 0.4203\n",
      "epoch:    9 | step:  199 | N:  4 | test Sqc: 0.3660 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   10 | step:   99 | N:  4 | train Sqc: 0.3659 | loss: 1.2261\n",
      "epoch:   10 | step:  199 | N:  4 | train Sqc: 0.3658 | loss: 1.2268\n",
      "epoch:   10 | step:  299 | N:  4 | train Sqc: 0.3664 | loss: 1.2272\n",
      "epoch:   10 | step:  399 | N:  4 | train Sqc: 0.3631 | loss: 1.2267\n",
      "epoch:   10 | step:  499 | N:  4 | train Sqc: 0.3618 | loss: 1.2265\n",
      "epoch:   10 | step:  599 | N:  4 | train Sqc: 0.3636 | loss: 1.2266\n",
      "epoch:   10 | step:  699 | N:  4 | train Sqc: 0.3633 | loss: 1.2265\n",
      "epoch:   10 | step:  799 | N:  4 | train Sqc: 0.3608 | loss: 1.2261\n",
      "epoch:   10 | step:  899 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:   10 | step:  999 | N:  4 | train Sqc: 0.3567 | loss: 1.2256\n",
      "epoch:   10 | step:  1099 | N:  4 | train Sqc: 0.3574 | loss: 1.2257\n",
      "epoch:   10 | step:  1199 | N:  4 | train Sqc: 0.3560 | loss: 1.2256\n",
      "epoch:   10 | step:  1299 | N:  4 | train Sqc: 0.3550 | loss: 1.2255\n",
      "epoch:   10 | step:  1399 | N:  4 | train Sqc: 0.3569 | loss: 1.2257\n",
      "epoch:   10 | step:  1499 | N:  4 | train Sqc: 0.3563 | loss: 1.2257\n",
      "epoch:   10 | step:  1599 | N:  4 | train Sqc: 0.3545 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   10 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4203\n",
      "epoch:   10 | step:  199 | N:  4 | test Sqc: 0.3661 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   11 | step:   99 | N:  4 | train Sqc: 0.3652 | loss: 1.2261\n",
      "epoch:   11 | step:  199 | N:  4 | train Sqc: 0.3653 | loss: 1.2268\n",
      "epoch:   11 | step:  299 | N:  4 | train Sqc: 0.3659 | loss: 1.2272\n",
      "epoch:   11 | step:  399 | N:  4 | train Sqc: 0.3628 | loss: 1.2267\n",
      "epoch:   11 | step:  499 | N:  4 | train Sqc: 0.3616 | loss: 1.2265\n",
      "epoch:   11 | step:  599 | N:  4 | train Sqc: 0.3634 | loss: 1.2266\n",
      "epoch:   11 | step:  699 | N:  4 | train Sqc: 0.3631 | loss: 1.2264\n",
      "epoch:   11 | step:  799 | N:  4 | train Sqc: 0.3606 | loss: 1.2261\n",
      "epoch:   11 | step:  899 | N:  4 | train Sqc: 0.3581 | loss: 1.2258\n",
      "epoch:   11 | step:  999 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:   11 | step:  1099 | N:  4 | train Sqc: 0.3572 | loss: 1.2257\n",
      "epoch:   11 | step:  1199 | N:  4 | train Sqc: 0.3558 | loss: 1.2256\n",
      "epoch:   11 | step:  1299 | N:  4 | train Sqc: 0.3548 | loss: 1.2255\n",
      "epoch:   11 | step:  1399 | N:  4 | train Sqc: 0.3567 | loss: 1.2257\n",
      "epoch:   11 | step:  1499 | N:  4 | train Sqc: 0.3561 | loss: 1.2257\n",
      "epoch:   11 | step:  1599 | N:  4 | train Sqc: 0.3543 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   11 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4203\n",
      "epoch:   11 | step:  199 | N:  4 | test Sqc: 0.3661 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   12 | step:   99 | N:  4 | train Sqc: 0.3650 | loss: 1.2260\n",
      "epoch:   12 | step:  199 | N:  4 | train Sqc: 0.3650 | loss: 1.2268\n",
      "epoch:   12 | step:  299 | N:  4 | train Sqc: 0.3656 | loss: 1.2271\n",
      "epoch:   12 | step:  399 | N:  4 | train Sqc: 0.3626 | loss: 1.2267\n",
      "epoch:   12 | step:  499 | N:  4 | train Sqc: 0.3613 | loss: 1.2265\n",
      "epoch:   12 | step:  599 | N:  4 | train Sqc: 0.3632 | loss: 1.2266\n",
      "epoch:   12 | step:  699 | N:  4 | train Sqc: 0.3629 | loss: 1.2264\n",
      "epoch:   12 | step:  799 | N:  4 | train Sqc: 0.3604 | loss: 1.2261\n",
      "epoch:   12 | step:  899 | N:  4 | train Sqc: 0.3578 | loss: 1.2257\n",
      "epoch:   12 | step:  999 | N:  4 | train Sqc: 0.3563 | loss: 1.2256\n",
      "epoch:   12 | step:  1099 | N:  4 | train Sqc: 0.3570 | loss: 1.2257\n",
      "epoch:   12 | step:  1199 | N:  4 | train Sqc: 0.3556 | loss: 1.2255\n",
      "epoch:   12 | step:  1299 | N:  4 | train Sqc: 0.3546 | loss: 1.2254\n",
      "epoch:   12 | step:  1399 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:   12 | step:  1499 | N:  4 | train Sqc: 0.3559 | loss: 1.2257\n",
      "epoch:   12 | step:  1599 | N:  4 | train Sqc: 0.3542 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   12 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4204\n",
      "epoch:   12 | step:  199 | N:  4 | test Sqc: 0.3660 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   13 | step:   99 | N:  4 | train Sqc: 0.3647 | loss: 1.2260\n",
      "epoch:   13 | step:  199 | N:  4 | train Sqc: 0.3647 | loss: 1.2268\n"
     ]
    }
   ],
   "source": [
    "for N in range(4,36,2):\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/data_{N}na.pickle', shuffle=True)\n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "    \n",
    "    prepseq = torch.cat([prepseq+1, torch.zeros(prepseq.shape[0],1).to(prepseq.dtype).to(device)], -1)\n",
    "    \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, N-1)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, N-1)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    mdl = LlamaPredictor(L_max=35,\n",
    "                     L=prepseq_train.shape[1]-1,\n",
    "                     n_embd=12, \n",
    "                     n_layer=6, \n",
    "                     n_head=6, \n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "    # load new model\n",
    "    if N > 4:\n",
    "        mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N-2}_na.pt'))\n",
    "    # load old model\n",
    "    # mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N}_na.pt'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        # Train\n",
    "        print('='*50+'   Train   '+'='*50)\n",
    "        mdl.train()\n",
    "        for i in range(prepseq_train.shape[0]):\n",
    "            rhoC = mdl(prepseq_train[i])\n",
    "            l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "            optimizer.zero_grad()\n",
    "            probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "            loss = -probs.log().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l['loss'].append(loss.item())\n",
    "            if (i+1)%100 == 0:\n",
    "                trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/gpt_N={N}_na.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/gpt_N={N}_na.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd304a4-53cc-469a-a5d0-1e79de9dcabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
