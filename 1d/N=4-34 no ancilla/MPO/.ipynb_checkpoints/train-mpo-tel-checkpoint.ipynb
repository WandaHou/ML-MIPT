{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg\n",
    "from mpo import tel_mpo\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "test = True\n",
    "file = \"save0\"\n",
    "train_ratio = 8/9\n",
    "batch = 500\n",
    "\n",
    "mdl = tel_mpo(34, bond=10)\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | train Sqc: 1.6889 | loss: 1.4971\n",
      "epoch:    0 | step:  199 | N:  6 | train Sqc: 1.4267 | loss: 1.4321\n",
      "epoch:    0 | step:  299 | N:  6 | train Sqc: 1.2534 | loss: 1.3965\n",
      "epoch:    0 | step:  399 | N:  6 | train Sqc: 1.1219 | loss: 1.3709\n",
      "epoch:    0 | step:  499 | N:  6 | train Sqc: 1.0140 | loss: 1.3502\n",
      "epoch:    0 | step:  599 | N:  6 | train Sqc: 0.9274 | loss: 1.3335\n",
      "epoch:    0 | step:  699 | N:  6 | train Sqc: 0.8607 | loss: 1.3205\n",
      "epoch:    0 | step:  799 | N:  6 | train Sqc: 0.8078 | loss: 1.3105\n",
      "epoch:    0 | step:  899 | N:  6 | train Sqc: 0.7659 | loss: 1.3025\n",
      "epoch:    0 | step:  999 | N:  6 | train Sqc: 0.7298 | loss: 1.2958\n",
      "epoch:    0 | step:  1099 | N:  6 | train Sqc: 0.7028 | loss: 1.2905\n",
      "epoch:    0 | step:  1199 | N:  6 | train Sqc: 0.6796 | loss: 1.2858\n",
      "epoch:    0 | step:  1299 | N:  6 | train Sqc: 0.6574 | loss: 1.2817\n",
      "epoch:    0 | step:  1399 | N:  6 | train Sqc: 0.6418 | loss: 1.2785\n",
      "epoch:    0 | step:  1499 | N:  6 | train Sqc: 0.6286 | loss: 1.2758\n",
      "epoch:    0 | step:  1599 | N:  6 | train Sqc: 0.6163 | loss: 1.2733\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4257 | test Neg: 0.4026\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4184 | test Neg: 0.4039\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | train Sqc: 0.4295 | loss: 1.2351\n",
      "epoch:    1 | step:  199 | N:  6 | train Sqc: 0.4153 | loss: 1.2349\n",
      "epoch:    1 | step:  299 | N:  6 | train Sqc: 0.4086 | loss: 1.2338\n",
      "epoch:    1 | step:  399 | N:  6 | train Sqc: 0.4115 | loss: 1.2342\n",
      "epoch:    1 | step:  499 | N:  6 | train Sqc: 0.4092 | loss: 1.2339\n",
      "epoch:    1 | step:  599 | N:  6 | train Sqc: 0.4074 | loss: 1.2338\n",
      "epoch:    1 | step:  699 | N:  6 | train Sqc: 0.4074 | loss: 1.2340\n",
      "epoch:    1 | step:  799 | N:  6 | train Sqc: 0.4078 | loss: 1.2344\n",
      "epoch:    1 | step:  899 | N:  6 | train Sqc: 0.4089 | loss: 1.2345\n",
      "epoch:    1 | step:  999 | N:  6 | train Sqc: 0.4071 | loss: 1.2346\n",
      "epoch:    1 | step:  1099 | N:  6 | train Sqc: 0.4086 | loss: 1.2347\n",
      "epoch:    1 | step:  1199 | N:  6 | train Sqc: 0.4093 | loss: 1.2347\n",
      "epoch:    1 | step:  1299 | N:  6 | train Sqc: 0.4074 | loss: 1.2344\n",
      "epoch:    1 | step:  1399 | N:  6 | train Sqc: 0.4094 | loss: 1.2346\n",
      "epoch:    1 | step:  1499 | N:  6 | train Sqc: 0.4114 | loss: 1.2348\n",
      "epoch:    1 | step:  1599 | N:  6 | train Sqc: 0.4125 | loss: 1.2349\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | test Sqc: 0.4213 | test Neg: 0.4028\n",
      "epoch:    1 | step:  199 | N:  6 | test Sqc: 0.4138 | test Neg: 0.4041\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | train Sqc: 0.4267 | loss: 1.2348\n",
      "epoch:    2 | step:  199 | N:  6 | train Sqc: 0.4132 | loss: 1.2347\n",
      "epoch:    2 | step:  299 | N:  6 | train Sqc: 0.4061 | loss: 1.2336\n",
      "epoch:    2 | step:  399 | N:  6 | train Sqc: 0.4084 | loss: 1.2340\n",
      "epoch:    2 | step:  499 | N:  6 | train Sqc: 0.4062 | loss: 1.2337\n",
      "epoch:    2 | step:  599 | N:  6 | train Sqc: 0.4046 | loss: 1.2336\n",
      "epoch:    2 | step:  699 | N:  6 | train Sqc: 0.4044 | loss: 1.2338\n",
      "epoch:    2 | step:  799 | N:  6 | train Sqc: 0.4050 | loss: 1.2342\n",
      "epoch:    2 | step:  899 | N:  6 | train Sqc: 0.4062 | loss: 1.2344\n",
      "epoch:    2 | step:  999 | N:  6 | train Sqc: 0.4045 | loss: 1.2344\n",
      "epoch:    2 | step:  1099 | N:  6 | train Sqc: 0.4061 | loss: 1.2345\n",
      "epoch:    2 | step:  1199 | N:  6 | train Sqc: 0.4068 | loss: 1.2345\n",
      "epoch:    2 | step:  1299 | N:  6 | train Sqc: 0.4050 | loss: 1.2343\n",
      "epoch:    2 | step:  1399 | N:  6 | train Sqc: 0.4071 | loss: 1.2344\n",
      "epoch:    2 | step:  1499 | N:  6 | train Sqc: 0.4091 | loss: 1.2347\n",
      "epoch:    2 | step:  1599 | N:  6 | train Sqc: 0.4103 | loss: 1.2348\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | test Sqc: 0.4194 | test Neg: 0.4029\n",
      "epoch:    2 | step:  199 | N:  6 | test Sqc: 0.4119 | test Neg: 0.4042\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  6 | train Sqc: 0.4255 | loss: 1.2347\n",
      "epoch:    3 | step:  199 | N:  6 | train Sqc: 0.4123 | loss: 1.2347\n",
      "epoch:    3 | step:  299 | N:  6 | train Sqc: 0.4050 | loss: 1.2335\n",
      "epoch:    3 | step:  399 | N:  6 | train Sqc: 0.4070 | loss: 1.2339\n",
      "epoch:    3 | step:  499 | N:  6 | train Sqc: 0.4048 | loss: 1.2337\n",
      "epoch:    3 | step:  599 | N:  6 | train Sqc: 0.4033 | loss: 1.2336\n",
      "epoch:    3 | step:  699 | N:  6 | train Sqc: 0.4030 | loss: 1.2338\n",
      "epoch:    3 | step:  799 | N:  6 | train Sqc: 0.4037 | loss: 1.2341\n",
      "epoch:    3 | step:  899 | N:  6 | train Sqc: 0.4050 | loss: 1.2343\n",
      "epoch:    3 | step:  999 | N:  6 | train Sqc: 0.4034 | loss: 1.2343\n",
      "epoch:    3 | step:  1099 | N:  6 | train Sqc: 0.4050 | loss: 1.2345\n",
      "epoch:    3 | step:  1199 | N:  6 | train Sqc: 0.4057 | loss: 1.2345\n",
      "epoch:    3 | step:  1299 | N:  6 | train Sqc: 0.4039 | loss: 1.2342\n",
      "epoch:    3 | step:  1399 | N:  6 | train Sqc: 0.4060 | loss: 1.2344\n",
      "epoch:    3 | step:  1499 | N:  6 | train Sqc: 0.4080 | loss: 1.2346\n",
      "epoch:    3 | step:  1599 | N:  6 | train Sqc: 0.4092 | loss: 1.2347\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  6 | test Sqc: 0.4181 | test Neg: 0.4030\n",
      "epoch:    3 | step:  199 | N:  6 | test Sqc: 0.4107 | test Neg: 0.4042\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  6 | train Sqc: 0.4244 | loss: 1.2347\n",
      "epoch:    4 | step:  199 | N:  6 | train Sqc: 0.4116 | loss: 1.2346\n",
      "epoch:    4 | step:  299 | N:  6 | train Sqc: 0.4042 | loss: 1.2334\n",
      "epoch:    4 | step:  399 | N:  6 | train Sqc: 0.4060 | loss: 1.2339\n",
      "epoch:    4 | step:  499 | N:  6 | train Sqc: 0.4038 | loss: 1.2336\n",
      "epoch:    4 | step:  599 | N:  6 | train Sqc: 0.4024 | loss: 1.2335\n",
      "epoch:    4 | step:  699 | N:  6 | train Sqc: 0.4021 | loss: 1.2337\n",
      "epoch:    4 | step:  799 | N:  6 | train Sqc: 0.4029 | loss: 1.2341\n",
      "epoch:    4 | step:  899 | N:  6 | train Sqc: 0.4042 | loss: 1.2343\n",
      "epoch:    4 | step:  999 | N:  6 | train Sqc: 0.4026 | loss: 1.2343\n",
      "epoch:    4 | step:  1099 | N:  6 | train Sqc: 0.4042 | loss: 1.2344\n",
      "epoch:    4 | step:  1199 | N:  6 | train Sqc: 0.4049 | loss: 1.2344\n",
      "epoch:    4 | step:  1299 | N:  6 | train Sqc: 0.4031 | loss: 1.2342\n",
      "epoch:    4 | step:  1399 | N:  6 | train Sqc: 0.4053 | loss: 1.2343\n",
      "epoch:    4 | step:  1499 | N:  6 | train Sqc: 0.4073 | loss: 1.2346\n",
      "epoch:    4 | step:  1599 | N:  6 | train Sqc: 0.4085 | loss: 1.2347\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  6 | test Sqc: 0.4171 | test Neg: 0.4031\n",
      "epoch:    4 | step:  199 | N:  6 | test Sqc: 0.4098 | test Neg: 0.4043\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | train Sqc: 1.6648 | loss: 1.4759\n",
      "epoch:    0 | step:  199 | N:  8 | train Sqc: 1.4290 | loss: 1.4226\n",
      "epoch:    0 | step:  299 | N:  8 | train Sqc: 1.2849 | loss: 1.3943\n",
      "epoch:    0 | step:  399 | N:  8 | train Sqc: 1.1773 | loss: 1.3741\n",
      "epoch:    0 | step:  499 | N:  8 | train Sqc: 1.0832 | loss: 1.3570\n",
      "epoch:    0 | step:  599 | N:  8 | train Sqc: 1.0029 | loss: 1.3426\n",
      "epoch:    0 | step:  699 | N:  8 | train Sqc: 0.9433 | loss: 1.3314\n",
      "epoch:    0 | step:  799 | N:  8 | train Sqc: 0.8937 | loss: 1.3223\n",
      "epoch:    0 | step:  899 | N:  8 | train Sqc: 0.8572 | loss: 1.3154\n",
      "epoch:    0 | step:  999 | N:  8 | train Sqc: 0.8233 | loss: 1.3093\n",
      "epoch:    0 | step:  1099 | N:  8 | train Sqc: 0.7961 | loss: 1.3041\n",
      "epoch:    0 | step:  1199 | N:  8 | train Sqc: 0.7749 | loss: 1.3000\n",
      "epoch:    0 | step:  1299 | N:  8 | train Sqc: 0.7565 | loss: 1.2967\n",
      "epoch:    0 | step:  1399 | N:  8 | train Sqc: 0.7395 | loss: 1.2934\n",
      "epoch:    0 | step:  1499 | N:  8 | train Sqc: 0.7247 | loss: 1.2907\n",
      "epoch:    0 | step:  1599 | N:  8 | train Sqc: 0.7129 | loss: 1.2885\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.5155 | test Neg: 0.3713\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.5183 | test Neg: 0.3714\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | train Sqc: 0.5568 | loss: 1.2571\n",
      "epoch:    1 | step:  199 | N:  8 | train Sqc: 0.5428 | loss: 1.2564\n",
      "epoch:    1 | step:  299 | N:  8 | train Sqc: 0.5343 | loss: 1.2554\n",
      "epoch:    1 | step:  399 | N:  8 | train Sqc: 0.5366 | loss: 1.2556\n",
      "epoch:    1 | step:  499 | N:  8 | train Sqc: 0.5323 | loss: 1.2550\n",
      "epoch:    1 | step:  599 | N:  8 | train Sqc: 0.5267 | loss: 1.2544\n",
      "epoch:    1 | step:  699 | N:  8 | train Sqc: 0.5272 | loss: 1.2546\n",
      "epoch:    1 | step:  799 | N:  8 | train Sqc: 0.5254 | loss: 1.2544\n",
      "epoch:    1 | step:  899 | N:  8 | train Sqc: 0.5267 | loss: 1.2547\n",
      "epoch:    1 | step:  999 | N:  8 | train Sqc: 0.5234 | loss: 1.2544\n",
      "epoch:    1 | step:  1099 | N:  8 | train Sqc: 0.5219 | loss: 1.2541\n",
      "epoch:    1 | step:  1199 | N:  8 | train Sqc: 0.5224 | loss: 1.2541\n",
      "epoch:    1 | step:  1299 | N:  8 | train Sqc: 0.5225 | loss: 1.2543\n",
      "epoch:    1 | step:  1399 | N:  8 | train Sqc: 0.5216 | loss: 1.2540\n",
      "epoch:    1 | step:  1499 | N:  8 | train Sqc: 0.5210 | loss: 1.2538\n",
      "epoch:    1 | step:  1599 | N:  8 | train Sqc: 0.5215 | loss: 1.2539\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | test Sqc: 0.5085 | test Neg: 0.3719\n",
      "epoch:    1 | step:  199 | N:  8 | test Sqc: 0.5127 | test Neg: 0.3719\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | train Sqc: 0.5518 | loss: 1.2566\n",
      "epoch:    2 | step:  199 | N:  8 | train Sqc: 0.5378 | loss: 1.2559\n",
      "epoch:    2 | step:  299 | N:  8 | train Sqc: 0.5297 | loss: 1.2550\n",
      "epoch:    2 | step:  399 | N:  8 | train Sqc: 0.5319 | loss: 1.2551\n",
      "epoch:    2 | step:  499 | N:  8 | train Sqc: 0.5278 | loss: 1.2546\n",
      "epoch:    2 | step:  599 | N:  8 | train Sqc: 0.5227 | loss: 1.2540\n",
      "epoch:    2 | step:  699 | N:  8 | train Sqc: 0.5234 | loss: 1.2542\n",
      "epoch:    2 | step:  799 | N:  8 | train Sqc: 0.5216 | loss: 1.2540\n",
      "epoch:    2 | step:  899 | N:  8 | train Sqc: 0.5229 | loss: 1.2544\n",
      "epoch:    2 | step:  999 | N:  8 | train Sqc: 0.5197 | loss: 1.2541\n",
      "epoch:    2 | step:  1099 | N:  8 | train Sqc: 0.5181 | loss: 1.2538\n",
      "epoch:    2 | step:  1199 | N:  8 | train Sqc: 0.5186 | loss: 1.2538\n",
      "epoch:    2 | step:  1299 | N:  8 | train Sqc: 0.5188 | loss: 1.2539\n",
      "epoch:    2 | step:  1399 | N:  8 | train Sqc: 0.5181 | loss: 1.2537\n",
      "epoch:    2 | step:  1499 | N:  8 | train Sqc: 0.5176 | loss: 1.2535\n",
      "epoch:    2 | step:  1599 | N:  8 | train Sqc: 0.5182 | loss: 1.2536\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | test Sqc: 0.5070 | test Neg: 0.3721\n",
      "epoch:    2 | step:  199 | N:  8 | test Sqc: 0.5115 | test Neg: 0.3720\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  8 | train Sqc: 0.5499 | loss: 1.2565\n",
      "epoch:    3 | step:  199 | N:  8 | train Sqc: 0.5363 | loss: 1.2558\n",
      "epoch:    3 | step:  299 | N:  8 | train Sqc: 0.5283 | loss: 1.2548\n",
      "epoch:    3 | step:  399 | N:  8 | train Sqc: 0.5304 | loss: 1.2550\n",
      "epoch:    3 | step:  499 | N:  8 | train Sqc: 0.5262 | loss: 1.2544\n",
      "epoch:    3 | step:  599 | N:  8 | train Sqc: 0.5214 | loss: 1.2538\n",
      "epoch:    3 | step:  699 | N:  8 | train Sqc: 0.5221 | loss: 1.2540\n",
      "epoch:    3 | step:  799 | N:  8 | train Sqc: 0.5202 | loss: 1.2539\n",
      "epoch:    3 | step:  899 | N:  8 | train Sqc: 0.5215 | loss: 1.2542\n",
      "epoch:    3 | step:  999 | N:  8 | train Sqc: 0.5183 | loss: 1.2539\n",
      "epoch:    3 | step:  1099 | N:  8 | train Sqc: 0.5166 | loss: 1.2536\n",
      "epoch:    3 | step:  1199 | N:  8 | train Sqc: 0.5171 | loss: 1.2536\n",
      "epoch:    3 | step:  1299 | N:  8 | train Sqc: 0.5174 | loss: 1.2538\n",
      "epoch:    3 | step:  1399 | N:  8 | train Sqc: 0.5167 | loss: 1.2535\n",
      "epoch:    3 | step:  1499 | N:  8 | train Sqc: 0.5162 | loss: 1.2534\n",
      "epoch:    3 | step:  1599 | N:  8 | train Sqc: 0.5168 | loss: 1.2534\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  8 | test Sqc: 0.5064 | test Neg: 0.3721\n",
      "epoch:    3 | step:  199 | N:  8 | test Sqc: 0.5109 | test Neg: 0.3720\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  8 | train Sqc: 0.5488 | loss: 1.2564\n",
      "epoch:    4 | step:  199 | N:  8 | train Sqc: 0.5353 | loss: 1.2557\n",
      "epoch:    4 | step:  299 | N:  8 | train Sqc: 0.5275 | loss: 1.2548\n",
      "epoch:    4 | step:  399 | N:  8 | train Sqc: 0.5295 | loss: 1.2549\n",
      "epoch:    4 | step:  499 | N:  8 | train Sqc: 0.5253 | loss: 1.2544\n",
      "epoch:    4 | step:  599 | N:  8 | train Sqc: 0.5206 | loss: 1.2538\n",
      "epoch:    4 | step:  699 | N:  8 | train Sqc: 0.5214 | loss: 1.2540\n",
      "epoch:    4 | step:  799 | N:  8 | train Sqc: 0.5194 | loss: 1.2538\n",
      "epoch:    4 | step:  899 | N:  8 | train Sqc: 0.5206 | loss: 1.2541\n",
      "epoch:    4 | step:  999 | N:  8 | train Sqc: 0.5174 | loss: 1.2539\n",
      "epoch:    4 | step:  1099 | N:  8 | train Sqc: 0.5156 | loss: 1.2536\n",
      "epoch:    4 | step:  1199 | N:  8 | train Sqc: 0.5161 | loss: 1.2535\n",
      "epoch:    4 | step:  1299 | N:  8 | train Sqc: 0.5165 | loss: 1.2537\n",
      "epoch:    4 | step:  1399 | N:  8 | train Sqc: 0.5158 | loss: 1.2535\n",
      "epoch:    4 | step:  1499 | N:  8 | train Sqc: 0.5153 | loss: 1.2533\n",
      "epoch:    4 | step:  1599 | N:  8 | train Sqc: 0.5159 | loss: 1.2534\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  8 | test Sqc: 0.5060 | test Neg: 0.3722\n",
      "epoch:    4 | step:  199 | N:  8 | test Sqc: 0.5105 | test Neg: 0.3721\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | train Sqc: 1.7021 | loss: 1.4742\n",
      "epoch:    0 | step:  199 | N:  10 | train Sqc: 1.4723 | loss: 1.4247\n",
      "epoch:    0 | step:  299 | N:  10 | train Sqc: 1.3263 | loss: 1.3976\n",
      "epoch:    0 | step:  399 | N:  10 | train Sqc: 1.2084 | loss: 1.3765\n"
     ]
    }
   ],
   "source": [
    "for N in range(6, 36, 2):\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'data/data_{N}na.pickle', shuffle=True)\n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "    \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, N-2)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, N-2)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    # load new model\n",
    "    if N == 4:\n",
    "        mdl = tel_mpo(34, bond=10)\n",
    "    else:\n",
    "        mdl.load_state_dict(torch.load(f'{file}/models/mpo_N={N-2}_na.pt'))\n",
    "    \n",
    "    # load old model\n",
    "    # mdl.load_state_dict(torch.load(f'{file}/models/mpo_N={N}_na.pt'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        # Train\n",
    "        print('='*50+'   Train   '+'='*50)\n",
    "        mdl.train()\n",
    "        for i in range(prepseq_train.shape[0]):\n",
    "            rhoC = mdl(prepseq_train[i])\n",
    "            l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "            optimizer.zero_grad()\n",
    "            probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "            loss = -probs.log().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l['loss'].append(loss.item())\n",
    "            if (i+1)%100 == 0:\n",
    "                trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/mpo_N={N}_na.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/mpo_N={N}_na.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432cd1f-ac10-4812-845e-19e3555c413a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
