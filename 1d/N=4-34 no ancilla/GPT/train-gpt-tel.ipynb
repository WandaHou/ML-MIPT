{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8f88f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# LLaMA-based Quantum State Reconstruction Training\n",
    "\n",
    "This notebook trains LLaMA transformer models to reconstruct quantum states from measurement data for 1D systems without ancilla qubits. \n",
    "\n",
    "**Training Configuration:**\n",
    "- **Epochs**: 30 training epochs per system size\n",
    "- **Data Split**: 8/9 training data, 1/9 test data (train_ratio = 8/9)\n",
    "- **Progressive Training**: Models trained sequentially from N=4 to N=34 qubits, with each model initialized from the previous smaller system\n",
    "- **Metrics**: Quantum coherence (Sqc) and negativity (Neg) measures for performance evaluation\n",
    "\n",
    "The training uses quantum shadow tomography data to teach the transformer to predict two-qubit density matrices from measurement outcome sequences.\n",
    "\n",
    "**Training Approach Comparison:**\n",
    "- **This notebook (Sequential)**: Trains separate models for each system size (N=4, N=6, N=8, ..., N=34), with transfer learning from smaller to larger systems. Each model specializes in one specific system size.\n",
    "- **GPT_together variant**: Trains a single unified model on all system sizes simultaneously (N=4 to N=34), where the model learns to handle variable-length sequences and generalize across different quantum system sizes in one training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6692"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "test = True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 8/9\n",
    "batch = 500\n",
    "\n",
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     L=4,\n",
    "                     n_embd=12, \n",
    "                     n_layer=6, \n",
    "                     n_head=6, \n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | train Sqc: 0.7487 | loss: 1.2846\n",
      "epoch:    0 | step:  199 | N:  4 | train Sqc: 0.6008 | loss: 1.2586\n",
      "epoch:    0 | step:  299 | N:  4 | train Sqc: 0.5432 | loss: 1.2494\n",
      "epoch:    0 | step:  399 | N:  4 | train Sqc: 0.5060 | loss: 1.2440\n",
      "epoch:    0 | step:  499 | N:  4 | train Sqc: 0.4850 | loss: 1.2408\n",
      "epoch:    0 | step:  599 | N:  4 | train Sqc: 0.4709 | loss: 1.2388\n",
      "epoch:    0 | step:  699 | N:  4 | train Sqc: 0.4579 | loss: 1.2371\n",
      "epoch:    0 | step:  799 | N:  4 | train Sqc: 0.4451 | loss: 1.2356\n",
      "epoch:    0 | step:  899 | N:  4 | train Sqc: 0.4356 | loss: 1.2344\n",
      "epoch:    0 | step:  999 | N:  4 | train Sqc: 0.4289 | loss: 1.2335\n",
      "epoch:    0 | step:  1099 | N:  4 | train Sqc: 0.4241 | loss: 1.2330\n",
      "epoch:    0 | step:  1199 | N:  4 | train Sqc: 0.4179 | loss: 1.2323\n",
      "epoch:    0 | step:  1299 | N:  4 | train Sqc: 0.4126 | loss: 1.2318\n",
      "epoch:    0 | step:  1399 | N:  4 | train Sqc: 0.4114 | loss: 1.2316\n",
      "epoch:    0 | step:  1499 | N:  4 | train Sqc: 0.4079 | loss: 1.2313\n",
      "epoch:    0 | step:  1599 | N:  4 | train Sqc: 0.4036 | loss: 1.2310\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3618 | test Neg: 0.4195\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3740 | test Neg: 0.4136\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | train Sqc: 0.3740 | loss: 1.2270\n",
      "epoch:    1 | step:  199 | N:  4 | train Sqc: 0.3733 | loss: 1.2277\n",
      "epoch:    1 | step:  299 | N:  4 | train Sqc: 0.3756 | loss: 1.2280\n",
      "epoch:    1 | step:  399 | N:  4 | train Sqc: 0.3715 | loss: 1.2275\n",
      "epoch:    1 | step:  499 | N:  4 | train Sqc: 0.3706 | loss: 1.2273\n",
      "epoch:    1 | step:  599 | N:  4 | train Sqc: 0.3719 | loss: 1.2274\n",
      "epoch:    1 | step:  699 | N:  4 | train Sqc: 0.3715 | loss: 1.2272\n",
      "epoch:    1 | step:  799 | N:  4 | train Sqc: 0.3681 | loss: 1.2268\n",
      "epoch:    1 | step:  899 | N:  4 | train Sqc: 0.3655 | loss: 1.2265\n",
      "epoch:    1 | step:  999 | N:  4 | train Sqc: 0.3638 | loss: 1.2263\n",
      "epoch:    1 | step:  1099 | N:  4 | train Sqc: 0.3644 | loss: 1.2264\n",
      "epoch:    1 | step:  1199 | N:  4 | train Sqc: 0.3624 | loss: 1.2263\n",
      "epoch:    1 | step:  1299 | N:  4 | train Sqc: 0.3611 | loss: 1.2262\n",
      "epoch:    1 | step:  1399 | N:  4 | train Sqc: 0.3632 | loss: 1.2264\n",
      "epoch:    1 | step:  1499 | N:  4 | train Sqc: 0.3625 | loss: 1.2264\n",
      "epoch:    1 | step:  1599 | N:  4 | train Sqc: 0.3608 | loss: 1.2263\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3576 | test Neg: 0.4200\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3699 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | train Sqc: 0.3712 | loss: 1.2266\n",
      "epoch:    2 | step:  199 | N:  4 | train Sqc: 0.3704 | loss: 1.2273\n",
      "epoch:    2 | step:  299 | N:  4 | train Sqc: 0.4336 | loss: 1.2350\n",
      "epoch:    2 | step:  399 | N:  4 | train Sqc: 0.4196 | loss: 1.2332\n",
      "epoch:    2 | step:  499 | N:  4 | train Sqc: 0.4097 | loss: 1.2319\n",
      "epoch:    2 | step:  599 | N:  4 | train Sqc: 0.4049 | loss: 1.2311\n",
      "epoch:    2 | step:  699 | N:  4 | train Sqc: 0.3995 | loss: 1.2304\n",
      "epoch:    2 | step:  799 | N:  4 | train Sqc: 0.3926 | loss: 1.2296\n",
      "epoch:    2 | step:  899 | N:  4 | train Sqc: 0.3867 | loss: 1.2289\n",
      "epoch:    2 | step:  999 | N:  4 | train Sqc: 0.3820 | loss: 1.2285\n",
      "epoch:    2 | step:  1099 | N:  4 | train Sqc: 0.3806 | loss: 1.2284\n",
      "epoch:    2 | step:  1199 | N:  4 | train Sqc: 0.3772 | loss: 1.2280\n",
      "epoch:    2 | step:  1299 | N:  4 | train Sqc: 0.3747 | loss: 1.2278\n",
      "epoch:    2 | step:  1399 | N:  4 | train Sqc: 0.3761 | loss: 1.2278\n",
      "epoch:    2 | step:  1499 | N:  4 | train Sqc: 0.3745 | loss: 1.2278\n",
      "epoch:    2 | step:  1599 | N:  4 | train Sqc: 0.3720 | loss: 1.2276\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3571 | test Neg: 0.4201\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3684 | test Neg: 0.4142\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | train Sqc: 0.3688 | loss: 1.2264\n",
      "epoch:    3 | step:  199 | N:  4 | train Sqc: 0.3673 | loss: 1.2272\n",
      "epoch:    3 | step:  299 | N:  4 | train Sqc: 0.3690 | loss: 1.2275\n",
      "epoch:    3 | step:  399 | N:  4 | train Sqc: 0.3656 | loss: 1.2271\n",
      "epoch:    3 | step:  499 | N:  4 | train Sqc: 0.3648 | loss: 1.2269\n",
      "epoch:    3 | step:  599 | N:  4 | train Sqc: 0.3666 | loss: 1.2270\n",
      "epoch:    3 | step:  699 | N:  4 | train Sqc: 0.3662 | loss: 1.2268\n",
      "epoch:    3 | step:  799 | N:  4 | train Sqc: 0.3637 | loss: 1.2264\n",
      "epoch:    3 | step:  899 | N:  4 | train Sqc: 0.3609 | loss: 1.2261\n",
      "epoch:    3 | step:  999 | N:  4 | train Sqc: 0.3589 | loss: 1.2259\n",
      "epoch:    3 | step:  1099 | N:  4 | train Sqc: 0.3596 | loss: 1.2260\n",
      "epoch:    3 | step:  1199 | N:  4 | train Sqc: 0.3580 | loss: 1.2259\n",
      "epoch:    3 | step:  1299 | N:  4 | train Sqc: 0.3569 | loss: 1.2258\n",
      "epoch:    3 | step:  1399 | N:  4 | train Sqc: 0.3592 | loss: 1.2260\n",
      "epoch:    3 | step:  1499 | N:  4 | train Sqc: 0.3586 | loss: 1.2260\n",
      "epoch:    3 | step:  1599 | N:  4 | train Sqc: 0.3569 | loss: 1.2260\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | test Sqc: 0.3557 | test Neg: 0.4202\n",
      "epoch:    3 | step:  199 | N:  4 | test Sqc: 0.3675 | test Neg: 0.4143\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | train Sqc: 0.3688 | loss: 1.2264\n",
      "epoch:    4 | step:  199 | N:  4 | train Sqc: 0.3679 | loss: 1.2271\n",
      "epoch:    4 | step:  299 | N:  4 | train Sqc: 0.3689 | loss: 1.2275\n",
      "epoch:    4 | step:  399 | N:  4 | train Sqc: 0.3654 | loss: 1.2270\n",
      "epoch:    4 | step:  499 | N:  4 | train Sqc: 0.3642 | loss: 1.2268\n",
      "epoch:    4 | step:  599 | N:  4 | train Sqc: 0.3660 | loss: 1.2269\n",
      "epoch:    4 | step:  699 | N:  4 | train Sqc: 0.3657 | loss: 1.2267\n",
      "epoch:    4 | step:  799 | N:  4 | train Sqc: 0.3631 | loss: 1.2263\n",
      "epoch:    4 | step:  899 | N:  4 | train Sqc: 0.3605 | loss: 1.2260\n",
      "epoch:    4 | step:  999 | N:  4 | train Sqc: 0.3585 | loss: 1.2259\n",
      "epoch:    4 | step:  1099 | N:  4 | train Sqc: 0.3592 | loss: 1.2260\n",
      "epoch:    4 | step:  1199 | N:  4 | train Sqc: 0.3576 | loss: 1.2258\n",
      "epoch:    4 | step:  1299 | N:  4 | train Sqc: 0.3564 | loss: 1.2257\n",
      "epoch:    4 | step:  1399 | N:  4 | train Sqc: 0.3586 | loss: 1.2259\n",
      "epoch:    4 | step:  1499 | N:  4 | train Sqc: 0.3580 | loss: 1.2260\n",
      "epoch:    4 | step:  1599 | N:  4 | train Sqc: 0.3563 | loss: 1.2259\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | test Sqc: 0.3554 | test Neg: 0.4202\n",
      "epoch:    4 | step:  199 | N:  4 | test Sqc: 0.3671 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | train Sqc: 0.3685 | loss: 1.2263\n",
      "epoch:    5 | step:  199 | N:  4 | train Sqc: 0.3679 | loss: 1.2270\n",
      "epoch:    5 | step:  299 | N:  4 | train Sqc: 0.3686 | loss: 1.2274\n",
      "epoch:    5 | step:  399 | N:  4 | train Sqc: 0.3650 | loss: 1.2269\n",
      "epoch:    5 | step:  499 | N:  4 | train Sqc: 0.3637 | loss: 1.2267\n",
      "epoch:    5 | step:  599 | N:  4 | train Sqc: 0.3654 | loss: 1.2268\n",
      "epoch:    5 | step:  699 | N:  4 | train Sqc: 0.3651 | loss: 1.2266\n",
      "epoch:    5 | step:  799 | N:  4 | train Sqc: 0.3626 | loss: 1.2263\n",
      "epoch:    5 | step:  899 | N:  4 | train Sqc: 0.3600 | loss: 1.2260\n",
      "epoch:    5 | step:  999 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:    5 | step:  1099 | N:  4 | train Sqc: 0.3589 | loss: 1.2259\n",
      "epoch:    5 | step:  1199 | N:  4 | train Sqc: 0.3573 | loss: 1.2258\n",
      "epoch:    5 | step:  1299 | N:  4 | train Sqc: 0.3561 | loss: 1.2257\n",
      "epoch:    5 | step:  1399 | N:  4 | train Sqc: 0.3583 | loss: 1.2259\n",
      "epoch:    5 | step:  1499 | N:  4 | train Sqc: 0.3577 | loss: 1.2259\n",
      "epoch:    5 | step:  1599 | N:  4 | train Sqc: 0.3559 | loss: 1.2258\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | test Sqc: 0.3549 | test Neg: 0.4203\n",
      "epoch:    5 | step:  199 | N:  4 | test Sqc: 0.3667 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | train Sqc: 0.3679 | loss: 1.2262\n",
      "epoch:    6 | step:  199 | N:  4 | train Sqc: 0.3674 | loss: 1.2270\n",
      "epoch:    6 | step:  299 | N:  4 | train Sqc: 0.3682 | loss: 1.2273\n",
      "epoch:    6 | step:  399 | N:  4 | train Sqc: 0.3645 | loss: 1.2269\n",
      "epoch:    6 | step:  499 | N:  4 | train Sqc: 0.3633 | loss: 1.2267\n",
      "epoch:    6 | step:  599 | N:  4 | train Sqc: 0.3650 | loss: 1.2268\n",
      "epoch:    6 | step:  699 | N:  4 | train Sqc: 0.3646 | loss: 1.2266\n",
      "epoch:    6 | step:  799 | N:  4 | train Sqc: 0.3621 | loss: 1.2262\n",
      "epoch:    6 | step:  899 | N:  4 | train Sqc: 0.3596 | loss: 1.2259\n",
      "epoch:    6 | step:  999 | N:  4 | train Sqc: 0.3577 | loss: 1.2257\n",
      "epoch:    6 | step:  1099 | N:  4 | train Sqc: 0.3584 | loss: 1.2258\n",
      "epoch:    6 | step:  1199 | N:  4 | train Sqc: 0.3568 | loss: 1.2257\n",
      "epoch:    6 | step:  1299 | N:  4 | train Sqc: 0.3558 | loss: 1.2256\n",
      "epoch:    6 | step:  1399 | N:  4 | train Sqc: 0.3579 | loss: 1.2258\n",
      "epoch:    6 | step:  1499 | N:  4 | train Sqc: 0.3573 | loss: 1.2259\n",
      "epoch:    6 | step:  1599 | N:  4 | train Sqc: 0.3555 | loss: 1.2258\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | test Sqc: 0.3549 | test Neg: 0.4203\n",
      "epoch:    6 | step:  199 | N:  4 | test Sqc: 0.3667 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | train Sqc: 0.3673 | loss: 1.2262\n",
      "epoch:    7 | step:  199 | N:  4 | train Sqc: 0.3670 | loss: 1.2269\n",
      "epoch:    7 | step:  299 | N:  4 | train Sqc: 0.3676 | loss: 1.2273\n",
      "epoch:    7 | step:  399 | N:  4 | train Sqc: 0.3641 | loss: 1.2268\n",
      "epoch:    7 | step:  499 | N:  4 | train Sqc: 0.3627 | loss: 1.2266\n",
      "epoch:    7 | step:  599 | N:  4 | train Sqc: 0.3645 | loss: 1.2267\n",
      "epoch:    7 | step:  699 | N:  4 | train Sqc: 0.3642 | loss: 1.2266\n",
      "epoch:    7 | step:  799 | N:  4 | train Sqc: 0.3617 | loss: 1.2262\n",
      "epoch:    7 | step:  899 | N:  4 | train Sqc: 0.3591 | loss: 1.2259\n",
      "epoch:    7 | step:  999 | N:  4 | train Sqc: 0.3574 | loss: 1.2257\n",
      "epoch:    7 | step:  1099 | N:  4 | train Sqc: 0.3581 | loss: 1.2258\n",
      "epoch:    7 | step:  1199 | N:  4 | train Sqc: 0.3567 | loss: 1.2257\n",
      "epoch:    7 | step:  1299 | N:  4 | train Sqc: 0.3556 | loss: 1.2256\n",
      "epoch:    7 | step:  1399 | N:  4 | train Sqc: 0.3577 | loss: 1.2258\n",
      "epoch:    7 | step:  1499 | N:  4 | train Sqc: 0.3570 | loss: 1.2258\n",
      "epoch:    7 | step:  1599 | N:  4 | train Sqc: 0.3553 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | test Sqc: 0.3546 | test Neg: 0.4203\n",
      "epoch:    7 | step:  199 | N:  4 | test Sqc: 0.3664 | test Neg: 0.4144\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | train Sqc: 0.3665 | loss: 1.2262\n",
      "epoch:    8 | step:  199 | N:  4 | train Sqc: 0.3661 | loss: 1.2269\n",
      "epoch:    8 | step:  299 | N:  4 | train Sqc: 0.3668 | loss: 1.2272\n",
      "epoch:    8 | step:  399 | N:  4 | train Sqc: 0.3635 | loss: 1.2268\n",
      "epoch:    8 | step:  499 | N:  4 | train Sqc: 0.3622 | loss: 1.2266\n",
      "epoch:    8 | step:  599 | N:  4 | train Sqc: 0.3640 | loss: 1.2267\n",
      "epoch:    8 | step:  699 | N:  4 | train Sqc: 0.3637 | loss: 1.2265\n",
      "epoch:    8 | step:  799 | N:  4 | train Sqc: 0.3612 | loss: 1.2261\n",
      "epoch:    8 | step:  899 | N:  4 | train Sqc: 0.3587 | loss: 1.2258\n",
      "epoch:    8 | step:  999 | N:  4 | train Sqc: 0.3570 | loss: 1.2257\n",
      "epoch:    8 | step:  1099 | N:  4 | train Sqc: 0.3577 | loss: 1.2258\n",
      "epoch:    8 | step:  1199 | N:  4 | train Sqc: 0.3563 | loss: 1.2256\n",
      "epoch:    8 | step:  1299 | N:  4 | train Sqc: 0.3552 | loss: 1.2255\n",
      "epoch:    8 | step:  1399 | N:  4 | train Sqc: 0.3575 | loss: 1.2257\n",
      "epoch:    8 | step:  1499 | N:  4 | train Sqc: 0.3567 | loss: 1.2258\n",
      "epoch:    8 | step:  1599 | N:  4 | train Sqc: 0.3551 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | test Sqc: 0.3555 | test Neg: 0.4203\n",
      "epoch:    8 | step:  199 | N:  4 | test Sqc: 0.3662 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | train Sqc: 0.3661 | loss: 1.2261\n",
      "epoch:    9 | step:  199 | N:  4 | train Sqc: 0.3654 | loss: 1.2269\n",
      "epoch:    9 | step:  299 | N:  4 | train Sqc: 0.3661 | loss: 1.2272\n",
      "epoch:    9 | step:  399 | N:  4 | train Sqc: 0.3630 | loss: 1.2268\n",
      "epoch:    9 | step:  499 | N:  4 | train Sqc: 0.3616 | loss: 1.2266\n",
      "epoch:    9 | step:  599 | N:  4 | train Sqc: 0.3635 | loss: 1.2267\n",
      "epoch:    9 | step:  699 | N:  4 | train Sqc: 0.3632 | loss: 1.2265\n",
      "epoch:    9 | step:  799 | N:  4 | train Sqc: 0.3608 | loss: 1.2261\n",
      "epoch:    9 | step:  899 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:    9 | step:  999 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:    9 | step:  1099 | N:  4 | train Sqc: 0.3573 | loss: 1.2257\n",
      "epoch:    9 | step:  1199 | N:  4 | train Sqc: 0.3559 | loss: 1.2256\n",
      "epoch:    9 | step:  1299 | N:  4 | train Sqc: 0.3549 | loss: 1.2255\n",
      "epoch:    9 | step:  1399 | N:  4 | train Sqc: 0.3569 | loss: 1.2257\n",
      "epoch:    9 | step:  1499 | N:  4 | train Sqc: 0.3562 | loss: 1.2257\n",
      "epoch:    9 | step:  1599 | N:  4 | train Sqc: 0.3545 | loss: 1.2257\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | test Sqc: 0.3545 | test Neg: 0.4203\n",
      "epoch:    9 | step:  199 | N:  4 | test Sqc: 0.3660 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   10 | step:   99 | N:  4 | train Sqc: 0.3659 | loss: 1.2261\n",
      "epoch:   10 | step:  199 | N:  4 | train Sqc: 0.3658 | loss: 1.2268\n",
      "epoch:   10 | step:  299 | N:  4 | train Sqc: 0.3664 | loss: 1.2272\n",
      "epoch:   10 | step:  399 | N:  4 | train Sqc: 0.3631 | loss: 1.2267\n",
      "epoch:   10 | step:  499 | N:  4 | train Sqc: 0.3618 | loss: 1.2265\n",
      "epoch:   10 | step:  599 | N:  4 | train Sqc: 0.3636 | loss: 1.2266\n",
      "epoch:   10 | step:  699 | N:  4 | train Sqc: 0.3633 | loss: 1.2265\n",
      "epoch:   10 | step:  799 | N:  4 | train Sqc: 0.3608 | loss: 1.2261\n",
      "epoch:   10 | step:  899 | N:  4 | train Sqc: 0.3582 | loss: 1.2258\n",
      "epoch:   10 | step:  999 | N:  4 | train Sqc: 0.3567 | loss: 1.2256\n",
      "epoch:   10 | step:  1099 | N:  4 | train Sqc: 0.3574 | loss: 1.2257\n",
      "epoch:   10 | step:  1199 | N:  4 | train Sqc: 0.3560 | loss: 1.2256\n",
      "epoch:   10 | step:  1299 | N:  4 | train Sqc: 0.3550 | loss: 1.2255\n",
      "epoch:   10 | step:  1399 | N:  4 | train Sqc: 0.3569 | loss: 1.2257\n",
      "epoch:   10 | step:  1499 | N:  4 | train Sqc: 0.3563 | loss: 1.2257\n",
      "epoch:   10 | step:  1599 | N:  4 | train Sqc: 0.3545 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   10 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4203\n",
      "epoch:   10 | step:  199 | N:  4 | test Sqc: 0.3661 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   11 | step:   99 | N:  4 | train Sqc: 0.3652 | loss: 1.2261\n",
      "epoch:   11 | step:  199 | N:  4 | train Sqc: 0.3653 | loss: 1.2268\n",
      "epoch:   11 | step:  299 | N:  4 | train Sqc: 0.3659 | loss: 1.2272\n",
      "epoch:   11 | step:  399 | N:  4 | train Sqc: 0.3628 | loss: 1.2267\n",
      "epoch:   11 | step:  499 | N:  4 | train Sqc: 0.3616 | loss: 1.2265\n",
      "epoch:   11 | step:  599 | N:  4 | train Sqc: 0.3634 | loss: 1.2266\n",
      "epoch:   11 | step:  699 | N:  4 | train Sqc: 0.3631 | loss: 1.2264\n",
      "epoch:   11 | step:  799 | N:  4 | train Sqc: 0.3606 | loss: 1.2261\n",
      "epoch:   11 | step:  899 | N:  4 | train Sqc: 0.3581 | loss: 1.2258\n",
      "epoch:   11 | step:  999 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:   11 | step:  1099 | N:  4 | train Sqc: 0.3572 | loss: 1.2257\n",
      "epoch:   11 | step:  1199 | N:  4 | train Sqc: 0.3558 | loss: 1.2256\n",
      "epoch:   11 | step:  1299 | N:  4 | train Sqc: 0.3548 | loss: 1.2255\n",
      "epoch:   11 | step:  1399 | N:  4 | train Sqc: 0.3567 | loss: 1.2257\n",
      "epoch:   11 | step:  1499 | N:  4 | train Sqc: 0.3561 | loss: 1.2257\n",
      "epoch:   11 | step:  1599 | N:  4 | train Sqc: 0.3543 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   11 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4203\n",
      "epoch:   11 | step:  199 | N:  4 | test Sqc: 0.3661 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   12 | step:   99 | N:  4 | train Sqc: 0.3650 | loss: 1.2260\n",
      "epoch:   12 | step:  199 | N:  4 | train Sqc: 0.3650 | loss: 1.2268\n",
      "epoch:   12 | step:  299 | N:  4 | train Sqc: 0.3656 | loss: 1.2271\n",
      "epoch:   12 | step:  399 | N:  4 | train Sqc: 0.3626 | loss: 1.2267\n",
      "epoch:   12 | step:  499 | N:  4 | train Sqc: 0.3613 | loss: 1.2265\n",
      "epoch:   12 | step:  599 | N:  4 | train Sqc: 0.3632 | loss: 1.2266\n",
      "epoch:   12 | step:  699 | N:  4 | train Sqc: 0.3629 | loss: 1.2264\n",
      "epoch:   12 | step:  799 | N:  4 | train Sqc: 0.3604 | loss: 1.2261\n",
      "epoch:   12 | step:  899 | N:  4 | train Sqc: 0.3578 | loss: 1.2257\n",
      "epoch:   12 | step:  999 | N:  4 | train Sqc: 0.3563 | loss: 1.2256\n",
      "epoch:   12 | step:  1099 | N:  4 | train Sqc: 0.3570 | loss: 1.2257\n",
      "epoch:   12 | step:  1199 | N:  4 | train Sqc: 0.3556 | loss: 1.2255\n",
      "epoch:   12 | step:  1299 | N:  4 | train Sqc: 0.3546 | loss: 1.2254\n",
      "epoch:   12 | step:  1399 | N:  4 | train Sqc: 0.3566 | loss: 1.2256\n",
      "epoch:   12 | step:  1499 | N:  4 | train Sqc: 0.3559 | loss: 1.2257\n",
      "epoch:   12 | step:  1599 | N:  4 | train Sqc: 0.3542 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   12 | step:   99 | N:  4 | test Sqc: 0.3547 | test Neg: 0.4204\n",
      "epoch:   12 | step:  199 | N:  4 | test Sqc: 0.3660 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   13 | step:   99 | N:  4 | train Sqc: 0.3647 | loss: 1.2260\n",
      "epoch:   13 | step:  199 | N:  4 | train Sqc: 0.3647 | loss: 1.2268\n",
      "epoch:   13 | step:  299 | N:  4 | train Sqc: 0.3653 | loss: 1.2271\n",
      "epoch:   13 | step:  399 | N:  4 | train Sqc: 0.3624 | loss: 1.2267\n",
      "epoch:   13 | step:  499 | N:  4 | train Sqc: 0.3611 | loss: 1.2265\n",
      "epoch:   13 | step:  599 | N:  4 | train Sqc: 0.3630 | loss: 1.2266\n",
      "epoch:   13 | step:  699 | N:  4 | train Sqc: 0.3627 | loss: 1.2264\n",
      "epoch:   13 | step:  799 | N:  4 | train Sqc: 0.3602 | loss: 1.2260\n",
      "epoch:   13 | step:  899 | N:  4 | train Sqc: 0.3576 | loss: 1.2257\n",
      "epoch:   13 | step:  999 | N:  4 | train Sqc: 0.3561 | loss: 1.2255\n",
      "epoch:   13 | step:  1099 | N:  4 | train Sqc: 0.3568 | loss: 1.2256\n",
      "epoch:   13 | step:  1199 | N:  4 | train Sqc: 0.3554 | loss: 1.2255\n",
      "epoch:   13 | step:  1299 | N:  4 | train Sqc: 0.3544 | loss: 1.2254\n",
      "epoch:   13 | step:  1399 | N:  4 | train Sqc: 0.3564 | loss: 1.2256\n",
      "epoch:   13 | step:  1499 | N:  4 | train Sqc: 0.3557 | loss: 1.2257\n",
      "epoch:   13 | step:  1599 | N:  4 | train Sqc: 0.3540 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   13 | step:   99 | N:  4 | test Sqc: 0.3545 | test Neg: 0.4204\n",
      "epoch:   13 | step:  199 | N:  4 | test Sqc: 0.3659 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   14 | step:   99 | N:  4 | train Sqc: 0.3645 | loss: 1.2260\n",
      "epoch:   14 | step:  199 | N:  4 | train Sqc: 0.3645 | loss: 1.2267\n",
      "epoch:   14 | step:  299 | N:  4 | train Sqc: 0.3651 | loss: 1.2271\n",
      "epoch:   14 | step:  399 | N:  4 | train Sqc: 0.3622 | loss: 1.2267\n",
      "epoch:   14 | step:  499 | N:  4 | train Sqc: 0.3610 | loss: 1.2265\n",
      "epoch:   14 | step:  599 | N:  4 | train Sqc: 0.3629 | loss: 1.2266\n",
      "epoch:   14 | step:  699 | N:  4 | train Sqc: 0.3626 | loss: 1.2264\n",
      "epoch:   14 | step:  799 | N:  4 | train Sqc: 0.3601 | loss: 1.2260\n",
      "epoch:   14 | step:  899 | N:  4 | train Sqc: 0.3575 | loss: 1.2257\n",
      "epoch:   14 | step:  999 | N:  4 | train Sqc: 0.3560 | loss: 1.2255\n",
      "epoch:   14 | step:  1099 | N:  4 | train Sqc: 0.3567 | loss: 1.2256\n",
      "epoch:   14 | step:  1199 | N:  4 | train Sqc: 0.3554 | loss: 1.2255\n",
      "epoch:   14 | step:  1299 | N:  4 | train Sqc: 0.3544 | loss: 1.2254\n",
      "epoch:   14 | step:  1399 | N:  4 | train Sqc: 0.3563 | loss: 1.2256\n",
      "epoch:   14 | step:  1499 | N:  4 | train Sqc: 0.3556 | loss: 1.2257\n",
      "epoch:   14 | step:  1599 | N:  4 | train Sqc: 0.3539 | loss: 1.2256\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   14 | step:   99 | N:  4 | test Sqc: 0.3543 | test Neg: 0.4204\n",
      "epoch:   14 | step:  199 | N:  4 | test Sqc: 0.3657 | test Neg: 0.4145\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   15 | step:   99 | N:  4 | train Sqc: 0.3643 | loss: 1.2260\n",
      "epoch:   15 | step:  199 | N:  4 | train Sqc: 0.3644 | loss: 1.2267\n",
      "epoch:   15 | step:  299 | N:  4 | train Sqc: 0.3650 | loss: 1.2271\n",
      "epoch:   15 | step:  399 | N:  4 | train Sqc: 0.3621 | loss: 1.2266\n",
      "epoch:   15 | step:  499 | N:  4 | train Sqc: 0.3608 | loss: 1.2265\n",
      "epoch:   15 | step:  599 | N:  4 | train Sqc: 0.3627 | loss: 1.2266\n",
      "epoch:   15 | step:  699 | N:  4 | train Sqc: 0.3624 | loss: 1.2264\n",
      "epoch:   15 | step:  799 | N:  4 | train Sqc: 0.3599 | loss: 1.2260\n",
      "epoch:   15 | step:  899 | N:  4 | train Sqc: 0.3585 | loss: 1.2259\n",
      "epoch:   15 | step:  999 | N:  4 | train Sqc: 0.3569 | loss: 1.2257\n",
      "epoch:   15 | step:  1099 | N:  4 | train Sqc: 0.3575 | loss: 1.2258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m mdl\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(prepseq_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 43\u001b[0m     rhoC \u001b[38;5;241m=\u001b[39m \u001b[43mmdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepseq_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     l[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain Sqc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(bSqc(rhoS_train[i], rhoC)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects_jupyter/Google collab/tel/N=4-34 no ancilla/training files/GPT/Llama2.py:52\u001b[0m, in \u001b[0;36mLlamaPredictor.forward\u001b[0;34m(self, measure)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, measure):\n\u001b[1;32m     51\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_msk\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(measure\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mmeasure\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 52\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     53\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(hidden_states) \u001b[38;5;66;03m# (batch, seq_length, 32)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m) \u001b[38;5;66;03m# (batch, 32)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:649\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\u001b[39;00m\n\u001b[1;32m    647\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    659\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for N in range(4,36,2):\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/data_{N}na.pickle', shuffle=True)\n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "    \n",
    "    prepseq = torch.cat([prepseq+1, torch.zeros(prepseq.shape[0],1).to(prepseq.dtype).to(device)], -1)\n",
    "    \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, N-1)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, N-1)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    mdl = LlamaPredictor(L_max=35,\n",
    "                     L=prepseq_train.shape[1]-1,\n",
    "                     n_embd=12, \n",
    "                     n_layer=6, \n",
    "                     n_head=6, \n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "    # load new model\n",
    "    if N > 4:\n",
    "        mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N-2}_na.pt'))\n",
    "    # load old model\n",
    "    # mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N}_na.pt'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        # Train\n",
    "        print('='*50+'   Train   '+'='*50)\n",
    "        mdl.train()\n",
    "        for i in range(prepseq_train.shape[0]):\n",
    "            rhoC = mdl(prepseq_train[i])\n",
    "            l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "            optimizer.zero_grad()\n",
    "            probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "            loss = -probs.log().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l['loss'].append(loss.item())\n",
    "            if (i+1)%100 == 0:\n",
    "                trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/gpt_N={N}_na.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/gpt_N={N}_na.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd304a4-53cc-469a-a5d0-1e79de9dcabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
