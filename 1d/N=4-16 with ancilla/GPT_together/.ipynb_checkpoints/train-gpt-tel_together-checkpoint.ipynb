{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "test = True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 8/9\n",
    "batch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae16959-337c-480b-84ec-cf28828f186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepseq_train = torch.load(f'../data/prepseq_train.pt')\n",
    "shadow_state_train = torch.load(f'../data/shadow_state_train.pt')\n",
    "rhoS_train = torch.load(f'../data/rhoS_train.pt')\n",
    "\n",
    "prepseq_test = torch.load(f'../data/prepseq_test.pt')\n",
    "shadow_state_test = torch.load(f'../data/shadow_state_test.pt')\n",
    "rhoS_test = torch.load(f'../data/rhoS_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3999e5c4-d433-4385-8c15-945a8acaf5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25600, 500, 33]), torch.Size([16, 200, 500, 33]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split in batches\n",
    "prepseq_train = prepseq_train.view(-1, batch, 33)\n",
    "shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "prepseq_test = prepseq_test.view(16, -1, batch, 33)\n",
    "shadow_state_test = shadow_state_test.view(16, -1, batch, 4)\n",
    "rhoS_test = rhoS_test.view(16, -1, batch, 4, 4)\n",
    "    \n",
    "prepseq_train.shape, prepseq_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64d1744-dd15-481b-a263-c9bfe3155a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     n_embd=24, \n",
    "                     n_layer=12, \n",
    "                     n_head=6, \n",
    "                     vocab_size=4, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'loss':[]}\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7ad23-b1b6-494a-8ef4-b66d63367a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     n_embd=12, \n",
    "                     n_layer=6, \n",
    "                     n_head=6, \n",
    "                     vocab_size=4, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'loss':[]}\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | train Sqc: 1.4176 | loss: 1.3912\n",
      "epoch:    0 | step:  199 | train Sqc: 1.4038 | loss: 1.3889\n",
      "epoch:    0 | step:  299 | train Sqc: 1.3988 | loss: 1.3883\n",
      "epoch:    0 | step:  399 | train Sqc: 1.3944 | loss: 1.3875\n",
      "epoch:    0 | step:  499 | train Sqc: 1.3902 | loss: 1.3868\n",
      "epoch:    0 | step:  599 | train Sqc: 1.3851 | loss: 1.3860\n",
      "epoch:    0 | step:  699 | train Sqc: 1.3782 | loss: 1.3851\n",
      "epoch:    0 | step:  799 | train Sqc: 1.3715 | loss: 1.3841\n",
      "epoch:    0 | step:  899 | train Sqc: 1.3672 | loss: 1.3832\n",
      "epoch:    0 | step:  999 | train Sqc: 1.3616 | loss: 1.3823\n",
      "epoch:    0 | step:  1099 | train Sqc: 1.3569 | loss: 1.3816\n",
      "epoch:    0 | step:  1199 | train Sqc: 1.3528 | loss: 1.3810\n",
      "epoch:    0 | step:  1299 | train Sqc: 1.3496 | loss: 1.3804\n",
      "epoch:    0 | step:  1399 | train Sqc: 1.3458 | loss: 1.3798\n",
      "epoch:    0 | step:  1499 | train Sqc: 1.3419 | loss: 1.3791\n",
      "epoch:    0 | step:  1599 | train Sqc: 1.3405 | loss: 1.3788\n",
      "epoch:    0 | step:  1699 | train Sqc: 1.3394 | loss: 1.3787\n",
      "epoch:    0 | step:  1799 | train Sqc: 1.3367 | loss: 1.3782\n",
      "epoch:    0 | step:  1899 | train Sqc: 1.3334 | loss: 1.3778\n",
      "epoch:    0 | step:  1999 | train Sqc: 1.3299 | loss: 1.3773\n",
      "epoch:    0 | step:  2099 | train Sqc: 1.3272 | loss: 1.3768\n",
      "epoch:    0 | step:  2199 | train Sqc: 1.3246 | loss: 1.3764\n",
      "epoch:    0 | step:  2299 | train Sqc: 1.3217 | loss: 1.3760\n",
      "epoch:    0 | step:  2399 | train Sqc: 1.3197 | loss: 1.3757\n",
      "epoch:    0 | step:  2499 | train Sqc: 1.3173 | loss: 1.3753\n",
      "epoch:    0 | step:  2599 | train Sqc: 1.3150 | loss: 1.3750\n",
      "epoch:    0 | step:  2699 | train Sqc: 1.3127 | loss: 1.3746\n",
      "epoch:    0 | step:  2799 | train Sqc: 1.3101 | loss: 1.3742\n",
      "epoch:    0 | step:  2899 | train Sqc: 1.3082 | loss: 1.3739\n",
      "epoch:    0 | step:  2999 | train Sqc: 1.3067 | loss: 1.3736\n",
      "epoch:    0 | step:  3099 | train Sqc: 1.3050 | loss: 1.3734\n",
      "epoch:    0 | step:  3199 | train Sqc: 1.3040 | loss: 1.3732\n",
      "epoch:    0 | step:  3299 | train Sqc: 1.3024 | loss: 1.3729\n",
      "epoch:    0 | step:  3399 | train Sqc: 1.3008 | loss: 1.3727\n",
      "epoch:    0 | step:  3499 | train Sqc: 1.2998 | loss: 1.3725\n",
      "epoch:    0 | step:  3599 | train Sqc: 1.2986 | loss: 1.3723\n",
      "epoch:    0 | step:  3699 | train Sqc: 1.2973 | loss: 1.3721\n",
      "epoch:    0 | step:  3799 | train Sqc: 1.2962 | loss: 1.3719\n",
      "epoch:    0 | step:  3899 | train Sqc: 1.2950 | loss: 1.3717\n",
      "epoch:    0 | step:  3999 | train Sqc: 1.2941 | loss: 1.3715\n",
      "epoch:    0 | step:  4099 | train Sqc: 1.2926 | loss: 1.3713\n",
      "epoch:    0 | step:  4199 | train Sqc: 1.2910 | loss: 1.3710\n",
      "epoch:    0 | step:  4299 | train Sqc: 1.2900 | loss: 1.3709\n",
      "epoch:    0 | step:  4399 | train Sqc: 1.2902 | loss: 1.3709\n",
      "epoch:    0 | step:  4499 | train Sqc: 1.2891 | loss: 1.3708\n",
      "epoch:    0 | step:  4599 | train Sqc: 1.2883 | loss: 1.3706\n",
      "epoch:    0 | step:  4699 | train Sqc: 1.2874 | loss: 1.3705\n",
      "epoch:    0 | step:  4799 | train Sqc: 1.2862 | loss: 1.3703\n",
      "epoch:    0 | step:  4899 | train Sqc: 1.2854 | loss: 1.3702\n",
      "epoch:    0 | step:  4999 | train Sqc: 1.2846 | loss: 1.3700\n",
      "epoch:    0 | step:  5099 | train Sqc: 1.2842 | loss: 1.3700\n",
      "epoch:    0 | step:  5199 | train Sqc: 1.2834 | loss: 1.3699\n",
      "epoch:    0 | step:  5299 | train Sqc: 1.2826 | loss: 1.3697\n",
      "epoch:    0 | step:  5399 | train Sqc: 1.2817 | loss: 1.3696\n",
      "epoch:    0 | step:  5499 | train Sqc: 1.2807 | loss: 1.3695\n",
      "epoch:    0 | step:  5599 | train Sqc: 1.2799 | loss: 1.3693\n",
      "epoch:    0 | step:  5699 | train Sqc: 1.2789 | loss: 1.3692\n",
      "epoch:    0 | step:  5799 | train Sqc: 1.2778 | loss: 1.3690\n",
      "epoch:    0 | step:  5899 | train Sqc: 1.2768 | loss: 1.3689\n",
      "epoch:    0 | step:  5999 | train Sqc: 1.2761 | loss: 1.3688\n",
      "epoch:    0 | step:  6099 | train Sqc: 1.2753 | loss: 1.3687\n",
      "epoch:    0 | step:  6199 | train Sqc: 1.2750 | loss: 1.3686\n",
      "epoch:    0 | step:  6299 | train Sqc: 1.2749 | loss: 1.3686\n",
      "epoch:    0 | step:  6399 | train Sqc: 1.2744 | loss: 1.3685\n",
      "epoch:    0 | step:  6499 | train Sqc: 1.2738 | loss: 1.3684\n",
      "epoch:    0 | step:  6599 | train Sqc: 1.2732 | loss: 1.3683\n",
      "epoch:    0 | step:  6699 | train Sqc: 1.2724 | loss: 1.3682\n",
      "epoch:    0 | step:  6799 | train Sqc: 1.2716 | loss: 1.3681\n",
      "epoch:    0 | step:  6899 | train Sqc: 1.2709 | loss: 1.3680\n",
      "epoch:    0 | step:  6999 | train Sqc: 1.2698 | loss: 1.3678\n",
      "epoch:    0 | step:  7099 | train Sqc: 1.2690 | loss: 1.3677\n",
      "epoch:    0 | step:  7199 | train Sqc: 1.2679 | loss: 1.3675\n",
      "epoch:    0 | step:  7299 | train Sqc: 1.2673 | loss: 1.3674\n",
      "epoch:    0 | step:  7399 | train Sqc: 1.2664 | loss: 1.3673\n",
      "epoch:    0 | step:  7499 | train Sqc: 1.2656 | loss: 1.3672\n",
      "epoch:    0 | step:  7599 | train Sqc: 1.2648 | loss: 1.3670\n",
      "epoch:    0 | step:  7699 | train Sqc: 1.2640 | loss: 1.3669\n",
      "epoch:    0 | step:  7799 | train Sqc: 1.2632 | loss: 1.3668\n",
      "epoch:    0 | step:  7899 | train Sqc: 1.2624 | loss: 1.3667\n",
      "epoch:    0 | step:  7999 | train Sqc: 1.2616 | loss: 1.3666\n",
      "epoch:    0 | step:  8099 | train Sqc: 1.2608 | loss: 1.3664\n",
      "epoch:    0 | step:  8199 | train Sqc: 1.2601 | loss: 1.3663\n",
      "epoch:    0 | step:  8299 | train Sqc: 1.2594 | loss: 1.3662\n",
      "epoch:    0 | step:  8399 | train Sqc: 1.2587 | loss: 1.3661\n",
      "epoch:    0 | step:  8499 | train Sqc: 1.2581 | loss: 1.3660\n",
      "epoch:    0 | step:  8599 | train Sqc: 1.2573 | loss: 1.3659\n",
      "epoch:    0 | step:  8699 | train Sqc: 1.2567 | loss: 1.3658\n",
      "epoch:    0 | step:  8799 | train Sqc: 1.2562 | loss: 1.3657\n",
      "epoch:    0 | step:  8899 | train Sqc: 1.2554 | loss: 1.3656\n",
      "epoch:    0 | step:  8999 | train Sqc: 1.2549 | loss: 1.3655\n",
      "epoch:    0 | step:  9099 | train Sqc: 1.2543 | loss: 1.3655\n",
      "epoch:    0 | step:  9199 | train Sqc: 1.2539 | loss: 1.3654\n",
      "epoch:    0 | step:  9299 | train Sqc: 1.2535 | loss: 1.3653\n",
      "epoch:    0 | step:  9399 | train Sqc: 1.2529 | loss: 1.3652\n",
      "epoch:    0 | step:  9499 | train Sqc: 1.2523 | loss: 1.3652\n",
      "epoch:    0 | step:  9599 | train Sqc: 1.2519 | loss: 1.3651\n",
      "epoch:    0 | step:  9699 | train Sqc: 1.2513 | loss: 1.3650\n",
      "epoch:    0 | step:  9799 | train Sqc: 1.2508 | loss: 1.3649\n",
      "epoch:    0 | step:  9899 | train Sqc: 1.2503 | loss: 1.3648\n",
      "epoch:    0 | step:  9999 | train Sqc: 1.2497 | loss: 1.3648\n",
      "epoch:    0 | step:  10099 | train Sqc: 1.2490 | loss: 1.3647\n",
      "epoch:    0 | step:  10199 | train Sqc: 1.2485 | loss: 1.3646\n",
      "epoch:    0 | step:  10299 | train Sqc: 1.2480 | loss: 1.3645\n",
      "epoch:    0 | step:  10399 | train Sqc: 1.2475 | loss: 1.3644\n",
      "epoch:    0 | step:  10499 | train Sqc: 1.2471 | loss: 1.3643\n",
      "epoch:    0 | step:  10599 | train Sqc: 1.2465 | loss: 1.3643\n",
      "epoch:    0 | step:  10699 | train Sqc: 1.2458 | loss: 1.3642\n",
      "epoch:    0 | step:  10799 | train Sqc: 1.2454 | loss: 1.3641\n",
      "epoch:    0 | step:  10899 | train Sqc: 1.2448 | loss: 1.3640\n",
      "epoch:    0 | step:  10999 | train Sqc: 1.2445 | loss: 1.3640\n",
      "epoch:    0 | step:  11099 | train Sqc: 1.2442 | loss: 1.3639\n",
      "epoch:    0 | step:  11199 | train Sqc: 1.2438 | loss: 1.3639\n",
      "epoch:    0 | step:  11299 | train Sqc: 1.2434 | loss: 1.3638\n",
      "epoch:    0 | step:  11399 | train Sqc: 1.2430 | loss: 1.3638\n",
      "epoch:    0 | step:  11499 | train Sqc: 1.2426 | loss: 1.3637\n",
      "epoch:    0 | step:  11599 | train Sqc: 1.2422 | loss: 1.3636\n",
      "epoch:    0 | step:  11699 | train Sqc: 1.2418 | loss: 1.3636\n",
      "epoch:    0 | step:  11799 | train Sqc: 1.2413 | loss: 1.3635\n",
      "epoch:    0 | step:  11899 | train Sqc: 1.2410 | loss: 1.3634\n",
      "epoch:    0 | step:  11999 | train Sqc: 1.2407 | loss: 1.3634\n",
      "epoch:    0 | step:  12099 | train Sqc: 1.2401 | loss: 1.3633\n",
      "epoch:    0 | step:  12199 | train Sqc: 1.2397 | loss: 1.3632\n",
      "epoch:    0 | step:  12299 | train Sqc: 1.2393 | loss: 1.3632\n",
      "epoch:    0 | step:  12399 | train Sqc: 1.2388 | loss: 1.3631\n",
      "epoch:    0 | step:  12499 | train Sqc: 1.2383 | loss: 1.3630\n",
      "epoch:    0 | step:  12599 | train Sqc: 1.2379 | loss: 1.3630\n",
      "epoch:    0 | step:  12699 | train Sqc: 1.2375 | loss: 1.3629\n",
      "epoch:    0 | step:  12799 | train Sqc: 1.2372 | loss: 1.3628\n",
      "epoch:    0 | step:  12899 | train Sqc: 1.2368 | loss: 1.3628\n",
      "epoch:    0 | step:  12999 | train Sqc: 1.2364 | loss: 1.3627\n",
      "epoch:    0 | step:  13099 | train Sqc: 1.2360 | loss: 1.3627\n",
      "epoch:    0 | step:  13199 | train Sqc: 1.2355 | loss: 1.3626\n",
      "epoch:    0 | step:  13299 | train Sqc: 1.2351 | loss: 1.3625\n",
      "epoch:    0 | step:  13399 | train Sqc: 1.2348 | loss: 1.3625\n",
      "epoch:    0 | step:  13499 | train Sqc: 1.2343 | loss: 1.3624\n",
      "epoch:    0 | step:  13599 | train Sqc: 1.2339 | loss: 1.3623\n",
      "epoch:    0 | step:  13699 | train Sqc: 1.2336 | loss: 1.3623\n",
      "epoch:    0 | step:  13799 | train Sqc: 1.2332 | loss: 1.3622\n",
      "epoch:    0 | step:  13899 | train Sqc: 1.2328 | loss: 1.3622\n",
      "epoch:    0 | step:  13999 | train Sqc: 1.2324 | loss: 1.3621\n",
      "epoch:    0 | step:  14099 | train Sqc: 1.2320 | loss: 1.3620\n",
      "epoch:    0 | step:  14199 | train Sqc: 1.2315 | loss: 1.3620\n",
      "epoch:    0 | step:  14299 | train Sqc: 1.2311 | loss: 1.3619\n",
      "epoch:    0 | step:  14399 | train Sqc: 1.2307 | loss: 1.3619\n",
      "epoch:    0 | step:  14499 | train Sqc: 1.2302 | loss: 1.3618\n",
      "epoch:    0 | step:  14599 | train Sqc: 1.2298 | loss: 1.3617\n",
      "epoch:    0 | step:  14699 | train Sqc: 1.2292 | loss: 1.3616\n",
      "epoch:    0 | step:  14799 | train Sqc: 1.2289 | loss: 1.3616\n",
      "epoch:    0 | step:  14899 | train Sqc: 1.2286 | loss: 1.3615\n",
      "epoch:    0 | step:  14999 | train Sqc: 1.2282 | loss: 1.3615\n",
      "epoch:    0 | step:  15099 | train Sqc: 1.2278 | loss: 1.3614\n",
      "epoch:    0 | step:  15199 | train Sqc: 1.2274 | loss: 1.3614\n",
      "epoch:    0 | step:  15299 | train Sqc: 1.2270 | loss: 1.3613\n",
      "epoch:    0 | step:  15399 | train Sqc: 1.2266 | loss: 1.3612\n",
      "epoch:    0 | step:  15499 | train Sqc: 1.2261 | loss: 1.3612\n",
      "epoch:    0 | step:  15599 | train Sqc: 1.2257 | loss: 1.3611\n",
      "epoch:    0 | step:  15699 | train Sqc: 1.2252 | loss: 1.3610\n",
      "epoch:    0 | step:  15799 | train Sqc: 1.2248 | loss: 1.3609\n",
      "epoch:    0 | step:  15899 | train Sqc: 1.2242 | loss: 1.3609\n",
      "epoch:    0 | step:  15999 | train Sqc: 1.2239 | loss: 1.3608\n",
      "epoch:    0 | step:  16099 | train Sqc: 1.2236 | loss: 1.3608\n",
      "epoch:    0 | step:  16199 | train Sqc: 1.2232 | loss: 1.3607\n",
      "epoch:    0 | step:  16299 | train Sqc: 1.2229 | loss: 1.3607\n",
      "epoch:    0 | step:  16399 | train Sqc: 1.2225 | loss: 1.3606\n",
      "epoch:    0 | step:  16499 | train Sqc: 1.2221 | loss: 1.3605\n",
      "epoch:    0 | step:  16599 | train Sqc: 1.2217 | loss: 1.3605\n",
      "epoch:    0 | step:  16699 | train Sqc: 1.2213 | loss: 1.3604\n",
      "epoch:    0 | step:  16799 | train Sqc: 1.2209 | loss: 1.3604\n",
      "epoch:    0 | step:  16899 | train Sqc: 1.2205 | loss: 1.3603\n",
      "epoch:    0 | step:  16999 | train Sqc: 1.2200 | loss: 1.3602\n",
      "epoch:    0 | step:  17099 | train Sqc: 1.2197 | loss: 1.3602\n",
      "epoch:    0 | step:  17199 | train Sqc: 1.2193 | loss: 1.3601\n",
      "epoch:    0 | step:  17299 | train Sqc: 1.2189 | loss: 1.3601\n",
      "epoch:    0 | step:  17399 | train Sqc: 1.2187 | loss: 1.3600\n",
      "epoch:    0 | step:  17499 | train Sqc: 1.2185 | loss: 1.3600\n",
      "epoch:    0 | step:  17599 | train Sqc: 1.2182 | loss: 1.3600\n",
      "epoch:    0 | step:  17699 | train Sqc: 1.2179 | loss: 1.3599\n",
      "epoch:    0 | step:  17799 | train Sqc: 1.2175 | loss: 1.3599\n",
      "epoch:    0 | step:  17899 | train Sqc: 1.2171 | loss: 1.3598\n",
      "epoch:    0 | step:  17999 | train Sqc: 1.2168 | loss: 1.3597\n",
      "epoch:    0 | step:  18099 | train Sqc: 1.2164 | loss: 1.3597\n",
      "epoch:    0 | step:  18199 | train Sqc: 1.2162 | loss: 1.3597\n",
      "epoch:    0 | step:  18299 | train Sqc: 1.2159 | loss: 1.3596\n",
      "epoch:    0 | step:  18399 | train Sqc: 1.2156 | loss: 1.3596\n",
      "epoch:    0 | step:  18499 | train Sqc: 1.2153 | loss: 1.3595\n",
      "epoch:    0 | step:  18599 | train Sqc: 1.2150 | loss: 1.3595\n",
      "epoch:    0 | step:  18699 | train Sqc: 1.2147 | loss: 1.3594\n",
      "epoch:    0 | step:  18799 | train Sqc: 1.2143 | loss: 1.3594\n",
      "epoch:    0 | step:  18899 | train Sqc: 1.2140 | loss: 1.3593\n",
      "epoch:    0 | step:  18999 | train Sqc: 1.2137 | loss: 1.3593\n",
      "epoch:    0 | step:  19099 | train Sqc: 1.2134 | loss: 1.3592\n",
      "epoch:    0 | step:  19199 | train Sqc: 1.2131 | loss: 1.3592\n",
      "epoch:    0 | step:  19299 | train Sqc: 1.2128 | loss: 1.3591\n",
      "epoch:    0 | step:  19399 | train Sqc: 1.2125 | loss: 1.3591\n",
      "epoch:    0 | step:  19499 | train Sqc: 1.2122 | loss: 1.3591\n",
      "epoch:    0 | step:  19599 | train Sqc: 1.2119 | loss: 1.3590\n",
      "epoch:    0 | step:  19699 | train Sqc: 1.2116 | loss: 1.3590\n",
      "epoch:    0 | step:  19799 | train Sqc: 1.2112 | loss: 1.3589\n",
      "epoch:    0 | step:  19899 | train Sqc: 1.2109 | loss: 1.3589\n",
      "epoch:    0 | step:  19999 | train Sqc: 1.2106 | loss: 1.3588\n",
      "epoch:    0 | step:  20099 | train Sqc: 1.2103 | loss: 1.3588\n",
      "epoch:    0 | step:  20199 | train Sqc: 1.2099 | loss: 1.3587\n",
      "epoch:    0 | step:  20299 | train Sqc: 1.2096 | loss: 1.3587\n",
      "epoch:    0 | step:  20399 | train Sqc: 1.2093 | loss: 1.3586\n",
      "epoch:    0 | step:  20499 | train Sqc: 1.2091 | loss: 1.3586\n",
      "epoch:    0 | step:  20599 | train Sqc: 1.2088 | loss: 1.3586\n",
      "epoch:    0 | step:  20699 | train Sqc: 1.2086 | loss: 1.3585\n",
      "epoch:    0 | step:  20799 | train Sqc: 1.2083 | loss: 1.3585\n",
      "epoch:    0 | step:  20899 | train Sqc: 1.2080 | loss: 1.3584\n",
      "epoch:    0 | step:  20999 | train Sqc: 1.2077 | loss: 1.3584\n",
      "epoch:    0 | step:  21099 | train Sqc: 1.2074 | loss: 1.3583\n",
      "epoch:    0 | step:  21199 | train Sqc: 1.2071 | loss: 1.3583\n",
      "epoch:    0 | step:  21299 | train Sqc: 1.2068 | loss: 1.3583\n",
      "epoch:    0 | step:  21399 | train Sqc: 1.2065 | loss: 1.3582\n",
      "epoch:    0 | step:  21499 | train Sqc: 1.2063 | loss: 1.3582\n",
      "epoch:    0 | step:  21599 | train Sqc: 1.2060 | loss: 1.3581\n",
      "epoch:    0 | step:  21699 | train Sqc: 1.2057 | loss: 1.3581\n",
      "epoch:    0 | step:  21799 | train Sqc: 1.2055 | loss: 1.3581\n",
      "epoch:    0 | step:  21899 | train Sqc: 1.2052 | loss: 1.3580\n",
      "epoch:    0 | step:  21999 | train Sqc: 1.2049 | loss: 1.3580\n",
      "epoch:    0 | step:  22099 | train Sqc: 1.2047 | loss: 1.3579\n",
      "epoch:    0 | step:  22199 | train Sqc: 1.2045 | loss: 1.3579\n",
      "epoch:    0 | step:  22299 | train Sqc: 1.2042 | loss: 1.3579\n",
      "epoch:    0 | step:  22399 | train Sqc: 1.2039 | loss: 1.3578\n",
      "epoch:    0 | step:  22499 | train Sqc: 1.2038 | loss: 1.3578\n",
      "epoch:    0 | step:  22599 | train Sqc: 1.2041 | loss: 1.3579\n",
      "epoch:    0 | step:  22699 | train Sqc: 1.2042 | loss: 1.3579\n",
      "epoch:    0 | step:  22799 | train Sqc: 1.2044 | loss: 1.3579\n",
      "epoch:    0 | step:  22899 | train Sqc: 1.2044 | loss: 1.3579\n",
      "epoch:    0 | step:  22999 | train Sqc: 1.2045 | loss: 1.3579\n",
      "epoch:    0 | step:  23099 | train Sqc: 1.2045 | loss: 1.3579\n",
      "epoch:    0 | step:  23199 | train Sqc: 1.2044 | loss: 1.3579\n",
      "epoch:    0 | step:  23299 | train Sqc: 1.2042 | loss: 1.3579\n",
      "epoch:    0 | step:  23399 | train Sqc: 1.2041 | loss: 1.3579\n",
      "epoch:    0 | step:  23499 | train Sqc: 1.2039 | loss: 1.3578\n",
      "epoch:    0 | step:  23599 | train Sqc: 1.2037 | loss: 1.3578\n",
      "epoch:    0 | step:  23699 | train Sqc: 1.2035 | loss: 1.3578\n",
      "epoch:    0 | step:  23799 | train Sqc: 1.2032 | loss: 1.3577\n",
      "epoch:    0 | step:  23899 | train Sqc: 1.2030 | loss: 1.3577\n",
      "epoch:    0 | step:  23999 | train Sqc: 1.2028 | loss: 1.3577\n",
      "epoch:    0 | step:  24099 | train Sqc: 1.2026 | loss: 1.3576\n",
      "epoch:    0 | step:  24199 | train Sqc: 1.2024 | loss: 1.3576\n",
      "epoch:    0 | step:  24299 | train Sqc: 1.2022 | loss: 1.3576\n",
      "epoch:    0 | step:  24399 | train Sqc: 1.2020 | loss: 1.3575\n",
      "epoch:    0 | step:  24499 | train Sqc: 1.2017 | loss: 1.3575\n",
      "epoch:    0 | step:  24599 | train Sqc: 1.2014 | loss: 1.3575\n",
      "epoch:    0 | step:  24699 | train Sqc: 1.2012 | loss: 1.3574\n",
      "epoch:    0 | step:  24799 | train Sqc: 1.2009 | loss: 1.3574\n",
      "epoch:    0 | step:  24899 | train Sqc: 1.2007 | loss: 1.3574\n",
      "epoch:    0 | step:  24999 | train Sqc: 1.2004 | loss: 1.3573\n",
      "epoch:    0 | step:  25099 | train Sqc: 1.2002 | loss: 1.3573\n",
      "epoch:    0 | step:  25199 | train Sqc: 1.2000 | loss: 1.3572\n",
      "epoch:    0 | step:  25299 | train Sqc: 1.1998 | loss: 1.3572\n",
      "epoch:    0 | step:  25399 | train Sqc: 1.1995 | loss: 1.3572\n",
      "epoch:    0 | step:  25499 | train Sqc: 1.1992 | loss: 1.3571\n",
      "epoch:    0 | step:  25599 | train Sqc: 1.1990 | loss: 1.3571\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3535 | test Neg: 0.4199\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3742 | test Neg: 0.4144\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4061 | test Neg: 0.4069\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4216 | test Neg: 0.4023\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.5301 | test Neg: 0.3631\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.5322 | test Neg: 0.3614\n",
      "epoch:    0 | step:   99 | N:  10 | test Sqc: 0.6487 | test Neg: 0.3253\n",
      "epoch:    0 | step:  199 | N:  10 | test Sqc: 0.6511 | test Neg: 0.3239\n",
      "epoch:    0 | step:   99 | N:  12 | test Sqc: 1.0800 | test Neg: 0.1216\n",
      "epoch:    0 | step:  199 | N:  12 | test Sqc: 1.0854 | test Neg: 0.1192\n",
      "epoch:    0 | step:   99 | N:  14 | test Sqc: 1.3212 | test Neg: 0.0266\n",
      "epoch:    0 | step:  199 | N:  14 | test Sqc: 1.3212 | test Neg: 0.0244\n",
      "epoch:    0 | step:   99 | N:  16 | test Sqc: 1.3756 | test Neg: 0.0039\n",
      "epoch:    0 | step:  199 | N:  16 | test Sqc: 1.3768 | test Neg: 0.0038\n",
      "epoch:    0 | step:   99 | N:  18 | test Sqc: 1.3838 | test Neg: 0.0001\n",
      "epoch:    0 | step:  199 | N:  18 | test Sqc: 1.3837 | test Neg: 0.0005\n",
      "epoch:    0 | step:   99 | N:  20 | test Sqc: 1.3875 | test Neg: -0.0004\n",
      "epoch:    0 | step:  199 | N:  20 | test Sqc: 1.3861 | test Neg: -0.0001\n",
      "epoch:    0 | step:   99 | N:  22 | test Sqc: 1.3845 | test Neg: 0.0003\n",
      "epoch:    0 | step:  199 | N:  22 | test Sqc: 1.3858 | test Neg: 0.0001\n",
      "epoch:    0 | step:   99 | N:  24 | test Sqc: 1.3870 | test Neg: 0.0000\n",
      "epoch:    0 | step:  199 | N:  24 | test Sqc: 1.3873 | test Neg: -0.0000\n",
      "epoch:    0 | step:   99 | N:  26 | test Sqc: 1.3863 | test Neg: -0.0000\n",
      "epoch:    0 | step:  199 | N:  26 | test Sqc: 1.3866 | test Neg: 0.0000\n",
      "epoch:    0 | step:   99 | N:  28 | test Sqc: 1.3871 | test Neg: 0.0000\n",
      "epoch:    0 | step:  199 | N:  28 | test Sqc: 1.3867 | test Neg: -0.0000\n",
      "epoch:    0 | step:   99 | N:  30 | test Sqc: 1.3865 | test Neg: 0.0000\n",
      "epoch:    0 | step:  199 | N:  30 | test Sqc: 1.3867 | test Neg: -0.0000\n",
      "epoch:    0 | step:   99 | N:  32 | test Sqc: 1.3860 | test Neg: 0.0000\n",
      "epoch:    0 | step:  199 | N:  32 | test Sqc: 1.3860 | test Neg: 0.0000\n",
      "epoch:    0 | step:   99 | N:  34 | test Sqc: 1.3869 | test Neg: 0.0000\n",
      "epoch:    0 | step:  199 | N:  34 | test Sqc: 1.3875 | test Neg: 0.0000\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | train Sqc: 1.1521 | loss: 1.3499\n",
      "epoch:    1 | step:  199 | train Sqc: 1.1501 | loss: 1.3492\n",
      "epoch:    1 | step:  299 | train Sqc: 1.1474 | loss: 1.3490\n",
      "epoch:    1 | step:  399 | train Sqc: 1.1487 | loss: 1.3491\n",
      "epoch:    1 | step:  499 | train Sqc: 1.1484 | loss: 1.3490\n",
      "epoch:    1 | step:  599 | train Sqc: 1.1456 | loss: 1.3487\n",
      "epoch:    1 | step:  699 | train Sqc: 1.1443 | loss: 1.3486\n",
      "epoch:    1 | step:  799 | train Sqc: 1.1415 | loss: 1.3483\n",
      "epoch:    1 | step:  899 | train Sqc: 1.1417 | loss: 1.3484\n",
      "epoch:    1 | step:  999 | train Sqc: 1.1402 | loss: 1.3483\n",
      "epoch:    1 | step:  1099 | train Sqc: 1.1396 | loss: 1.3483\n",
      "epoch:    1 | step:  1199 | train Sqc: 1.1400 | loss: 1.3483\n",
      "epoch:    1 | step:  1299 | train Sqc: 1.1410 | loss: 1.3484\n",
      "epoch:    1 | step:  1399 | train Sqc: 1.1397 | loss: 1.3482\n",
      "epoch:    1 | step:  1499 | train Sqc: 1.1399 | loss: 1.3481\n",
      "epoch:    1 | step:  1599 | train Sqc: 1.1387 | loss: 1.3480\n",
      "epoch:    1 | step:  1699 | train Sqc: 1.1393 | loss: 1.3481\n",
      "epoch:    1 | step:  1799 | train Sqc: 1.1386 | loss: 1.3480\n",
      "epoch:    1 | step:  1899 | train Sqc: 1.1382 | loss: 1.3480\n",
      "epoch:    1 | step:  1999 | train Sqc: 1.1373 | loss: 1.3478\n",
      "epoch:    1 | step:  2099 | train Sqc: 1.1370 | loss: 1.3477\n",
      "epoch:    1 | step:  2199 | train Sqc: 1.1364 | loss: 1.3476\n",
      "epoch:    1 | step:  2299 | train Sqc: 1.1351 | loss: 1.3475\n",
      "epoch:    1 | step:  2399 | train Sqc: 1.1352 | loss: 1.3475\n",
      "epoch:    1 | step:  2499 | train Sqc: 1.1348 | loss: 1.3475\n",
      "epoch:    1 | step:  2599 | train Sqc: 1.1345 | loss: 1.3474\n",
      "epoch:    1 | step:  2699 | train Sqc: 1.1342 | loss: 1.3474\n",
      "epoch:    1 | step:  2799 | train Sqc: 1.1335 | loss: 1.3473\n",
      "epoch:    1 | step:  2899 | train Sqc: 1.1331 | loss: 1.3472\n",
      "epoch:    1 | step:  2999 | train Sqc: 1.1331 | loss: 1.3472\n",
      "epoch:    1 | step:  3099 | train Sqc: 1.1328 | loss: 1.3471\n",
      "epoch:    1 | step:  3199 | train Sqc: 1.1320 | loss: 1.3470\n",
      "epoch:    1 | step:  3299 | train Sqc: 1.1314 | loss: 1.3469\n",
      "epoch:    1 | step:  3399 | train Sqc: 1.1304 | loss: 1.3468\n",
      "epoch:    1 | step:  3499 | train Sqc: 1.1304 | loss: 1.3468\n",
      "epoch:    1 | step:  3599 | train Sqc: 1.1304 | loss: 1.3468\n",
      "epoch:    1 | step:  3699 | train Sqc: 1.1300 | loss: 1.3468\n",
      "epoch:    1 | step:  3799 | train Sqc: 1.1300 | loss: 1.3467\n",
      "epoch:    1 | step:  3899 | train Sqc: 1.1300 | loss: 1.3467\n",
      "epoch:    1 | step:  3999 | train Sqc: 1.1300 | loss: 1.3467\n",
      "epoch:    1 | step:  4099 | train Sqc: 1.1292 | loss: 1.3466\n",
      "epoch:    1 | step:  4199 | train Sqc: 1.1286 | loss: 1.3465\n",
      "epoch:    1 | step:  4299 | train Sqc: 1.1285 | loss: 1.3465\n",
      "epoch:    1 | step:  4399 | train Sqc: 1.1287 | loss: 1.3465\n",
      "epoch:    1 | step:  4499 | train Sqc: 1.1277 | loss: 1.3464\n",
      "epoch:    1 | step:  4599 | train Sqc: 1.1277 | loss: 1.3464\n",
      "epoch:    1 | step:  4699 | train Sqc: 1.1273 | loss: 1.3463\n",
      "epoch:    1 | step:  4799 | train Sqc: 1.1269 | loss: 1.3463\n",
      "epoch:    1 | step:  4899 | train Sqc: 1.1264 | loss: 1.3462\n",
      "epoch:    1 | step:  4999 | train Sqc: 1.1261 | loss: 1.3461\n",
      "epoch:    1 | step:  5099 | train Sqc: 1.1258 | loss: 1.3461\n",
      "epoch:    1 | step:  5199 | train Sqc: 1.1255 | loss: 1.3461\n",
      "epoch:    1 | step:  5299 | train Sqc: 1.1256 | loss: 1.3461\n",
      "epoch:    1 | step:  5399 | train Sqc: 1.1256 | loss: 1.3461\n",
      "epoch:    1 | step:  5499 | train Sqc: 1.1256 | loss: 1.3461\n",
      "epoch:    1 | step:  5599 | train Sqc: 1.1259 | loss: 1.3461\n",
      "epoch:    1 | step:  5699 | train Sqc: 1.1257 | loss: 1.3461\n",
      "epoch:    1 | step:  5799 | train Sqc: 1.1254 | loss: 1.3461\n",
      "epoch:    1 | step:  5899 | train Sqc: 1.1249 | loss: 1.3460\n",
      "epoch:    1 | step:  5999 | train Sqc: 1.1246 | loss: 1.3460\n",
      "epoch:    1 | step:  6099 | train Sqc: 1.1245 | loss: 1.3459\n",
      "epoch:    1 | step:  6199 | train Sqc: 1.1242 | loss: 1.3459\n",
      "epoch:    1 | step:  6299 | train Sqc: 1.1240 | loss: 1.3459\n",
      "epoch:    1 | step:  6399 | train Sqc: 1.1241 | loss: 1.3459\n",
      "epoch:    1 | step:  6499 | train Sqc: 1.1239 | loss: 1.3458\n",
      "epoch:    1 | step:  6599 | train Sqc: 1.1237 | loss: 1.3458\n",
      "epoch:    1 | step:  6699 | train Sqc: 1.1234 | loss: 1.3458\n",
      "epoch:    1 | step:  6799 | train Sqc: 1.1231 | loss: 1.3457\n",
      "epoch:    1 | step:  6899 | train Sqc: 1.1230 | loss: 1.3457\n",
      "epoch:    1 | step:  6999 | train Sqc: 1.1227 | loss: 1.3456\n",
      "epoch:    1 | step:  7099 | train Sqc: 1.1225 | loss: 1.3456\n",
      "epoch:    1 | step:  7199 | train Sqc: 1.1221 | loss: 1.3456\n",
      "epoch:    1 | step:  7299 | train Sqc: 1.1219 | loss: 1.3455\n",
      "epoch:    1 | step:  7399 | train Sqc: 1.1217 | loss: 1.3455\n",
      "epoch:    1 | step:  7499 | train Sqc: 1.1215 | loss: 1.3454\n",
      "epoch:    1 | step:  7599 | train Sqc: 1.1214 | loss: 1.3454\n",
      "epoch:    1 | step:  7699 | train Sqc: 1.1212 | loss: 1.3454\n",
      "epoch:    1 | step:  7799 | train Sqc: 1.1209 | loss: 1.3454\n",
      "epoch:    1 | step:  7899 | train Sqc: 1.1207 | loss: 1.3454\n",
      "epoch:    1 | step:  7999 | train Sqc: 1.1206 | loss: 1.3453\n",
      "epoch:    1 | step:  8099 | train Sqc: 1.1203 | loss: 1.3453\n",
      "epoch:    1 | step:  8199 | train Sqc: 1.1201 | loss: 1.3453\n",
      "epoch:    1 | step:  8299 | train Sqc: 1.1198 | loss: 1.3452\n",
      "epoch:    1 | step:  8399 | train Sqc: 1.1199 | loss: 1.3452\n",
      "epoch:    1 | step:  8499 | train Sqc: 1.1199 | loss: 1.3452\n",
      "epoch:    1 | step:  8599 | train Sqc: 1.1194 | loss: 1.3452\n",
      "epoch:    1 | step:  8699 | train Sqc: 1.1192 | loss: 1.3451\n",
      "epoch:    1 | step:  8799 | train Sqc: 1.1191 | loss: 1.3451\n",
      "epoch:    1 | step:  8899 | train Sqc: 1.1188 | loss: 1.3451\n",
      "epoch:    1 | step:  8999 | train Sqc: 1.1187 | loss: 1.3450\n",
      "epoch:    1 | step:  9099 | train Sqc: 1.1186 | loss: 1.3450\n",
      "epoch:    1 | step:  9199 | train Sqc: 1.1183 | loss: 1.3450\n",
      "epoch:    1 | step:  9299 | train Sqc: 1.1182 | loss: 1.3450\n",
      "epoch:    1 | step:  9399 | train Sqc: 1.1181 | loss: 1.3449\n",
      "epoch:    1 | step:  9499 | train Sqc: 1.1178 | loss: 1.3449\n",
      "epoch:    1 | step:  9599 | train Sqc: 1.1178 | loss: 1.3449\n",
      "epoch:    1 | step:  9699 | train Sqc: 1.1176 | loss: 1.3449\n",
      "epoch:    1 | step:  9799 | train Sqc: 1.1175 | loss: 1.3449\n",
      "epoch:    1 | step:  9899 | train Sqc: 1.1175 | loss: 1.3448\n",
      "epoch:    1 | step:  9999 | train Sqc: 1.1173 | loss: 1.3448\n",
      "epoch:    1 | step:  10099 | train Sqc: 1.1171 | loss: 1.3448\n",
      "epoch:    1 | step:  10199 | train Sqc: 1.1170 | loss: 1.3448\n",
      "epoch:    1 | step:  10299 | train Sqc: 1.1169 | loss: 1.3448\n",
      "epoch:    1 | step:  10399 | train Sqc: 1.1167 | loss: 1.3448\n",
      "epoch:    1 | step:  10499 | train Sqc: 1.1167 | loss: 1.3447\n",
      "epoch:    1 | step:  10599 | train Sqc: 1.1165 | loss: 1.3447\n",
      "epoch:    1 | step:  10699 | train Sqc: 1.1163 | loss: 1.3447\n",
      "epoch:    1 | step:  10799 | train Sqc: 1.1161 | loss: 1.3446\n",
      "epoch:    1 | step:  10899 | train Sqc: 1.1159 | loss: 1.3446\n",
      "epoch:    1 | step:  10999 | train Sqc: 1.1158 | loss: 1.3446\n",
      "epoch:    1 | step:  11099 | train Sqc: 1.1156 | loss: 1.3446\n",
      "epoch:    1 | step:  11199 | train Sqc: 1.1155 | loss: 1.3446\n",
      "epoch:    1 | step:  11299 | train Sqc: 1.1153 | loss: 1.3445\n",
      "epoch:    1 | step:  11399 | train Sqc: 1.1152 | loss: 1.3445\n",
      "epoch:    1 | step:  11499 | train Sqc: 1.1151 | loss: 1.3445\n",
      "epoch:    1 | step:  11599 | train Sqc: 1.1150 | loss: 1.3445\n",
      "epoch:    1 | step:  11699 | train Sqc: 1.1149 | loss: 1.3445\n",
      "epoch:    1 | step:  11799 | train Sqc: 1.1148 | loss: 1.3444\n",
      "epoch:    1 | step:  11899 | train Sqc: 1.1147 | loss: 1.3444\n",
      "epoch:    1 | step:  11999 | train Sqc: 1.1145 | loss: 1.3444\n",
      "epoch:    1 | step:  12099 | train Sqc: 1.1142 | loss: 1.3444\n",
      "epoch:    1 | step:  12199 | train Sqc: 1.1141 | loss: 1.3443\n",
      "epoch:    1 | step:  12299 | train Sqc: 1.1141 | loss: 1.3443\n",
      "epoch:    1 | step:  12399 | train Sqc: 1.1140 | loss: 1.3443\n",
      "epoch:    1 | step:  12499 | train Sqc: 1.1136 | loss: 1.3443\n",
      "epoch:    1 | step:  12599 | train Sqc: 1.1134 | loss: 1.3442\n",
      "epoch:    1 | step:  12699 | train Sqc: 1.1134 | loss: 1.3442\n",
      "epoch:    1 | step:  12799 | train Sqc: 1.1132 | loss: 1.3442\n",
      "epoch:    1 | step:  12899 | train Sqc: 1.1131 | loss: 1.3442\n",
      "epoch:    1 | step:  12999 | train Sqc: 1.1129 | loss: 1.3442\n",
      "epoch:    1 | step:  13099 | train Sqc: 1.1128 | loss: 1.3442\n",
      "epoch:    1 | step:  13199 | train Sqc: 1.1126 | loss: 1.3441\n",
      "epoch:    1 | step:  13299 | train Sqc: 1.1126 | loss: 1.3441\n",
      "epoch:    1 | step:  13399 | train Sqc: 1.1126 | loss: 1.3441\n",
      "epoch:    1 | step:  13499 | train Sqc: 1.1124 | loss: 1.3441\n",
      "epoch:    1 | step:  13599 | train Sqc: 1.1123 | loss: 1.3441\n",
      "epoch:    1 | step:  13699 | train Sqc: 1.1123 | loss: 1.3441\n",
      "epoch:    1 | step:  13799 | train Sqc: 1.1122 | loss: 1.3441\n",
      "epoch:    1 | step:  13899 | train Sqc: 1.1121 | loss: 1.3441\n",
      "epoch:    1 | step:  13999 | train Sqc: 1.1121 | loss: 1.3441\n",
      "epoch:    1 | step:  14099 | train Sqc: 1.1119 | loss: 1.3440\n",
      "epoch:    1 | step:  14199 | train Sqc: 1.1117 | loss: 1.3440\n",
      "epoch:    1 | step:  14299 | train Sqc: 1.1117 | loss: 1.3440\n",
      "epoch:    1 | step:  14399 | train Sqc: 1.1115 | loss: 1.3440\n",
      "epoch:    1 | step:  14499 | train Sqc: 1.1113 | loss: 1.3439\n",
      "epoch:    1 | step:  14599 | train Sqc: 1.1110 | loss: 1.3439\n",
      "epoch:    1 | step:  14699 | train Sqc: 1.1108 | loss: 1.3439\n",
      "epoch:    1 | step:  14799 | train Sqc: 1.1108 | loss: 1.3439\n",
      "epoch:    1 | step:  14899 | train Sqc: 1.1107 | loss: 1.3439\n",
      "epoch:    1 | step:  14999 | train Sqc: 1.1105 | loss: 1.3438\n",
      "epoch:    1 | step:  15099 | train Sqc: 1.1104 | loss: 1.3438\n",
      "epoch:    1 | step:  15199 | train Sqc: 1.1102 | loss: 1.3438\n",
      "epoch:    1 | step:  15299 | train Sqc: 1.1100 | loss: 1.3438\n",
      "epoch:    1 | step:  15399 | train Sqc: 1.1098 | loss: 1.3437\n",
      "epoch:    1 | step:  15499 | train Sqc: 1.1096 | loss: 1.3437\n",
      "epoch:    1 | step:  15599 | train Sqc: 1.1094 | loss: 1.3437\n",
      "epoch:    1 | step:  15699 | train Sqc: 1.1091 | loss: 1.3436\n",
      "epoch:    1 | step:  15799 | train Sqc: 1.1090 | loss: 1.3436\n",
      "epoch:    1 | step:  15899 | train Sqc: 1.1086 | loss: 1.3436\n",
      "epoch:    1 | step:  15999 | train Sqc: 1.1084 | loss: 1.3435\n",
      "epoch:    1 | step:  16099 | train Sqc: 1.1082 | loss: 1.3435\n",
      "epoch:    1 | step:  16199 | train Sqc: 1.1081 | loss: 1.3435\n",
      "epoch:    1 | step:  16299 | train Sqc: 1.1079 | loss: 1.3435\n",
      "epoch:    1 | step:  16399 | train Sqc: 1.1078 | loss: 1.3434\n",
      "epoch:    1 | step:  16499 | train Sqc: 1.1076 | loss: 1.3434\n",
      "epoch:    1 | step:  16599 | train Sqc: 1.1073 | loss: 1.3434\n",
      "epoch:    1 | step:  16699 | train Sqc: 1.1071 | loss: 1.3433\n",
      "epoch:    1 | step:  16799 | train Sqc: 1.1070 | loss: 1.3433\n",
      "epoch:    1 | step:  16899 | train Sqc: 1.1068 | loss: 1.3433\n",
      "epoch:    1 | step:  16999 | train Sqc: 1.1065 | loss: 1.3433\n",
      "epoch:    1 | step:  17099 | train Sqc: 1.1062 | loss: 1.3432\n",
      "epoch:    1 | step:  17199 | train Sqc: 1.1061 | loss: 1.3432\n",
      "epoch:    1 | step:  17299 | train Sqc: 1.1058 | loss: 1.3432\n",
      "epoch:    1 | step:  17399 | train Sqc: 1.1057 | loss: 1.3432\n",
      "epoch:    1 | step:  17499 | train Sqc: 1.1056 | loss: 1.3431\n",
      "epoch:    1 | step:  17599 | train Sqc: 1.1053 | loss: 1.3431\n",
      "epoch:    1 | step:  17699 | train Sqc: 1.1051 | loss: 1.3431\n",
      "epoch:    1 | step:  17799 | train Sqc: 1.1049 | loss: 1.3430\n",
      "epoch:    1 | step:  17899 | train Sqc: 1.1046 | loss: 1.3430\n",
      "epoch:    1 | step:  17999 | train Sqc: 1.1044 | loss: 1.3430\n",
      "epoch:    1 | step:  18099 | train Sqc: 1.1041 | loss: 1.3429\n",
      "epoch:    1 | step:  18199 | train Sqc: 1.1040 | loss: 1.3429\n",
      "epoch:    1 | step:  18299 | train Sqc: 1.1037 | loss: 1.3429\n",
      "epoch:    1 | step:  18399 | train Sqc: 1.1035 | loss: 1.3428\n",
      "epoch:    1 | step:  18499 | train Sqc: 1.1031 | loss: 1.3428\n",
      "epoch:    1 | step:  18599 | train Sqc: 1.1029 | loss: 1.3428\n",
      "epoch:    1 | step:  18699 | train Sqc: 1.1027 | loss: 1.3427\n",
      "epoch:    1 | step:  18799 | train Sqc: 1.1024 | loss: 1.3427\n",
      "epoch:    1 | step:  18899 | train Sqc: 1.1023 | loss: 1.3427\n",
      "epoch:    1 | step:  18999 | train Sqc: 1.1020 | loss: 1.3426\n",
      "epoch:    1 | step:  19099 | train Sqc: 1.1018 | loss: 1.3426\n",
      "epoch:    1 | step:  19199 | train Sqc: 1.1016 | loss: 1.3426\n",
      "epoch:    1 | step:  19299 | train Sqc: 1.1013 | loss: 1.3425\n",
      "epoch:    1 | step:  19399 | train Sqc: 1.1011 | loss: 1.3425\n",
      "epoch:    1 | step:  19499 | train Sqc: 1.1007 | loss: 1.3425\n",
      "epoch:    1 | step:  19599 | train Sqc: 1.1004 | loss: 1.3424\n",
      "epoch:    1 | step:  19699 | train Sqc: 1.1002 | loss: 1.3424\n",
      "epoch:    1 | step:  19799 | train Sqc: 1.0998 | loss: 1.3423\n",
      "epoch:    1 | step:  19899 | train Sqc: 1.0995 | loss: 1.3423\n",
      "epoch:    1 | step:  19999 | train Sqc: 1.0992 | loss: 1.3422\n",
      "epoch:    1 | step:  20099 | train Sqc: 1.0988 | loss: 1.3422\n",
      "epoch:    1 | step:  20199 | train Sqc: 1.0985 | loss: 1.3422\n",
      "epoch:    1 | step:  20299 | train Sqc: 1.0981 | loss: 1.3421\n",
      "epoch:    1 | step:  20399 | train Sqc: 1.0978 | loss: 1.3421\n",
      "epoch:    1 | step:  20499 | train Sqc: 1.0975 | loss: 1.3420\n",
      "epoch:    1 | step:  20599 | train Sqc: 1.0972 | loss: 1.3420\n",
      "epoch:    1 | step:  20699 | train Sqc: 1.0969 | loss: 1.3419\n",
      "epoch:    1 | step:  20799 | train Sqc: 1.0966 | loss: 1.3419\n",
      "epoch:    1 | step:  20899 | train Sqc: 1.0964 | loss: 1.3419\n",
      "epoch:    1 | step:  20999 | train Sqc: 1.0960 | loss: 1.3418\n",
      "epoch:    1 | step:  21099 | train Sqc: 1.0956 | loss: 1.3418\n",
      "epoch:    1 | step:  21199 | train Sqc: 1.0953 | loss: 1.3417\n",
      "epoch:    1 | step:  21299 | train Sqc: 1.0950 | loss: 1.3417\n",
      "epoch:    1 | step:  21399 | train Sqc: 1.0947 | loss: 1.3416\n",
      "epoch:    1 | step:  21499 | train Sqc: 1.0945 | loss: 1.3416\n",
      "epoch:    1 | step:  21599 | train Sqc: 1.0942 | loss: 1.3416\n",
      "epoch:    1 | step:  21699 | train Sqc: 1.0938 | loss: 1.3415\n",
      "epoch:    1 | step:  21799 | train Sqc: 1.0935 | loss: 1.3415\n",
      "epoch:    1 | step:  21899 | train Sqc: 1.0931 | loss: 1.3414\n",
      "epoch:    1 | step:  21999 | train Sqc: 1.0928 | loss: 1.3414\n",
      "epoch:    1 | step:  22099 | train Sqc: 1.0925 | loss: 1.3413\n",
      "epoch:    1 | step:  22199 | train Sqc: 1.0922 | loss: 1.3413\n",
      "epoch:    1 | step:  22299 | train Sqc: 1.0917 | loss: 1.3412\n",
      "epoch:    1 | step:  22399 | train Sqc: 1.0913 | loss: 1.3412\n",
      "epoch:    1 | step:  22499 | train Sqc: 1.0909 | loss: 1.3411\n",
      "epoch:    1 | step:  22599 | train Sqc: 1.0905 | loss: 1.3411\n",
      "epoch:    1 | step:  22699 | train Sqc: 1.0902 | loss: 1.3410\n",
      "epoch:    1 | step:  22799 | train Sqc: 1.0897 | loss: 1.3409\n",
      "epoch:    1 | step:  22899 | train Sqc: 1.0893 | loss: 1.3409\n",
      "epoch:    1 | step:  22999 | train Sqc: 1.0890 | loss: 1.3408\n",
      "epoch:    1 | step:  23099 | train Sqc: 1.0886 | loss: 1.3408\n",
      "epoch:    1 | step:  23199 | train Sqc: 1.0882 | loss: 1.3407\n",
      "epoch:    1 | step:  23299 | train Sqc: 1.0877 | loss: 1.3407\n",
      "epoch:    1 | step:  23399 | train Sqc: 1.0872 | loss: 1.3406\n",
      "epoch:    1 | step:  23499 | train Sqc: 1.0868 | loss: 1.3405\n",
      "epoch:    1 | step:  23599 | train Sqc: 1.0864 | loss: 1.3405\n",
      "epoch:    1 | step:  23699 | train Sqc: 1.0859 | loss: 1.3404\n",
      "epoch:    1 | step:  23799 | train Sqc: 1.0855 | loss: 1.3404\n",
      "epoch:    1 | step:  23899 | train Sqc: 1.0851 | loss: 1.3403\n",
      "epoch:    1 | step:  23999 | train Sqc: 1.0848 | loss: 1.3403\n",
      "epoch:    1 | step:  24099 | train Sqc: 1.0844 | loss: 1.3402\n",
      "epoch:    1 | step:  24199 | train Sqc: 1.0842 | loss: 1.3402\n",
      "epoch:    1 | step:  24299 | train Sqc: 1.0840 | loss: 1.3401\n",
      "epoch:    1 | step:  24399 | train Sqc: 1.0836 | loss: 1.3401\n",
      "epoch:    1 | step:  24499 | train Sqc: 1.0832 | loss: 1.3400\n",
      "epoch:    1 | step:  24599 | train Sqc: 1.0828 | loss: 1.3400\n",
      "epoch:    1 | step:  24699 | train Sqc: 1.0824 | loss: 1.3399\n",
      "epoch:    1 | step:  24799 | train Sqc: 1.0820 | loss: 1.3399\n",
      "epoch:    1 | step:  24899 | train Sqc: 1.0815 | loss: 1.3398\n",
      "epoch:    1 | step:  24999 | train Sqc: 1.0813 | loss: 1.3398\n",
      "epoch:    1 | step:  25099 | train Sqc: 1.0808 | loss: 1.3397\n",
      "epoch:    1 | step:  25199 | train Sqc: 1.0804 | loss: 1.3397\n",
      "epoch:    1 | step:  25299 | train Sqc: 1.0800 | loss: 1.3396\n",
      "epoch:    1 | step:  25399 | train Sqc: 1.0795 | loss: 1.3395\n",
      "epoch:    1 | step:  25499 | train Sqc: 1.0790 | loss: 1.3395\n",
      "epoch:    1 | step:  25599 | train Sqc: 1.0786 | loss: 1.3394\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3554 | test Neg: 0.4202\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3734 | test Neg: 0.4147\n",
      "epoch:    1 | step:   99 | N:  6 | test Sqc: 0.4096 | test Neg: 0.4072\n",
      "epoch:    1 | step:  199 | N:  6 | test Sqc: 0.4202 | test Neg: 0.4025\n",
      "epoch:    1 | step:   99 | N:  8 | test Sqc: 0.5301 | test Neg: 0.3679\n",
      "epoch:    1 | step:  199 | N:  8 | test Sqc: 0.5321 | test Neg: 0.3676\n",
      "epoch:    1 | step:   99 | N:  10 | test Sqc: 0.6412 | test Neg: 0.3246\n",
      "epoch:    1 | step:  199 | N:  10 | test Sqc: 0.6435 | test Neg: 0.3228\n",
      "epoch:    1 | step:   99 | N:  12 | test Sqc: 0.7333 | test Neg: 0.2904\n",
      "epoch:    1 | step:  199 | N:  12 | test Sqc: 0.7382 | test Neg: 0.2886\n",
      "epoch:    1 | step:   99 | N:  14 | test Sqc: 0.7595 | test Neg: 0.2786\n",
      "epoch:    1 | step:  199 | N:  14 | test Sqc: 0.7759 | test Neg: 0.2709\n",
      "epoch:    1 | step:   99 | N:  16 | test Sqc: 0.8607 | test Neg: 0.2334\n",
      "epoch:    1 | step:  199 | N:  16 | test Sqc: 0.8659 | test Neg: 0.2287\n",
      "epoch:    1 | step:   99 | N:  18 | test Sqc: 0.9129 | test Neg: 0.2035\n",
      "epoch:    1 | step:  199 | N:  18 | test Sqc: 0.9125 | test Neg: 0.2023\n",
      "epoch:    1 | step:   99 | N:  20 | test Sqc: 1.0318 | test Neg: 0.1277\n",
      "epoch:    1 | step:  199 | N:  20 | test Sqc: 1.0335 | test Neg: 0.1265\n",
      "epoch:    1 | step:   99 | N:  22 | test Sqc: 1.1645 | test Neg: 0.0498\n",
      "epoch:    1 | step:  199 | N:  22 | test Sqc: 1.1649 | test Neg: 0.0496\n",
      "epoch:    1 | step:   99 | N:  24 | test Sqc: 1.2762 | test Neg: 0.0114\n",
      "epoch:    1 | step:  199 | N:  24 | test Sqc: 1.2722 | test Neg: 0.0118\n",
      "epoch:    1 | step:   99 | N:  26 | test Sqc: 1.3378 | test Neg: 0.0000\n",
      "epoch:    1 | step:  199 | N:  26 | test Sqc: 1.3409 | test Neg: 0.0012\n",
      "epoch:    1 | step:   99 | N:  28 | test Sqc: 1.3618 | test Neg: -0.0009\n",
      "epoch:    1 | step:  199 | N:  28 | test Sqc: 1.3658 | test Neg: -0.0004\n",
      "epoch:    1 | step:   99 | N:  30 | test Sqc: 1.3828 | test Neg: 0.0005\n",
      "epoch:    1 | step:  199 | N:  30 | test Sqc: 1.3836 | test Neg: -0.0000\n",
      "epoch:    1 | step:   99 | N:  32 | test Sqc: 1.3870 | test Neg: -0.0004\n",
      "epoch:    1 | step:  199 | N:  32 | test Sqc: 1.3881 | test Neg: -0.0002\n",
      "epoch:    1 | step:   99 | N:  34 | test Sqc: 1.3885 | test Neg: -0.0002\n",
      "epoch:    1 | step:  199 | N:  34 | test Sqc: 1.3882 | test Neg: -0.0000\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | train Sqc: 0.9745 | loss: 1.3251\n",
      "epoch:    2 | step:  199 | train Sqc: 0.9680 | loss: 1.3241\n",
      "epoch:    2 | step:  299 | train Sqc: 0.9615 | loss: 1.3234\n",
      "epoch:    2 | step:  399 | train Sqc: 0.9639 | loss: 1.3235\n",
      "epoch:    2 | step:  499 | train Sqc: 0.9674 | loss: 1.3239\n",
      "epoch:    2 | step:  599 | train Sqc: 0.9693 | loss: 1.3243\n",
      "epoch:    2 | step:  699 | train Sqc: 0.9725 | loss: 1.3247\n",
      "epoch:    2 | step:  799 | train Sqc: 0.9699 | loss: 1.3245\n",
      "epoch:    2 | step:  899 | train Sqc: 0.9676 | loss: 1.3241\n",
      "epoch:    2 | step:  999 | train Sqc: 0.9654 | loss: 1.3239\n",
      "epoch:    2 | step:  1099 | train Sqc: 0.9660 | loss: 1.3240\n",
      "epoch:    2 | step:  1199 | train Sqc: 0.9666 | loss: 1.3241\n",
      "epoch:    2 | step:  1299 | train Sqc: 0.9685 | loss: 1.3244\n",
      "epoch:    2 | step:  1399 | train Sqc: 0.9718 | loss: 1.3249\n",
      "epoch:    2 | step:  1499 | train Sqc: 0.9720 | loss: 1.3248\n",
      "epoch:    2 | step:  1599 | train Sqc: 0.9705 | loss: 1.3246\n",
      "epoch:    2 | step:  1699 | train Sqc: 0.9700 | loss: 1.3245\n",
      "epoch:    2 | step:  1799 | train Sqc: 0.9695 | loss: 1.3244\n",
      "epoch:    2 | step:  1899 | train Sqc: 0.9690 | loss: 1.3243\n",
      "epoch:    2 | step:  1999 | train Sqc: 0.9681 | loss: 1.3241\n",
      "epoch:    2 | step:  2099 | train Sqc: 0.9681 | loss: 1.3241\n",
      "epoch:    2 | step:  2199 | train Sqc: 0.9674 | loss: 1.3239\n",
      "epoch:    2 | step:  2299 | train Sqc: 0.9665 | loss: 1.3238\n",
      "epoch:    2 | step:  2399 | train Sqc: 0.9663 | loss: 1.3238\n",
      "epoch:    2 | step:  2499 | train Sqc: 0.9656 | loss: 1.3237\n",
      "epoch:    2 | step:  2599 | train Sqc: 0.9653 | loss: 1.3237\n",
      "epoch:    2 | step:  2699 | train Sqc: 0.9649 | loss: 1.3236\n",
      "epoch:    2 | step:  2799 | train Sqc: 0.9639 | loss: 1.3235\n",
      "epoch:    2 | step:  2899 | train Sqc: 0.9634 | loss: 1.3234\n",
      "epoch:    2 | step:  2999 | train Sqc: 0.9632 | loss: 1.3233\n",
      "epoch:    2 | step:  3099 | train Sqc: 0.9628 | loss: 1.3233\n",
      "epoch:    2 | step:  3199 | train Sqc: 0.9621 | loss: 1.3232\n",
      "epoch:    2 | step:  3299 | train Sqc: 0.9617 | loss: 1.3231\n",
      "epoch:    2 | step:  3399 | train Sqc: 0.9608 | loss: 1.3230\n",
      "epoch:    2 | step:  3499 | train Sqc: 0.9606 | loss: 1.3230\n",
      "epoch:    2 | step:  3599 | train Sqc: 0.9608 | loss: 1.3230\n",
      "epoch:    2 | step:  3699 | train Sqc: 0.9601 | loss: 1.3229\n",
      "epoch:    2 | step:  3799 | train Sqc: 0.9602 | loss: 1.3229\n",
      "epoch:    2 | step:  3899 | train Sqc: 0.9606 | loss: 1.3230\n",
      "epoch:    2 | step:  3999 | train Sqc: 0.9605 | loss: 1.3229\n",
      "epoch:    2 | step:  4099 | train Sqc: 0.9593 | loss: 1.3228\n",
      "epoch:    2 | step:  4199 | train Sqc: 0.9585 | loss: 1.3227\n",
      "epoch:    2 | step:  4299 | train Sqc: 0.9585 | loss: 1.3227\n",
      "epoch:    2 | step:  4399 | train Sqc: 0.9588 | loss: 1.3227\n",
      "epoch:    2 | step:  4499 | train Sqc: 0.9581 | loss: 1.3226\n",
      "epoch:    2 | step:  4599 | train Sqc: 0.9581 | loss: 1.3226\n",
      "epoch:    2 | step:  4699 | train Sqc: 0.9577 | loss: 1.3226\n",
      "epoch:    2 | step:  4799 | train Sqc: 0.9575 | loss: 1.3225\n",
      "epoch:    2 | step:  4899 | train Sqc: 0.9569 | loss: 1.3224\n",
      "epoch:    2 | step:  4999 | train Sqc: 0.9567 | loss: 1.3224\n",
      "epoch:    2 | step:  5099 | train Sqc: 0.9563 | loss: 1.3223\n",
      "epoch:    2 | step:  5199 | train Sqc: 0.9560 | loss: 1.3223\n",
      "epoch:    2 | step:  5299 | train Sqc: 0.9559 | loss: 1.3223\n",
      "epoch:    2 | step:  5399 | train Sqc: 0.9558 | loss: 1.3223\n",
      "epoch:    2 | step:  5499 | train Sqc: 0.9557 | loss: 1.3222\n",
      "epoch:    2 | step:  5599 | train Sqc: 0.9561 | loss: 1.3223\n",
      "epoch:    2 | step:  5699 | train Sqc: 0.9557 | loss: 1.3223\n",
      "epoch:    2 | step:  5799 | train Sqc: 0.9556 | loss: 1.3222\n",
      "epoch:    2 | step:  5899 | train Sqc: 0.9553 | loss: 1.3222\n",
      "epoch:    2 | step:  5999 | train Sqc: 0.9551 | loss: 1.3222\n",
      "epoch:    2 | step:  6099 | train Sqc: 0.9548 | loss: 1.3221\n",
      "epoch:    2 | step:  6199 | train Sqc: 0.9543 | loss: 1.3221\n",
      "epoch:    2 | step:  6299 | train Sqc: 0.9542 | loss: 1.3220\n",
      "epoch:    2 | step:  6399 | train Sqc: 0.9540 | loss: 1.3220\n",
      "epoch:    2 | step:  6499 | train Sqc: 0.9537 | loss: 1.3220\n",
      "epoch:    2 | step:  6599 | train Sqc: 0.9534 | loss: 1.3219\n",
      "epoch:    2 | step:  6699 | train Sqc: 0.9531 | loss: 1.3219\n",
      "epoch:    2 | step:  6799 | train Sqc: 0.9528 | loss: 1.3218\n",
      "epoch:    2 | step:  6899 | train Sqc: 0.9523 | loss: 1.3218\n",
      "epoch:    2 | step:  6999 | train Sqc: 0.9515 | loss: 1.3216\n",
      "epoch:    2 | step:  7099 | train Sqc: 0.9513 | loss: 1.3216\n",
      "epoch:    2 | step:  7199 | train Sqc: 0.9506 | loss: 1.3215\n",
      "epoch:    2 | step:  7299 | train Sqc: 0.9503 | loss: 1.3215\n",
      "epoch:    2 | step:  7399 | train Sqc: 0.9501 | loss: 1.3214\n",
      "epoch:    2 | step:  7499 | train Sqc: 0.9498 | loss: 1.3214\n",
      "epoch:    2 | step:  7599 | train Sqc: 0.9498 | loss: 1.3214\n",
      "epoch:    2 | step:  7699 | train Sqc: 0.9493 | loss: 1.3213\n",
      "epoch:    2 | step:  7799 | train Sqc: 0.9488 | loss: 1.3213\n",
      "epoch:    2 | step:  7899 | train Sqc: 0.9487 | loss: 1.3213\n",
      "epoch:    2 | step:  7999 | train Sqc: 0.9484 | loss: 1.3212\n",
      "epoch:    2 | step:  8099 | train Sqc: 0.9479 | loss: 1.3212\n",
      "epoch:    2 | step:  8199 | train Sqc: 0.9477 | loss: 1.3211\n",
      "epoch:    2 | step:  8299 | train Sqc: 0.9473 | loss: 1.3211\n",
      "epoch:    2 | step:  8399 | train Sqc: 0.9471 | loss: 1.3211\n",
      "epoch:    2 | step:  8499 | train Sqc: 0.9470 | loss: 1.3210\n",
      "epoch:    2 | step:  8599 | train Sqc: 0.9465 | loss: 1.3210\n",
      "epoch:    2 | step:  8699 | train Sqc: 0.9463 | loss: 1.3210\n",
      "epoch:    2 | step:  8799 | train Sqc: 0.9461 | loss: 1.3209\n",
      "epoch:    2 | step:  8899 | train Sqc: 0.9458 | loss: 1.3209\n",
      "epoch:    2 | step:  8999 | train Sqc: 0.9455 | loss: 1.3208\n",
      "epoch:    2 | step:  9099 | train Sqc: 0.9453 | loss: 1.3208\n",
      "epoch:    2 | step:  9199 | train Sqc: 0.9448 | loss: 1.3207\n",
      "epoch:    2 | step:  9299 | train Sqc: 0.9447 | loss: 1.3207\n",
      "epoch:    2 | step:  9399 | train Sqc: 0.9444 | loss: 1.3207\n",
      "epoch:    2 | step:  9499 | train Sqc: 0.9439 | loss: 1.3206\n",
      "epoch:    2 | step:  9599 | train Sqc: 0.9437 | loss: 1.3206\n",
      "epoch:    2 | step:  9699 | train Sqc: 0.9435 | loss: 1.3205\n",
      "epoch:    2 | step:  9799 | train Sqc: 0.9433 | loss: 1.3205\n",
      "epoch:    2 | step:  9899 | train Sqc: 0.9431 | loss: 1.3205\n",
      "epoch:    2 | step:  9999 | train Sqc: 0.9429 | loss: 1.3205\n",
      "epoch:    2 | step:  10099 | train Sqc: 0.9426 | loss: 1.3204\n",
      "epoch:    2 | step:  10199 | train Sqc: 0.9425 | loss: 1.3204\n",
      "epoch:    2 | step:  10299 | train Sqc: 0.9425 | loss: 1.3204\n",
      "epoch:    2 | step:  10399 | train Sqc: 0.9426 | loss: 1.3204\n",
      "epoch:    2 | step:  10499 | train Sqc: 0.9428 | loss: 1.3204\n",
      "epoch:    2 | step:  10599 | train Sqc: 0.9427 | loss: 1.3204\n",
      "epoch:    2 | step:  10699 | train Sqc: 0.9424 | loss: 1.3204\n",
      "epoch:    2 | step:  10799 | train Sqc: 0.9420 | loss: 1.3203\n",
      "epoch:    2 | step:  10899 | train Sqc: 0.9418 | loss: 1.3203\n",
      "epoch:    2 | step:  10999 | train Sqc: 0.9417 | loss: 1.3203\n",
      "epoch:    2 | step:  11099 | train Sqc: 0.9416 | loss: 1.3203\n",
      "epoch:    2 | step:  11199 | train Sqc: 0.9414 | loss: 1.3203\n",
      "epoch:    2 | step:  11299 | train Sqc: 0.9413 | loss: 1.3203\n",
      "epoch:    2 | step:  11399 | train Sqc: 0.9411 | loss: 1.3202\n",
      "epoch:    2 | step:  11499 | train Sqc: 0.9411 | loss: 1.3202\n",
      "epoch:    2 | step:  11599 | train Sqc: 0.9408 | loss: 1.3202\n",
      "epoch:    2 | step:  11699 | train Sqc: 0.9407 | loss: 1.3202\n",
      "epoch:    2 | step:  11799 | train Sqc: 0.9404 | loss: 1.3201\n",
      "epoch:    2 | step:  11899 | train Sqc: 0.9402 | loss: 1.3201\n",
      "epoch:    2 | step:  11999 | train Sqc: 0.9398 | loss: 1.3200\n",
      "epoch:    2 | step:  12099 | train Sqc: 0.9395 | loss: 1.3200\n",
      "epoch:    2 | step:  12199 | train Sqc: 0.9393 | loss: 1.3200\n",
      "epoch:    2 | step:  12299 | train Sqc: 0.9391 | loss: 1.3199\n",
      "epoch:    2 | step:  12399 | train Sqc: 0.9388 | loss: 1.3199\n",
      "epoch:    2 | step:  12499 | train Sqc: 0.9384 | loss: 1.3198\n",
      "epoch:    2 | step:  12599 | train Sqc: 0.9378 | loss: 1.3198\n",
      "epoch:    2 | step:  12699 | train Sqc: 0.9377 | loss: 1.3197\n",
      "epoch:    2 | step:  12799 | train Sqc: 0.9374 | loss: 1.3197\n",
      "epoch:    2 | step:  12899 | train Sqc: 0.9374 | loss: 1.3197\n",
      "epoch:    2 | step:  12999 | train Sqc: 0.9371 | loss: 1.3197\n",
      "epoch:    2 | step:  13099 | train Sqc: 0.9369 | loss: 1.3196\n",
      "epoch:    2 | step:  13199 | train Sqc: 0.9365 | loss: 1.3196\n",
      "epoch:    2 | step:  13299 | train Sqc: 0.9361 | loss: 1.3195\n",
      "epoch:    2 | step:  13399 | train Sqc: 0.9358 | loss: 1.3195\n",
      "epoch:    2 | step:  13499 | train Sqc: 0.9354 | loss: 1.3194\n",
      "epoch:    2 | step:  13599 | train Sqc: 0.9352 | loss: 1.3194\n",
      "epoch:    2 | step:  13699 | train Sqc: 0.9349 | loss: 1.3194\n",
      "epoch:    2 | step:  13799 | train Sqc: 0.9345 | loss: 1.3193\n",
      "epoch:    2 | step:  13899 | train Sqc: 0.9343 | loss: 1.3193\n",
      "epoch:    2 | step:  13999 | train Sqc: 0.9340 | loss: 1.3192\n",
      "epoch:    2 | step:  14099 | train Sqc: 0.9335 | loss: 1.3192\n",
      "epoch:    2 | step:  14199 | train Sqc: 0.9332 | loss: 1.3191\n",
      "epoch:    2 | step:  14299 | train Sqc: 0.9329 | loss: 1.3191\n",
      "epoch:    2 | step:  14399 | train Sqc: 0.9325 | loss: 1.3191\n",
      "epoch:    2 | step:  14499 | train Sqc: 0.9322 | loss: 1.3190\n",
      "epoch:    2 | step:  14599 | train Sqc: 0.9319 | loss: 1.3190\n",
      "epoch:    2 | step:  14699 | train Sqc: 0.9316 | loss: 1.3189\n",
      "epoch:    2 | step:  14799 | train Sqc: 0.9316 | loss: 1.3189\n",
      "epoch:    2 | step:  14899 | train Sqc: 0.9315 | loss: 1.3189\n",
      "epoch:    2 | step:  14999 | train Sqc: 0.9313 | loss: 1.3189\n",
      "epoch:    2 | step:  15099 | train Sqc: 0.9311 | loss: 1.3189\n",
      "epoch:    2 | step:  15199 | train Sqc: 0.9308 | loss: 1.3188\n",
      "epoch:    2 | step:  15299 | train Sqc: 0.9306 | loss: 1.3188\n",
      "epoch:    2 | step:  15399 | train Sqc: 0.9303 | loss: 1.3188\n",
      "epoch:    2 | step:  15499 | train Sqc: 0.9300 | loss: 1.3187\n",
      "epoch:    2 | step:  15599 | train Sqc: 0.9298 | loss: 1.3187\n",
      "epoch:    2 | step:  15699 | train Sqc: 0.9295 | loss: 1.3186\n",
      "epoch:    2 | step:  15799 | train Sqc: 0.9295 | loss: 1.3186\n",
      "epoch:    2 | step:  15899 | train Sqc: 0.9292 | loss: 1.3186\n",
      "epoch:    2 | step:  15999 | train Sqc: 0.9291 | loss: 1.3186\n",
      "epoch:    2 | step:  16099 | train Sqc: 0.9290 | loss: 1.3186\n",
      "epoch:    2 | step:  16199 | train Sqc: 0.9290 | loss: 1.3185\n",
      "epoch:    2 | step:  16299 | train Sqc: 0.9288 | loss: 1.3185\n",
      "epoch:    2 | step:  16399 | train Sqc: 0.9287 | loss: 1.3185\n",
      "epoch:    2 | step:  16499 | train Sqc: 0.9287 | loss: 1.3185\n",
      "epoch:    2 | step:  16599 | train Sqc: 0.9285 | loss: 1.3185\n",
      "epoch:    2 | step:  16699 | train Sqc: 0.9284 | loss: 1.3185\n",
      "epoch:    2 | step:  16799 | train Sqc: 0.9282 | loss: 1.3184\n",
      "epoch:    2 | step:  16899 | train Sqc: 0.9280 | loss: 1.3184\n",
      "epoch:    2 | step:  16999 | train Sqc: 0.9278 | loss: 1.3184\n",
      "epoch:    2 | step:  17099 | train Sqc: 0.9274 | loss: 1.3183\n",
      "epoch:    2 | step:  17199 | train Sqc: 0.9272 | loss: 1.3183\n",
      "epoch:    2 | step:  17299 | train Sqc: 0.9268 | loss: 1.3183\n",
      "epoch:    2 | step:  17399 | train Sqc: 0.9277 | loss: 1.3184\n",
      "epoch:    2 | step:  17499 | train Sqc: 0.9277 | loss: 1.3184\n",
      "epoch:    2 | step:  17599 | train Sqc: 0.9275 | loss: 1.3184\n",
      "epoch:    2 | step:  17699 | train Sqc: 0.9273 | loss: 1.3184\n",
      "epoch:    2 | step:  17799 | train Sqc: 0.9272 | loss: 1.3183\n",
      "epoch:    2 | step:  17899 | train Sqc: 0.9272 | loss: 1.3183\n",
      "epoch:    2 | step:  17999 | train Sqc: 0.9270 | loss: 1.3183\n",
      "epoch:    2 | step:  18099 | train Sqc: 0.9268 | loss: 1.3183\n",
      "epoch:    2 | step:  18199 | train Sqc: 0.9268 | loss: 1.3183\n",
      "epoch:    2 | step:  18299 | train Sqc: 0.9266 | loss: 1.3183\n",
      "epoch:    2 | step:  18399 | train Sqc: 0.9264 | loss: 1.3182\n",
      "epoch:    2 | step:  18499 | train Sqc: 0.9261 | loss: 1.3182\n",
      "epoch:    2 | step:  18599 | train Sqc: 0.9259 | loss: 1.3182\n",
      "epoch:    2 | step:  18699 | train Sqc: 0.9257 | loss: 1.3181\n",
      "epoch:    2 | step:  18799 | train Sqc: 0.9254 | loss: 1.3181\n",
      "epoch:    2 | step:  18899 | train Sqc: 0.9252 | loss: 1.3181\n",
      "epoch:    2 | step:  18999 | train Sqc: 0.9251 | loss: 1.3181\n",
      "epoch:    2 | step:  19099 | train Sqc: 0.9250 | loss: 1.3180\n",
      "epoch:    2 | step:  19199 | train Sqc: 0.9248 | loss: 1.3180\n",
      "epoch:    2 | step:  19299 | train Sqc: 0.9246 | loss: 1.3180\n",
      "epoch:    2 | step:  19399 | train Sqc: 0.9245 | loss: 1.3180\n",
      "epoch:    2 | step:  19499 | train Sqc: 0.9242 | loss: 1.3179\n",
      "epoch:    2 | step:  19599 | train Sqc: 0.9240 | loss: 1.3179\n",
      "epoch:    2 | step:  19699 | train Sqc: 0.9239 | loss: 1.3179\n",
      "epoch:    2 | step:  19799 | train Sqc: 0.9237 | loss: 1.3179\n",
      "epoch:    2 | step:  19899 | train Sqc: 0.9234 | loss: 1.3178\n",
      "epoch:    2 | step:  19999 | train Sqc: 0.9232 | loss: 1.3178\n",
      "epoch:    2 | step:  20099 | train Sqc: 0.9230 | loss: 1.3178\n",
      "epoch:    2 | step:  20199 | train Sqc: 0.9228 | loss: 1.3177\n",
      "epoch:    2 | step:  20299 | train Sqc: 0.9225 | loss: 1.3177\n",
      "epoch:    2 | step:  20399 | train Sqc: 0.9223 | loss: 1.3177\n",
      "epoch:    2 | step:  20499 | train Sqc: 0.9221 | loss: 1.3177\n",
      "epoch:    2 | step:  20599 | train Sqc: 0.9219 | loss: 1.3176\n",
      "epoch:    2 | step:  20699 | train Sqc: 0.9219 | loss: 1.3176\n",
      "epoch:    2 | step:  20799 | train Sqc: 0.9217 | loss: 1.3176\n",
      "epoch:    2 | step:  20899 | train Sqc: 0.9216 | loss: 1.3176\n",
      "epoch:    2 | step:  20999 | train Sqc: 0.9213 | loss: 1.3176\n",
      "epoch:    2 | step:  21099 | train Sqc: 0.9211 | loss: 1.3175\n",
      "epoch:    2 | step:  21199 | train Sqc: 0.9209 | loss: 1.3175\n",
      "epoch:    2 | step:  21299 | train Sqc: 0.9207 | loss: 1.3175\n",
      "epoch:    2 | step:  21399 | train Sqc: 0.9204 | loss: 1.3174\n",
      "epoch:    2 | step:  21499 | train Sqc: 0.9201 | loss: 1.3174\n",
      "epoch:    2 | step:  21599 | train Sqc: 0.9199 | loss: 1.3174\n",
      "epoch:    2 | step:  21699 | train Sqc: 0.9198 | loss: 1.3174\n",
      "epoch:    2 | step:  21799 | train Sqc: 0.9197 | loss: 1.3173\n",
      "epoch:    2 | step:  21899 | train Sqc: 0.9195 | loss: 1.3173\n",
      "epoch:    2 | step:  21999 | train Sqc: 0.9193 | loss: 1.3173\n",
      "epoch:    2 | step:  22099 | train Sqc: 0.9192 | loss: 1.3173\n",
      "epoch:    2 | step:  22199 | train Sqc: 0.9191 | loss: 1.3172\n",
      "epoch:    2 | step:  22299 | train Sqc: 0.9189 | loss: 1.3172\n",
      "epoch:    2 | step:  22399 | train Sqc: 0.9187 | loss: 1.3172\n",
      "epoch:    2 | step:  22499 | train Sqc: 0.9186 | loss: 1.3172\n",
      "epoch:    2 | step:  22599 | train Sqc: 0.9187 | loss: 1.3172\n",
      "epoch:    2 | step:  22699 | train Sqc: 0.9186 | loss: 1.3172\n",
      "epoch:    2 | step:  22799 | train Sqc: 0.9183 | loss: 1.3172\n",
      "epoch:    2 | step:  22899 | train Sqc: 0.9181 | loss: 1.3171\n",
      "epoch:    2 | step:  22999 | train Sqc: 0.9181 | loss: 1.3171\n",
      "epoch:    2 | step:  23099 | train Sqc: 0.9179 | loss: 1.3171\n",
      "epoch:    2 | step:  23199 | train Sqc: 0.9177 | loss: 1.3171\n",
      "epoch:    2 | step:  23299 | train Sqc: 0.9175 | loss: 1.3170\n",
      "epoch:    2 | step:  23399 | train Sqc: 0.9173 | loss: 1.3170\n",
      "epoch:    2 | step:  23499 | train Sqc: 0.9171 | loss: 1.3170\n",
      "epoch:    2 | step:  23599 | train Sqc: 0.9169 | loss: 1.3170\n",
      "epoch:    2 | step:  23699 | train Sqc: 0.9167 | loss: 1.3169\n",
      "epoch:    2 | step:  23799 | train Sqc: 0.9165 | loss: 1.3169\n",
      "epoch:    2 | step:  23899 | train Sqc: 0.9164 | loss: 1.3169\n",
      "epoch:    2 | step:  23999 | train Sqc: 0.9163 | loss: 1.3169\n",
      "epoch:    2 | step:  24099 | train Sqc: 0.9162 | loss: 1.3169\n",
      "epoch:    2 | step:  24199 | train Sqc: 0.9161 | loss: 1.3168\n",
      "epoch:    2 | step:  24299 | train Sqc: 0.9160 | loss: 1.3168\n",
      "epoch:    2 | step:  24399 | train Sqc: 0.9158 | loss: 1.3168\n",
      "epoch:    2 | step:  24499 | train Sqc: 0.9156 | loss: 1.3168\n",
      "epoch:    2 | step:  24599 | train Sqc: 0.9155 | loss: 1.3168\n",
      "epoch:    2 | step:  24699 | train Sqc: 0.9154 | loss: 1.3167\n",
      "epoch:    2 | step:  24799 | train Sqc: 0.9153 | loss: 1.3167\n",
      "epoch:    2 | step:  24899 | train Sqc: 0.9151 | loss: 1.3167\n",
      "epoch:    2 | step:  24999 | train Sqc: 0.9150 | loss: 1.3167\n",
      "epoch:    2 | step:  25099 | train Sqc: 0.9148 | loss: 1.3167\n",
      "epoch:    2 | step:  25199 | train Sqc: 0.9146 | loss: 1.3166\n",
      "epoch:    2 | step:  25299 | train Sqc: 0.9145 | loss: 1.3166\n",
      "epoch:    2 | step:  25399 | train Sqc: 0.9142 | loss: 1.3166\n",
      "epoch:    2 | step:  25499 | train Sqc: 0.9139 | loss: 1.3166\n",
      "epoch:    2 | step:  25599 | train Sqc: 0.9138 | loss: 1.3165\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3519 | test Neg: 0.4202\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3708 | test Neg: 0.4147\n",
      "epoch:    2 | step:   99 | N:  6 | test Sqc: 0.4054 | test Neg: 0.4071\n",
      "epoch:    2 | step:  199 | N:  6 | test Sqc: 0.4180 | test Neg: 0.4024\n",
      "epoch:    2 | step:   99 | N:  8 | test Sqc: 0.5253 | test Neg: 0.3685\n",
      "epoch:    2 | step:  199 | N:  8 | test Sqc: 0.5236 | test Neg: 0.3682\n",
      "epoch:    2 | step:   99 | N:  10 | test Sqc: 0.6381 | test Neg: 0.3242\n",
      "epoch:    2 | step:  199 | N:  10 | test Sqc: 0.6444 | test Neg: 0.3217\n",
      "epoch:    2 | step:   99 | N:  12 | test Sqc: 0.7351 | test Neg: 0.2868\n",
      "epoch:    2 | step:  199 | N:  12 | test Sqc: 0.7394 | test Neg: 0.2844\n",
      "epoch:    2 | step:   99 | N:  14 | test Sqc: 0.7376 | test Neg: 0.2876\n",
      "epoch:    2 | step:  199 | N:  14 | test Sqc: 0.7541 | test Neg: 0.2792\n",
      "epoch:    2 | step:   99 | N:  16 | test Sqc: 0.8015 | test Neg: 0.2604\n",
      "epoch:    2 | step:  199 | N:  16 | test Sqc: 0.8109 | test Neg: 0.2550\n",
      "epoch:    2 | step:   99 | N:  18 | test Sqc: 0.8573 | test Neg: 0.2349\n",
      "epoch:    2 | step:  199 | N:  18 | test Sqc: 0.8584 | test Neg: 0.2314\n",
      "epoch:    2 | step:   99 | N:  20 | test Sqc: 0.9385 | test Neg: 0.1931\n",
      "epoch:    2 | step:  199 | N:  20 | test Sqc: 0.9351 | test Neg: 0.1929\n",
      "epoch:    2 | step:   99 | N:  22 | test Sqc: 1.0021 | test Neg: 0.1566\n",
      "epoch:    2 | step:  199 | N:  22 | test Sqc: 0.9995 | test Neg: 0.1582\n",
      "epoch:    2 | step:   99 | N:  24 | test Sqc: 1.0602 | test Neg: 0.1211\n",
      "epoch:    2 | step:  199 | N:  24 | test Sqc: 1.0576 | test Neg: 0.1224\n",
      "epoch:    2 | step:   99 | N:  26 | test Sqc: 1.1095 | test Neg: 0.0896\n",
      "epoch:    2 | step:  199 | N:  26 | test Sqc: 1.1080 | test Neg: 0.0871\n",
      "epoch:    2 | step:   99 | N:  28 | test Sqc: 1.1351 | test Neg: 0.0732\n",
      "epoch:    2 | step:  199 | N:  28 | test Sqc: 1.1372 | test Neg: 0.0690\n",
      "epoch:    2 | step:   99 | N:  30 | test Sqc: 1.1892 | test Neg: 0.0328\n",
      "epoch:    2 | step:  199 | N:  30 | test Sqc: 1.1873 | test Neg: 0.0346\n",
      "epoch:    2 | step:   99 | N:  32 | test Sqc: 1.2404 | test Neg: 0.0148\n",
      "epoch:    2 | step:  199 | N:  32 | test Sqc: 1.2412 | test Neg: 0.0146\n",
      "epoch:    2 | step:   99 | N:  34 | test Sqc: 1.2768 | test Neg: 0.0055\n",
      "epoch:    2 | step:  199 | N:  34 | test Sqc: 1.2729 | test Neg: 0.0036\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | train Sqc: 0.8723 | loss: 1.3105\n",
      "epoch:    3 | step:  199 | train Sqc: 0.8738 | loss: 1.3108\n",
      "epoch:    3 | step:  299 | train Sqc: 0.8645 | loss: 1.3098\n",
      "epoch:    3 | step:  399 | train Sqc: 0.8676 | loss: 1.3102\n",
      "epoch:    3 | step:  499 | train Sqc: 0.8698 | loss: 1.3104\n",
      "epoch:    3 | step:  599 | train Sqc: 0.8732 | loss: 1.3110\n",
      "epoch:    3 | step:  699 | train Sqc: 0.8754 | loss: 1.3113\n",
      "epoch:    3 | step:  799 | train Sqc: 0.8739 | loss: 1.3112\n",
      "epoch:    3 | step:  899 | train Sqc: 0.8722 | loss: 1.3109\n",
      "epoch:    3 | step:  999 | train Sqc: 0.8702 | loss: 1.3107\n",
      "epoch:    3 | step:  1099 | train Sqc: 0.8711 | loss: 1.3108\n",
      "epoch:    3 | step:  1199 | train Sqc: 0.8714 | loss: 1.3108\n",
      "epoch:    3 | step:  1299 | train Sqc: 0.8738 | loss: 1.3111\n",
      "epoch:    3 | step:  1399 | train Sqc: 0.8737 | loss: 1.3112\n",
      "epoch:    3 | step:  1499 | train Sqc: 0.8744 | loss: 1.3112\n",
      "epoch:    3 | step:  1599 | train Sqc: 0.8739 | loss: 1.3111\n",
      "epoch:    3 | step:  1699 | train Sqc: 0.8741 | loss: 1.3111\n",
      "epoch:    3 | step:  1799 | train Sqc: 0.8741 | loss: 1.3112\n",
      "epoch:    3 | step:  1899 | train Sqc: 0.8743 | loss: 1.3112\n",
      "epoch:    3 | step:  1999 | train Sqc: 0.8740 | loss: 1.3111\n",
      "epoch:    3 | step:  2099 | train Sqc: 0.8746 | loss: 1.3111\n",
      "epoch:    3 | step:  2199 | train Sqc: 0.8744 | loss: 1.3111\n",
      "epoch:    3 | step:  2299 | train Sqc: 0.8739 | loss: 1.3110\n",
      "epoch:    3 | step:  2399 | train Sqc: 0.8740 | loss: 1.3110\n",
      "epoch:    3 | step:  2499 | train Sqc: 0.8737 | loss: 1.3110\n",
      "epoch:    3 | step:  2599 | train Sqc: 0.8751 | loss: 1.3112\n",
      "epoch:    3 | step:  2699 | train Sqc: 0.8760 | loss: 1.3113\n",
      "epoch:    3 | step:  2799 | train Sqc: 0.8762 | loss: 1.3113\n",
      "epoch:    3 | step:  2899 | train Sqc: 0.8763 | loss: 1.3113\n",
      "epoch:    3 | step:  2999 | train Sqc: 0.8766 | loss: 1.3114\n",
      "epoch:    3 | step:  3099 | train Sqc: 0.8811 | loss: 1.3120\n",
      "epoch:    3 | step:  3199 | train Sqc: 0.8828 | loss: 1.3123\n",
      "epoch:    3 | step:  3299 | train Sqc: 0.8845 | loss: 1.3125\n",
      "epoch:    3 | step:  3399 | train Sqc: 0.8850 | loss: 1.3126\n",
      "epoch:    3 | step:  3499 | train Sqc: 0.8857 | loss: 1.3127\n",
      "epoch:    3 | step:  3599 | train Sqc: 0.8865 | loss: 1.3128\n",
      "epoch:    3 | step:  3699 | train Sqc: 0.8860 | loss: 1.3128\n",
      "epoch:    3 | step:  3799 | train Sqc: 0.8864 | loss: 1.3128\n",
      "epoch:    3 | step:  3899 | train Sqc: 0.8871 | loss: 1.3129\n",
      "epoch:    3 | step:  3999 | train Sqc: 0.8872 | loss: 1.3129\n",
      "epoch:    3 | step:  4099 | train Sqc: 0.8861 | loss: 1.3127\n",
      "epoch:    3 | step:  4199 | train Sqc: 0.8853 | loss: 1.3126\n",
      "epoch:    3 | step:  4299 | train Sqc: 0.8849 | loss: 1.3126\n",
      "epoch:    3 | step:  4399 | train Sqc: 0.8854 | loss: 1.3126\n",
      "epoch:    3 | step:  4499 | train Sqc: 0.8845 | loss: 1.3125\n",
      "epoch:    3 | step:  4599 | train Sqc: 0.8846 | loss: 1.3125\n",
      "epoch:    3 | step:  4699 | train Sqc: 0.8842 | loss: 1.3125\n",
      "epoch:    3 | step:  4799 | train Sqc: 0.8840 | loss: 1.3124\n",
      "epoch:    3 | step:  4899 | train Sqc: 0.8837 | loss: 1.3124\n",
      "epoch:    3 | step:  4999 | train Sqc: 0.8837 | loss: 1.3124\n",
      "epoch:    3 | step:  5099 | train Sqc: 0.8833 | loss: 1.3124\n",
      "epoch:    3 | step:  5199 | train Sqc: 0.8832 | loss: 1.3123\n",
      "epoch:    3 | step:  5299 | train Sqc: 0.8831 | loss: 1.3123\n",
      "epoch:    3 | step:  5399 | train Sqc: 0.8830 | loss: 1.3123\n",
      "epoch:    3 | step:  5499 | train Sqc: 0.8830 | loss: 1.3123\n",
      "epoch:    3 | step:  5599 | train Sqc: 0.8834 | loss: 1.3123\n",
      "epoch:    3 | step:  5699 | train Sqc: 0.8831 | loss: 1.3123\n",
      "epoch:    3 | step:  5799 | train Sqc: 0.8828 | loss: 1.3123\n",
      "epoch:    3 | step:  5899 | train Sqc: 0.8824 | loss: 1.3122\n",
      "epoch:    3 | step:  5999 | train Sqc: 0.8821 | loss: 1.3122\n",
      "epoch:    3 | step:  6099 | train Sqc: 0.8817 | loss: 1.3121\n",
      "epoch:    3 | step:  6199 | train Sqc: 0.8814 | loss: 1.3121\n",
      "epoch:    3 | step:  6299 | train Sqc: 0.8815 | loss: 1.3121\n",
      "epoch:    3 | step:  6399 | train Sqc: 0.8816 | loss: 1.3121\n",
      "epoch:    3 | step:  6499 | train Sqc: 0.8815 | loss: 1.3121\n",
      "epoch:    3 | step:  6599 | train Sqc: 0.8812 | loss: 1.3121\n",
      "epoch:    3 | step:  6699 | train Sqc: 0.8811 | loss: 1.3120\n",
      "epoch:    3 | step:  6799 | train Sqc: 0.8810 | loss: 1.3120\n",
      "epoch:    3 | step:  6899 | train Sqc: 0.8808 | loss: 1.3120\n",
      "epoch:    3 | step:  6999 | train Sqc: 0.8804 | loss: 1.3119\n",
      "epoch:    3 | step:  7099 | train Sqc: 0.8803 | loss: 1.3119\n",
      "epoch:    3 | step:  7199 | train Sqc: 0.8797 | loss: 1.3118\n",
      "epoch:    3 | step:  7299 | train Sqc: 0.8796 | loss: 1.3118\n",
      "epoch:    3 | step:  7399 | train Sqc: 0.8796 | loss: 1.3118\n",
      "epoch:    3 | step:  7499 | train Sqc: 0.8794 | loss: 1.3117\n",
      "epoch:    3 | step:  7599 | train Sqc: 0.8795 | loss: 1.3118\n",
      "epoch:    3 | step:  7699 | train Sqc: 0.8793 | loss: 1.3117\n",
      "epoch:    3 | step:  7799 | train Sqc: 0.8793 | loss: 1.3117\n",
      "epoch:    3 | step:  7899 | train Sqc: 0.8794 | loss: 1.3118\n",
      "epoch:    3 | step:  7999 | train Sqc: 0.8793 | loss: 1.3117\n",
      "epoch:    3 | step:  8099 | train Sqc: 0.8790 | loss: 1.3117\n",
      "epoch:    3 | step:  8199 | train Sqc: 0.8790 | loss: 1.3117\n",
      "epoch:    3 | step:  8299 | train Sqc: 0.8791 | loss: 1.3117\n",
      "epoch:    3 | step:  8399 | train Sqc: 0.8795 | loss: 1.3118\n",
      "epoch:    3 | step:  8499 | train Sqc: 0.8795 | loss: 1.3118\n",
      "epoch:    3 | step:  8599 | train Sqc: 0.8791 | loss: 1.3117\n",
      "epoch:    3 | step:  8699 | train Sqc: 0.8790 | loss: 1.3117\n",
      "epoch:    3 | step:  8799 | train Sqc: 0.8790 | loss: 1.3117\n",
      "epoch:    3 | step:  8899 | train Sqc: 0.8788 | loss: 1.3117\n",
      "epoch:    3 | step:  8999 | train Sqc: 0.8787 | loss: 1.3117\n",
      "epoch:    3 | step:  9099 | train Sqc: 0.8786 | loss: 1.3117\n",
      "epoch:    3 | step:  9199 | train Sqc: 0.8784 | loss: 1.3116\n",
      "epoch:    3 | step:  9299 | train Sqc: 0.8785 | loss: 1.3116\n",
      "epoch:    3 | step:  9399 | train Sqc: 0.8785 | loss: 1.3116\n",
      "epoch:    3 | step:  9499 | train Sqc: 0.8782 | loss: 1.3116\n",
      "epoch:    3 | step:  9599 | train Sqc: 0.8780 | loss: 1.3116\n",
      "epoch:    3 | step:  9699 | train Sqc: 0.8780 | loss: 1.3116\n",
      "epoch:    3 | step:  9799 | train Sqc: 0.8780 | loss: 1.3116\n",
      "epoch:    3 | step:  9899 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  9999 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  10099 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  10199 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  10299 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  10399 | train Sqc: 0.8777 | loss: 1.3115\n",
      "epoch:    3 | step:  10499 | train Sqc: 0.8779 | loss: 1.3115\n",
      "epoch:    3 | step:  10599 | train Sqc: 0.8778 | loss: 1.3115\n",
      "epoch:    3 | step:  10699 | train Sqc: 0.8776 | loss: 1.3115\n",
      "epoch:    3 | step:  10799 | train Sqc: 0.8774 | loss: 1.3115\n",
      "epoch:    3 | step:  10899 | train Sqc: 0.8773 | loss: 1.3115\n",
      "epoch:    3 | step:  10999 | train Sqc: 0.8773 | loss: 1.3115\n",
      "epoch:    3 | step:  11099 | train Sqc: 0.8773 | loss: 1.3115\n",
      "epoch:    3 | step:  11199 | train Sqc: 0.8773 | loss: 1.3115\n",
      "epoch:    3 | step:  11299 | train Sqc: 0.8772 | loss: 1.3115\n",
      "epoch:    3 | step:  11399 | train Sqc: 0.8773 | loss: 1.3115\n",
      "epoch:    3 | step:  11499 | train Sqc: 0.8772 | loss: 1.3114\n",
      "epoch:    3 | step:  11599 | train Sqc: 0.8771 | loss: 1.3114\n",
      "epoch:    3 | step:  11699 | train Sqc: 0.8771 | loss: 1.3114\n",
      "epoch:    3 | step:  11799 | train Sqc: 0.8770 | loss: 1.3114\n",
      "epoch:    3 | step:  11899 | train Sqc: 0.8769 | loss: 1.3114\n",
      "epoch:    3 | step:  11999 | train Sqc: 0.8767 | loss: 1.3114\n",
      "epoch:    3 | step:  12099 | train Sqc: 0.8766 | loss: 1.3113\n",
      "epoch:    3 | step:  12199 | train Sqc: 0.8767 | loss: 1.3113\n",
      "epoch:    3 | step:  12299 | train Sqc: 0.8767 | loss: 1.3113\n",
      "epoch:    3 | step:  12399 | train Sqc: 0.8766 | loss: 1.3113\n",
      "epoch:    3 | step:  12499 | train Sqc: 0.8764 | loss: 1.3113\n",
      "epoch:    3 | step:  12599 | train Sqc: 0.8762 | loss: 1.3113\n",
      "epoch:    3 | step:  12699 | train Sqc: 0.8762 | loss: 1.3113\n",
      "epoch:    3 | step:  12799 | train Sqc: 0.8761 | loss: 1.3113\n",
      "epoch:    3 | step:  12899 | train Sqc: 0.8759 | loss: 1.3112\n",
      "epoch:    3 | step:  12999 | train Sqc: 0.8758 | loss: 1.3112\n",
      "epoch:    3 | step:  13099 | train Sqc: 0.8757 | loss: 1.3112\n",
      "epoch:    3 | step:  13199 | train Sqc: 0.8755 | loss: 1.3112\n",
      "epoch:    3 | step:  13299 | train Sqc: 0.8753 | loss: 1.3111\n",
      "epoch:    3 | step:  13399 | train Sqc: 0.8752 | loss: 1.3111\n",
      "epoch:    3 | step:  13499 | train Sqc: 0.8751 | loss: 1.3111\n",
      "epoch:    3 | step:  13599 | train Sqc: 0.8750 | loss: 1.3111\n",
      "epoch:    3 | step:  13699 | train Sqc: 0.8751 | loss: 1.3111\n",
      "epoch:    3 | step:  13799 | train Sqc: 0.8750 | loss: 1.3111\n",
      "epoch:    3 | step:  13899 | train Sqc: 0.8751 | loss: 1.3111\n",
      "epoch:    3 | step:  13999 | train Sqc: 0.8749 | loss: 1.3111\n",
      "epoch:    3 | step:  14099 | train Sqc: 0.8746 | loss: 1.3111\n",
      "epoch:    3 | step:  14199 | train Sqc: 0.8745 | loss: 1.3111\n",
      "epoch:    3 | step:  14299 | train Sqc: 0.8745 | loss: 1.3111\n",
      "epoch:    3 | step:  14399 | train Sqc: 0.8742 | loss: 1.3110\n",
      "epoch:    3 | step:  14499 | train Sqc: 0.8741 | loss: 1.3110\n",
      "epoch:    3 | step:  14599 | train Sqc: 0.8740 | loss: 1.3110\n",
      "epoch:    3 | step:  14699 | train Sqc: 0.8739 | loss: 1.3110\n",
      "epoch:    3 | step:  14799 | train Sqc: 0.8740 | loss: 1.3110\n",
      "epoch:    3 | step:  14899 | train Sqc: 0.8740 | loss: 1.3110\n",
      "epoch:    3 | step:  14999 | train Sqc: 0.8737 | loss: 1.3110\n",
      "epoch:    3 | step:  15099 | train Sqc: 0.8737 | loss: 1.3109\n",
      "epoch:    3 | step:  15199 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  15299 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  15399 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  15499 | train Sqc: 0.8733 | loss: 1.3109\n",
      "epoch:    3 | step:  15599 | train Sqc: 0.8733 | loss: 1.3109\n",
      "epoch:    3 | step:  15699 | train Sqc: 0.8732 | loss: 1.3109\n",
      "epoch:    3 | step:  15799 | train Sqc: 0.8732 | loss: 1.3109\n",
      "epoch:    3 | step:  15899 | train Sqc: 0.8730 | loss: 1.3108\n",
      "epoch:    3 | step:  15999 | train Sqc: 0.8731 | loss: 1.3108\n",
      "epoch:    3 | step:  16099 | train Sqc: 0.8734 | loss: 1.3109\n",
      "epoch:    3 | step:  16199 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  16299 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  16399 | train Sqc: 0.8737 | loss: 1.3109\n",
      "epoch:    3 | step:  16499 | train Sqc: 0.8737 | loss: 1.3109\n",
      "epoch:    3 | step:  16599 | train Sqc: 0.8738 | loss: 1.3109\n",
      "epoch:    3 | step:  16699 | train Sqc: 0.8739 | loss: 1.3109\n",
      "epoch:    3 | step:  16799 | train Sqc: 0.8738 | loss: 1.3109\n",
      "epoch:    3 | step:  16899 | train Sqc: 0.8738 | loss: 1.3109\n",
      "epoch:    3 | step:  16999 | train Sqc: 0.8737 | loss: 1.3109\n",
      "epoch:    3 | step:  17099 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  17199 | train Sqc: 0.8736 | loss: 1.3109\n",
      "epoch:    3 | step:  17299 | train Sqc: 0.8734 | loss: 1.3109\n",
      "epoch:    3 | step:  17399 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17499 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17599 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17699 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17799 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17899 | train Sqc: 0.8735 | loss: 1.3109\n",
      "epoch:    3 | step:  17999 | train Sqc: 0.8733 | loss: 1.3109\n",
      "epoch:    3 | step:  18099 | train Sqc: 0.8732 | loss: 1.3109\n",
      "epoch:    3 | step:  18199 | train Sqc: 0.8734 | loss: 1.3109\n",
      "epoch:    3 | step:  18299 | train Sqc: 0.8733 | loss: 1.3109\n",
      "epoch:    3 | step:  18399 | train Sqc: 0.8732 | loss: 1.3109\n",
      "epoch:    3 | step:  18499 | train Sqc: 0.8731 | loss: 1.3109\n",
      "epoch:    3 | step:  18599 | train Sqc: 0.8731 | loss: 1.3109\n",
      "epoch:    3 | step:  18699 | train Sqc: 0.8731 | loss: 1.3109\n",
      "epoch:    3 | step:  18799 | train Sqc: 0.8730 | loss: 1.3108\n",
      "epoch:    3 | step:  18899 | train Sqc: 0.8730 | loss: 1.3109\n",
      "epoch:    3 | step:  18999 | train Sqc: 0.8730 | loss: 1.3108\n",
      "epoch:    3 | step:  19099 | train Sqc: 0.8730 | loss: 1.3108\n",
      "epoch:    3 | step:  19199 | train Sqc: 0.8729 | loss: 1.3108\n",
      "epoch:    3 | step:  19299 | train Sqc: 0.8729 | loss: 1.3108\n",
      "epoch:    3 | step:  19399 | train Sqc: 0.8730 | loss: 1.3109\n",
      "epoch:    3 | step:  19499 | train Sqc: 0.8729 | loss: 1.3108\n",
      "epoch:    3 | step:  19599 | train Sqc: 0.8728 | loss: 1.3108\n",
      "epoch:    3 | step:  19699 | train Sqc: 0.8729 | loss: 1.3108\n",
      "epoch:    3 | step:  19799 | train Sqc: 0.8728 | loss: 1.3108\n",
      "epoch:    3 | step:  19899 | train Sqc: 0.8727 | loss: 1.3108\n",
      "epoch:    3 | step:  19999 | train Sqc: 0.8726 | loss: 1.3108\n",
      "epoch:    3 | step:  20099 | train Sqc: 0.8726 | loss: 1.3108\n",
      "epoch:    3 | step:  20199 | train Sqc: 0.8726 | loss: 1.3108\n",
      "epoch:    3 | step:  20299 | train Sqc: 0.8724 | loss: 1.3108\n",
      "epoch:    3 | step:  20399 | train Sqc: 0.8724 | loss: 1.3108\n",
      "epoch:    3 | step:  20499 | train Sqc: 0.8723 | loss: 1.3108\n",
      "epoch:    3 | step:  20599 | train Sqc: 0.8723 | loss: 1.3108\n",
      "epoch:    3 | step:  20699 | train Sqc: 0.8724 | loss: 1.3108\n",
      "epoch:    3 | step:  20799 | train Sqc: 0.8724 | loss: 1.3108\n",
      "epoch:    3 | step:  20899 | train Sqc: 0.8724 | loss: 1.3108\n",
      "epoch:    3 | step:  20999 | train Sqc: 0.8723 | loss: 1.3108\n",
      "epoch:    3 | step:  21099 | train Sqc: 0.8723 | loss: 1.3108\n",
      "epoch:    3 | step:  21199 | train Sqc: 0.8723 | loss: 1.3108\n",
      "epoch:    3 | step:  21299 | train Sqc: 0.8722 | loss: 1.3108\n",
      "epoch:    3 | step:  21399 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  21499 | train Sqc: 0.8720 | loss: 1.3107\n",
      "epoch:    3 | step:  21599 | train Sqc: 0.8720 | loss: 1.3107\n",
      "epoch:    3 | step:  21699 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  21799 | train Sqc: 0.8722 | loss: 1.3108\n",
      "epoch:    3 | step:  21899 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  21999 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  22099 | train Sqc: 0.8722 | loss: 1.3108\n",
      "epoch:    3 | step:  22199 | train Sqc: 0.8722 | loss: 1.3108\n",
      "epoch:    3 | step:  22299 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  22399 | train Sqc: 0.8721 | loss: 1.3108\n",
      "epoch:    3 | step:  22499 | train Sqc: 0.8720 | loss: 1.3108\n",
      "epoch:    3 | step:  22599 | train Sqc: 0.8719 | loss: 1.3107\n",
      "epoch:    3 | step:  22699 | train Sqc: 0.8720 | loss: 1.3107\n",
      "epoch:    3 | step:  22799 | train Sqc: 0.8718 | loss: 1.3107\n",
      "epoch:    3 | step:  22899 | train Sqc: 0.8717 | loss: 1.3107\n",
      "epoch:    3 | step:  22999 | train Sqc: 0.8718 | loss: 1.3107\n",
      "epoch:    3 | step:  23099 | train Sqc: 0.8717 | loss: 1.3107\n",
      "epoch:    3 | step:  23199 | train Sqc: 0.8717 | loss: 1.3107\n",
      "epoch:    3 | step:  23299 | train Sqc: 0.8715 | loss: 1.3107\n",
      "epoch:    3 | step:  23399 | train Sqc: 0.8715 | loss: 1.3107\n",
      "epoch:    3 | step:  23499 | train Sqc: 0.8714 | loss: 1.3107\n",
      "epoch:    3 | step:  23599 | train Sqc: 0.8714 | loss: 1.3107\n",
      "epoch:    3 | step:  23699 | train Sqc: 0.8713 | loss: 1.3106\n",
      "epoch:    3 | step:  23799 | train Sqc: 0.8712 | loss: 1.3106\n",
      "epoch:    3 | step:  23899 | train Sqc: 0.8712 | loss: 1.3106\n",
      "epoch:    3 | step:  23999 | train Sqc: 0.8712 | loss: 1.3106\n",
      "epoch:    3 | step:  24099 | train Sqc: 0.8713 | loss: 1.3106\n",
      "epoch:    3 | step:  24199 | train Sqc: 0.8713 | loss: 1.3106\n",
      "epoch:    3 | step:  24299 | train Sqc: 0.8713 | loss: 1.3106\n",
      "epoch:    3 | step:  24399 | train Sqc: 0.8713 | loss: 1.3106\n",
      "epoch:    3 | step:  24499 | train Sqc: 0.8711 | loss: 1.3106\n",
      "epoch:    3 | step:  24599 | train Sqc: 0.8711 | loss: 1.3106\n",
      "epoch:    3 | step:  24699 | train Sqc: 0.8711 | loss: 1.3106\n",
      "epoch:    3 | step:  24799 | train Sqc: 0.8710 | loss: 1.3106\n",
      "epoch:    3 | step:  24899 | train Sqc: 0.8709 | loss: 1.3106\n",
      "epoch:    3 | step:  24999 | train Sqc: 0.8709 | loss: 1.3106\n",
      "epoch:    3 | step:  25099 | train Sqc: 0.8708 | loss: 1.3106\n",
      "epoch:    3 | step:  25199 | train Sqc: 0.8708 | loss: 1.3106\n",
      "epoch:    3 | step:  25299 | train Sqc: 0.8708 | loss: 1.3106\n",
      "epoch:    3 | step:  25399 | train Sqc: 0.8707 | loss: 1.3106\n",
      "epoch:    3 | step:  25499 | train Sqc: 0.8705 | loss: 1.3105\n",
      "epoch:    3 | step:  25599 | train Sqc: 0.8707 | loss: 1.3106\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | test Sqc: 0.3535 | test Neg: 0.4203\n",
      "epoch:    3 | step:  199 | N:  4 | test Sqc: 0.3694 | test Neg: 0.4147\n",
      "epoch:    3 | step:   99 | N:  6 | test Sqc: 0.4022 | test Neg: 0.4073\n",
      "epoch:    3 | step:  199 | N:  6 | test Sqc: 0.4145 | test Neg: 0.4026\n",
      "epoch:    3 | step:   99 | N:  8 | test Sqc: 0.6222 | test Neg: 0.3295\n",
      "epoch:    3 | step:  199 | N:  8 | test Sqc: 0.6176 | test Neg: 0.3303\n",
      "epoch:    3 | step:   99 | N:  10 | test Sqc: 0.6627 | test Neg: 0.3152\n",
      "epoch:    3 | step:  199 | N:  10 | test Sqc: 0.6646 | test Neg: 0.3135\n",
      "epoch:    3 | step:   99 | N:  12 | test Sqc: 0.7470 | test Neg: 0.2838\n",
      "epoch:    3 | step:  199 | N:  12 | test Sqc: 0.7503 | test Neg: 0.2818\n",
      "epoch:    3 | step:   99 | N:  14 | test Sqc: 0.7629 | test Neg: 0.2741\n",
      "epoch:    3 | step:  199 | N:  14 | test Sqc: 0.7767 | test Neg: 0.2682\n",
      "epoch:    3 | step:   99 | N:  16 | test Sqc: 0.8179 | test Neg: 0.2521\n",
      "epoch:    3 | step:  199 | N:  16 | test Sqc: 0.8290 | test Neg: 0.2465\n",
      "epoch:    3 | step:   99 | N:  18 | test Sqc: 0.8667 | test Neg: 0.2292\n",
      "epoch:    3 | step:  199 | N:  18 | test Sqc: 0.8710 | test Neg: 0.2246\n",
      "epoch:    3 | step:   99 | N:  20 | test Sqc: 0.9454 | test Neg: 0.1861\n",
      "epoch:    3 | step:  199 | N:  20 | test Sqc: 0.9441 | test Neg: 0.1869\n",
      "epoch:    3 | step:   99 | N:  22 | test Sqc: 1.0117 | test Neg: 0.1486\n",
      "epoch:    3 | step:  199 | N:  22 | test Sqc: 1.0063 | test Neg: 0.1514\n",
      "epoch:    3 | step:   99 | N:  24 | test Sqc: 1.0561 | test Neg: 0.1270\n",
      "epoch:    3 | step:  199 | N:  24 | test Sqc: 1.0565 | test Neg: 0.1265\n",
      "epoch:    3 | step:   99 | N:  26 | test Sqc: 1.1056 | test Neg: 0.0937\n",
      "epoch:    3 | step:  199 | N:  26 | test Sqc: 1.1051 | test Neg: 0.0929\n",
      "epoch:    3 | step:   99 | N:  28 | test Sqc: 1.1118 | test Neg: 0.0927\n",
      "epoch:    3 | step:  199 | N:  28 | test Sqc: 1.1169 | test Neg: 0.0865\n",
      "epoch:    3 | step:   99 | N:  30 | test Sqc: 1.1564 | test Neg: 0.0572\n",
      "epoch:    3 | step:  199 | N:  30 | test Sqc: 1.1540 | test Neg: 0.0600\n",
      "epoch:    3 | step:   99 | N:  32 | test Sqc: 1.2099 | test Neg: 0.0335\n",
      "epoch:    3 | step:  199 | N:  32 | test Sqc: 1.2075 | test Neg: 0.0312\n",
      "epoch:    3 | step:   99 | N:  34 | test Sqc: 1.2391 | test Neg: 0.0176\n",
      "epoch:    3 | step:  199 | N:  34 | test Sqc: 1.2337 | test Neg: 0.0173\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | train Sqc: 0.8800 | loss: 1.3122\n",
      "epoch:    4 | step:  199 | train Sqc: 0.8734 | loss: 1.3110\n",
      "epoch:    4 | step:  299 | train Sqc: 0.8608 | loss: 1.3095\n",
      "epoch:    4 | step:  399 | train Sqc: 0.8618 | loss: 1.3096\n",
      "epoch:    4 | step:  499 | train Sqc: 0.8615 | loss: 1.3095\n",
      "epoch:    4 | step:  599 | train Sqc: 0.8629 | loss: 1.3097\n",
      "epoch:    4 | step:  699 | train Sqc: 0.8651 | loss: 1.3100\n",
      "epoch:    4 | step:  799 | train Sqc: 0.8636 | loss: 1.3099\n",
      "epoch:    4 | step:  899 | train Sqc: 0.8617 | loss: 1.3095\n",
      "epoch:    4 | step:  999 | train Sqc: 0.8597 | loss: 1.3093\n",
      "epoch:    4 | step:  1099 | train Sqc: 0.8604 | loss: 1.3094\n",
      "epoch:    4 | step:  1199 | train Sqc: 0.8612 | loss: 1.3096\n",
      "epoch:    4 | step:  1299 | train Sqc: 0.8635 | loss: 1.3099\n",
      "epoch:    4 | step:  1399 | train Sqc: 0.8628 | loss: 1.3098\n",
      "epoch:    4 | step:  1499 | train Sqc: 0.8632 | loss: 1.3097\n",
      "epoch:    4 | step:  1599 | train Sqc: 0.8625 | loss: 1.3096\n",
      "epoch:    4 | step:  1699 | train Sqc: 0.8627 | loss: 1.3096\n",
      "epoch:    4 | step:  1799 | train Sqc: 0.8624 | loss: 1.3096\n",
      "epoch:    4 | step:  1899 | train Sqc: 0.8621 | loss: 1.3096\n",
      "epoch:    4 | step:  1999 | train Sqc: 0.8618 | loss: 1.3094\n",
      "epoch:    4 | step:  2099 | train Sqc: 0.8624 | loss: 1.3095\n",
      "epoch:    4 | step:  2199 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    4 | step:  2299 | train Sqc: 0.8615 | loss: 1.3093\n",
      "epoch:    4 | step:  2399 | train Sqc: 0.8617 | loss: 1.3093\n",
      "epoch:    4 | step:  2499 | train Sqc: 0.8618 | loss: 1.3094\n",
      "epoch:    4 | step:  2599 | train Sqc: 0.8622 | loss: 1.3094\n",
      "epoch:    4 | step:  2699 | train Sqc: 0.8625 | loss: 1.3094\n",
      "epoch:    4 | step:  2799 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    4 | step:  2899 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  2999 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  3099 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    4 | step:  3199 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    4 | step:  3299 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    4 | step:  3399 | train Sqc: 0.8611 | loss: 1.3092\n",
      "epoch:    4 | step:  3499 | train Sqc: 0.8613 | loss: 1.3092\n",
      "epoch:    4 | step:  3599 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    4 | step:  3699 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    4 | step:  3799 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  3899 | train Sqc: 0.8627 | loss: 1.3094\n",
      "epoch:    4 | step:  3999 | train Sqc: 0.8631 | loss: 1.3094\n",
      "epoch:    4 | step:  4099 | train Sqc: 0.8623 | loss: 1.3093\n",
      "epoch:    4 | step:  4199 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    4 | step:  4299 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    4 | step:  4399 | train Sqc: 0.8625 | loss: 1.3093\n",
      "epoch:    4 | step:  4499 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  4599 | train Sqc: 0.8622 | loss: 1.3093\n",
      "epoch:    4 | step:  4699 | train Sqc: 0.8619 | loss: 1.3092\n",
      "epoch:    4 | step:  4799 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  4899 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    4 | step:  4999 | train Sqc: 0.8621 | loss: 1.3092\n",
      "epoch:    4 | step:  5099 | train Sqc: 0.8620 | loss: 1.3092\n",
      "epoch:    4 | step:  5199 | train Sqc: 0.8619 | loss: 1.3092\n",
      "epoch:    4 | step:  5299 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    4 | step:  5399 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    4 | step:  5499 | train Sqc: 0.8622 | loss: 1.3093\n",
      "epoch:    4 | step:  5599 | train Sqc: 0.8627 | loss: 1.3093\n",
      "epoch:    4 | step:  5699 | train Sqc: 0.8625 | loss: 1.3093\n",
      "epoch:    4 | step:  5799 | train Sqc: 0.8623 | loss: 1.3093\n",
      "epoch:    4 | step:  5899 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    4 | step:  5999 | train Sqc: 0.8618 | loss: 1.3092\n",
      "epoch:    4 | step:  6099 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    4 | step:  6199 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    4 | step:  6299 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    4 | step:  6399 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    4 | step:  6499 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    4 | step:  6599 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    4 | step:  6699 | train Sqc: 0.8613 | loss: 1.3092\n",
      "epoch:    4 | step:  6799 | train Sqc: 0.8613 | loss: 1.3092\n",
      "epoch:    4 | step:  6899 | train Sqc: 0.8612 | loss: 1.3091\n",
      "epoch:    4 | step:  6999 | train Sqc: 0.8608 | loss: 1.3091\n",
      "epoch:    4 | step:  7099 | train Sqc: 0.8609 | loss: 1.3091\n",
      "epoch:    4 | step:  7199 | train Sqc: 0.8605 | loss: 1.3090\n",
      "epoch:    4 | step:  7299 | train Sqc: 0.8603 | loss: 1.3090\n",
      "epoch:    4 | step:  7399 | train Sqc: 0.8604 | loss: 1.3090\n",
      "epoch:    4 | step:  7499 | train Sqc: 0.8603 | loss: 1.3090\n",
      "epoch:    4 | step:  7599 | train Sqc: 0.8605 | loss: 1.3090\n",
      "epoch:    4 | step:  7699 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  7799 | train Sqc: 0.8599 | loss: 1.3089\n",
      "epoch:    4 | step:  7899 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  7999 | train Sqc: 0.8599 | loss: 1.3089\n",
      "epoch:    4 | step:  8099 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    4 | step:  8199 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  8299 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  8399 | train Sqc: 0.8599 | loss: 1.3089\n",
      "epoch:    4 | step:  8499 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    4 | step:  8599 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  8699 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  8799 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    4 | step:  8899 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    4 | step:  8999 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  9099 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  9199 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  9299 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  9399 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  9499 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  9599 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    4 | step:  9699 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    4 | step:  9799 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  9899 | train Sqc: 0.8599 | loss: 1.3089\n",
      "epoch:    4 | step:  9999 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  10099 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  10199 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  10299 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  10399 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  10499 | train Sqc: 0.8604 | loss: 1.3090\n",
      "epoch:    4 | step:  10599 | train Sqc: 0.8604 | loss: 1.3090\n",
      "epoch:    4 | step:  10699 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    4 | step:  10799 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  10899 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  10999 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    4 | step:  11099 | train Sqc: 0.8598 | loss: 1.3090\n",
      "epoch:    4 | step:  11199 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  11299 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  11399 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  11499 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    4 | step:  11599 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    4 | step:  11699 | train Sqc: 0.8599 | loss: 1.3089\n",
      "epoch:    4 | step:  11799 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    4 | step:  11899 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    4 | step:  11999 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    4 | step:  12099 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    4 | step:  12199 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    4 | step:  12299 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    4 | step:  12399 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    4 | step:  12499 | train Sqc: 0.8593 | loss: 1.3088\n",
      "epoch:    4 | step:  12599 | train Sqc: 0.8591 | loss: 1.3088\n",
      "epoch:    4 | step:  12699 | train Sqc: 0.8592 | loss: 1.3088\n",
      "epoch:    4 | step:  12799 | train Sqc: 0.8591 | loss: 1.3088\n",
      "epoch:    4 | step:  12899 | train Sqc: 0.8590 | loss: 1.3088\n",
      "epoch:    4 | step:  12999 | train Sqc: 0.8590 | loss: 1.3088\n",
      "epoch:    4 | step:  13099 | train Sqc: 0.8590 | loss: 1.3088\n",
      "epoch:    4 | step:  13199 | train Sqc: 0.8588 | loss: 1.3088\n",
      "epoch:    4 | step:  13299 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    4 | step:  13399 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    4 | step:  13499 | train Sqc: 0.8585 | loss: 1.3088\n",
      "epoch:    4 | step:  13599 | train Sqc: 0.8585 | loss: 1.3088\n",
      "epoch:    4 | step:  13699 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    4 | step:  13799 | train Sqc: 0.8585 | loss: 1.3088\n",
      "epoch:    4 | step:  13899 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    4 | step:  13999 | train Sqc: 0.8584 | loss: 1.3087\n",
      "epoch:    4 | step:  14099 | train Sqc: 0.8582 | loss: 1.3087\n",
      "epoch:    4 | step:  14199 | train Sqc: 0.8582 | loss: 1.3087\n",
      "epoch:    4 | step:  14299 | train Sqc: 0.8582 | loss: 1.3087\n",
      "epoch:    4 | step:  14399 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  14499 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  14599 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  14699 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  14799 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  14899 | train Sqc: 0.8582 | loss: 1.3087\n",
      "epoch:    4 | step:  14999 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  15099 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  15199 | train Sqc: 0.8579 | loss: 1.3087\n",
      "epoch:    4 | step:  15299 | train Sqc: 0.8579 | loss: 1.3087\n",
      "epoch:    4 | step:  15399 | train Sqc: 0.8578 | loss: 1.3087\n",
      "epoch:    4 | step:  15499 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  15599 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  15699 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  15799 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  15899 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  15999 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  16099 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  16199 | train Sqc: 0.8578 | loss: 1.3086\n",
      "epoch:    4 | step:  16299 | train Sqc: 0.8579 | loss: 1.3086\n",
      "epoch:    4 | step:  16399 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  16499 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  16599 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  16699 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  16799 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  16899 | train Sqc: 0.8581 | loss: 1.3087\n",
      "epoch:    4 | step:  16999 | train Sqc: 0.8580 | loss: 1.3087\n",
      "epoch:    4 | step:  17099 | train Sqc: 0.8578 | loss: 1.3086\n",
      "epoch:    4 | step:  17199 | train Sqc: 0.8578 | loss: 1.3086\n",
      "epoch:    4 | step:  17299 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  17399 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  17499 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  17599 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  17699 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  17799 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  17899 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  17999 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  18099 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  18199 | train Sqc: 0.8577 | loss: 1.3086\n",
      "epoch:    4 | step:  18299 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  18399 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  18499 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  18599 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  18699 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  18799 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  18899 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  18999 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  19099 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  19199 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  19299 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  19399 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  19499 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  19599 | train Sqc: 0.8576 | loss: 1.3087\n",
      "epoch:    4 | step:  19699 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  19799 | train Sqc: 0.8576 | loss: 1.3087\n",
      "epoch:    4 | step:  19899 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  19999 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  20099 | train Sqc: 0.8576 | loss: 1.3086\n",
      "epoch:    4 | step:  20199 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  20299 | train Sqc: 0.8574 | loss: 1.3086\n",
      "epoch:    4 | step:  20399 | train Sqc: 0.8574 | loss: 1.3086\n",
      "epoch:    4 | step:  20499 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  20599 | train Sqc: 0.8574 | loss: 1.3086\n",
      "epoch:    4 | step:  20699 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  20799 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  20899 | train Sqc: 0.8575 | loss: 1.3086\n",
      "epoch:    4 | step:  20999 | train Sqc: 0.8574 | loss: 1.3086\n",
      "epoch:    4 | step:  21099 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  21199 | train Sqc: 0.8574 | loss: 1.3086\n",
      "epoch:    4 | step:  21299 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  21399 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  21499 | train Sqc: 0.8572 | loss: 1.3086\n",
      "epoch:    4 | step:  21599 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  21699 | train Sqc: 0.8572 | loss: 1.3086\n",
      "epoch:    4 | step:  21799 | train Sqc: 0.8573 | loss: 1.3086\n",
      "epoch:    4 | step:  21899 | train Sqc: 0.8575 | loss: 1.3087\n",
      "epoch:    4 | step:  21999 | train Sqc: 0.8575 | loss: 1.3087\n",
      "epoch:    4 | step:  22099 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  22199 | train Sqc: 0.8578 | loss: 1.3087\n",
      "epoch:    4 | step:  22299 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  22399 | train Sqc: 0.8578 | loss: 1.3087\n",
      "epoch:    4 | step:  22499 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  22599 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  22699 | train Sqc: 0.8578 | loss: 1.3087\n",
      "epoch:    4 | step:  22799 | train Sqc: 0.8576 | loss: 1.3087\n",
      "epoch:    4 | step:  22899 | train Sqc: 0.8576 | loss: 1.3087\n",
      "epoch:    4 | step:  22999 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  23099 | train Sqc: 0.8577 | loss: 1.3087\n",
      "epoch:    4 | step:  23199 | train Sqc: 0.8576 | loss: 1.3087\n",
      "epoch:    4 | step:  23299 | train Sqc: 0.8575 | loss: 1.3087\n",
      "epoch:    4 | step:  23399 | train Sqc: 0.8575 | loss: 1.3087\n",
      "epoch:    4 | step:  23499 | train Sqc: 0.8574 | loss: 1.3087\n",
      "epoch:    4 | step:  23599 | train Sqc: 0.8575 | loss: 1.3087\n",
      "epoch:    4 | step:  23699 | train Sqc: 0.8574 | loss: 1.3087\n",
      "epoch:    4 | step:  23799 | train Sqc: 0.8574 | loss: 1.3087\n",
      "epoch:    4 | step:  23899 | train Sqc: 0.8574 | loss: 1.3087\n",
      "epoch:    4 | step:  23999 | train Sqc: 0.8584 | loss: 1.3088\n",
      "epoch:    4 | step:  24099 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    4 | step:  24199 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    4 | step:  24299 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    4 | step:  24399 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    4 | step:  24499 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    4 | step:  24599 | train Sqc: 0.8593 | loss: 1.3090\n",
      "epoch:    4 | step:  24699 | train Sqc: 0.8593 | loss: 1.3090\n",
      "epoch:    4 | step:  24799 | train Sqc: 0.8593 | loss: 1.3090\n",
      "epoch:    4 | step:  24899 | train Sqc: 0.8592 | loss: 1.3090\n",
      "epoch:    4 | step:  24999 | train Sqc: 0.8593 | loss: 1.3090\n",
      "epoch:    4 | step:  25099 | train Sqc: 0.8593 | loss: 1.3090\n",
      "epoch:    4 | step:  25199 | train Sqc: 0.8594 | loss: 1.3090\n",
      "epoch:    4 | step:  25299 | train Sqc: 0.8594 | loss: 1.3090\n",
      "epoch:    4 | step:  25399 | train Sqc: 0.8594 | loss: 1.3090\n",
      "epoch:    4 | step:  25499 | train Sqc: 0.8592 | loss: 1.3090\n",
      "epoch:    4 | step:  25599 | train Sqc: 0.8593 | loss: 1.3090\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | test Sqc: 0.3553 | test Neg: 0.4200\n",
      "epoch:    4 | step:  199 | N:  4 | test Sqc: 0.3725 | test Neg: 0.4145\n",
      "epoch:    4 | step:   99 | N:  6 | test Sqc: 0.4093 | test Neg: 0.4067\n",
      "epoch:    4 | step:  199 | N:  6 | test Sqc: 0.4237 | test Neg: 0.4020\n",
      "epoch:    4 | step:   99 | N:  8 | test Sqc: 0.5341 | test Neg: 0.3687\n",
      "epoch:    4 | step:  199 | N:  8 | test Sqc: 0.5318 | test Neg: 0.3687\n",
      "epoch:    4 | step:   99 | N:  10 | test Sqc: 0.6337 | test Neg: 0.3284\n",
      "epoch:    4 | step:  199 | N:  10 | test Sqc: 0.6362 | test Neg: 0.3271\n",
      "epoch:    4 | step:   99 | N:  12 | test Sqc: 0.7316 | test Neg: 0.2912\n",
      "epoch:    4 | step:  199 | N:  12 | test Sqc: 0.7305 | test Neg: 0.2896\n",
      "epoch:    4 | step:   99 | N:  14 | test Sqc: 0.7519 | test Neg: 0.2801\n",
      "epoch:    4 | step:  199 | N:  14 | test Sqc: 0.7674 | test Neg: 0.2729\n",
      "epoch:    4 | step:   99 | N:  16 | test Sqc: 0.8072 | test Neg: 0.2553\n",
      "epoch:    4 | step:  199 | N:  16 | test Sqc: 0.8189 | test Neg: 0.2503\n",
      "epoch:    4 | step:   99 | N:  18 | test Sqc: 0.8565 | test Neg: 0.2338\n",
      "epoch:    4 | step:  199 | N:  18 | test Sqc: 0.8601 | test Neg: 0.2298\n",
      "epoch:    4 | step:   99 | N:  20 | test Sqc: 0.9445 | test Neg: 0.1876\n",
      "epoch:    4 | step:  199 | N:  20 | test Sqc: 0.9444 | test Neg: 0.1872\n",
      "epoch:    4 | step:   99 | N:  22 | test Sqc: 1.0031 | test Neg: 0.1542\n",
      "epoch:    4 | step:  199 | N:  22 | test Sqc: 1.0009 | test Neg: 0.1561\n",
      "epoch:    4 | step:   99 | N:  24 | test Sqc: 1.0545 | test Neg: 0.1260\n",
      "epoch:    4 | step:  199 | N:  24 | test Sqc: 1.0540 | test Neg: 0.1264\n",
      "epoch:    4 | step:   99 | N:  26 | test Sqc: 1.0931 | test Neg: 0.0987\n",
      "epoch:    4 | step:  199 | N:  26 | test Sqc: 1.0945 | test Neg: 0.0983\n",
      "epoch:    4 | step:   99 | N:  28 | test Sqc: 1.1083 | test Neg: 0.0944\n",
      "epoch:    4 | step:  199 | N:  28 | test Sqc: 1.1116 | test Neg: 0.0909\n",
      "epoch:    4 | step:   99 | N:  30 | test Sqc: 1.1522 | test Neg: 0.0607\n",
      "epoch:    4 | step:  199 | N:  30 | test Sqc: 1.1484 | test Neg: 0.0622\n",
      "epoch:    4 | step:   99 | N:  32 | test Sqc: 1.1997 | test Neg: 0.0383\n",
      "epoch:    4 | step:  199 | N:  32 | test Sqc: 1.1996 | test Neg: 0.0365\n",
      "epoch:    4 | step:   99 | N:  34 | test Sqc: 1.2251 | test Neg: 0.0210\n",
      "epoch:    4 | step:  199 | N:  34 | test Sqc: 1.2191 | test Neg: 0.0223\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | train Sqc: 0.8738 | loss: 1.3105\n",
      "epoch:    5 | step:  199 | train Sqc: 0.8725 | loss: 1.3103\n",
      "epoch:    5 | step:  299 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  399 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  499 | train Sqc: 0.8620 | loss: 1.3094\n",
      "epoch:    5 | step:  599 | train Sqc: 0.8634 | loss: 1.3096\n",
      "epoch:    5 | step:  699 | train Sqc: 0.8665 | loss: 1.3100\n",
      "epoch:    5 | step:  799 | train Sqc: 0.8648 | loss: 1.3099\n",
      "epoch:    5 | step:  899 | train Sqc: 0.8633 | loss: 1.3096\n",
      "epoch:    5 | step:  999 | train Sqc: 0.8603 | loss: 1.3093\n",
      "epoch:    5 | step:  1099 | train Sqc: 0.8610 | loss: 1.3093\n",
      "epoch:    5 | step:  1199 | train Sqc: 0.8616 | loss: 1.3094\n",
      "epoch:    5 | step:  1299 | train Sqc: 0.8632 | loss: 1.3096\n",
      "epoch:    5 | step:  1399 | train Sqc: 0.8633 | loss: 1.3097\n",
      "epoch:    5 | step:  1499 | train Sqc: 0.8647 | loss: 1.3098\n",
      "epoch:    5 | step:  1599 | train Sqc: 0.8646 | loss: 1.3098\n",
      "epoch:    5 | step:  1699 | train Sqc: 0.8652 | loss: 1.3099\n",
      "epoch:    5 | step:  1799 | train Sqc: 0.8650 | loss: 1.3099\n",
      "epoch:    5 | step:  1899 | train Sqc: 0.8648 | loss: 1.3099\n",
      "epoch:    5 | step:  1999 | train Sqc: 0.8643 | loss: 1.3097\n",
      "epoch:    5 | step:  2099 | train Sqc: 0.8644 | loss: 1.3097\n",
      "epoch:    5 | step:  2199 | train Sqc: 0.8643 | loss: 1.3096\n",
      "epoch:    5 | step:  2299 | train Sqc: 0.8642 | loss: 1.3096\n",
      "epoch:    5 | step:  2399 | train Sqc: 0.8646 | loss: 1.3097\n",
      "epoch:    5 | step:  2499 | train Sqc: 0.8642 | loss: 1.3097\n",
      "epoch:    5 | step:  2599 | train Sqc: 0.8645 | loss: 1.3097\n",
      "epoch:    5 | step:  2699 | train Sqc: 0.8648 | loss: 1.3097\n",
      "epoch:    5 | step:  2799 | train Sqc: 0.8641 | loss: 1.3096\n",
      "epoch:    5 | step:  2899 | train Sqc: 0.8641 | loss: 1.3096\n",
      "epoch:    5 | step:  2999 | train Sqc: 0.8638 | loss: 1.3095\n",
      "epoch:    5 | step:  3099 | train Sqc: 0.8637 | loss: 1.3095\n",
      "epoch:    5 | step:  3199 | train Sqc: 0.8637 | loss: 1.3095\n",
      "epoch:    5 | step:  3299 | train Sqc: 0.8633 | loss: 1.3094\n",
      "epoch:    5 | step:  3399 | train Sqc: 0.8625 | loss: 1.3094\n",
      "epoch:    5 | step:  3499 | train Sqc: 0.8625 | loss: 1.3094\n",
      "epoch:    5 | step:  3599 | train Sqc: 0.8632 | loss: 1.3095\n",
      "epoch:    5 | step:  3699 | train Sqc: 0.8630 | loss: 1.3095\n",
      "epoch:    5 | step:  3799 | train Sqc: 0.8635 | loss: 1.3095\n",
      "epoch:    5 | step:  3899 | train Sqc: 0.8643 | loss: 1.3096\n",
      "epoch:    5 | step:  3999 | train Sqc: 0.8649 | loss: 1.3097\n",
      "epoch:    5 | step:  4099 | train Sqc: 0.8644 | loss: 1.3096\n",
      "epoch:    5 | step:  4199 | train Sqc: 0.8638 | loss: 1.3096\n",
      "epoch:    5 | step:  4299 | train Sqc: 0.8637 | loss: 1.3095\n",
      "epoch:    5 | step:  4399 | train Sqc: 0.8643 | loss: 1.3096\n",
      "epoch:    5 | step:  4499 | train Sqc: 0.8637 | loss: 1.3095\n",
      "epoch:    5 | step:  4599 | train Sqc: 0.8640 | loss: 1.3096\n",
      "epoch:    5 | step:  4699 | train Sqc: 0.8638 | loss: 1.3095\n",
      "epoch:    5 | step:  4799 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  4899 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  4999 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  5099 | train Sqc: 0.8632 | loss: 1.3095\n",
      "epoch:    5 | step:  5199 | train Sqc: 0.8633 | loss: 1.3095\n",
      "epoch:    5 | step:  5299 | train Sqc: 0.8633 | loss: 1.3095\n",
      "epoch:    5 | step:  5399 | train Sqc: 0.8635 | loss: 1.3095\n",
      "epoch:    5 | step:  5499 | train Sqc: 0.8639 | loss: 1.3096\n",
      "epoch:    5 | step:  5599 | train Sqc: 0.8644 | loss: 1.3096\n",
      "epoch:    5 | step:  5699 | train Sqc: 0.8646 | loss: 1.3097\n",
      "epoch:    5 | step:  5799 | train Sqc: 0.8645 | loss: 1.3097\n",
      "epoch:    5 | step:  5899 | train Sqc: 0.8640 | loss: 1.3096\n",
      "epoch:    5 | step:  5999 | train Sqc: 0.8639 | loss: 1.3096\n",
      "epoch:    5 | step:  6099 | train Sqc: 0.8636 | loss: 1.3096\n",
      "epoch:    5 | step:  6199 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  6299 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  6399 | train Sqc: 0.8635 | loss: 1.3095\n",
      "epoch:    5 | step:  6499 | train Sqc: 0.8636 | loss: 1.3095\n",
      "epoch:    5 | step:  6599 | train Sqc: 0.8634 | loss: 1.3095\n",
      "epoch:    5 | step:  6699 | train Sqc: 0.8633 | loss: 1.3095\n",
      "epoch:    5 | step:  6799 | train Sqc: 0.8633 | loss: 1.3095\n",
      "epoch:    5 | step:  6899 | train Sqc: 0.8633 | loss: 1.3095\n",
      "epoch:    5 | step:  6999 | train Sqc: 0.8627 | loss: 1.3094\n",
      "epoch:    5 | step:  7099 | train Sqc: 0.8628 | loss: 1.3094\n",
      "epoch:    5 | step:  7199 | train Sqc: 0.8623 | loss: 1.3094\n",
      "epoch:    5 | step:  7299 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    5 | step:  7399 | train Sqc: 0.8622 | loss: 1.3093\n",
      "epoch:    5 | step:  7499 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    5 | step:  7599 | train Sqc: 0.8622 | loss: 1.3093\n",
      "epoch:    5 | step:  7699 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    5 | step:  7799 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  7899 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  7999 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  8099 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  8199 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  8299 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  8399 | train Sqc: 0.8618 | loss: 1.3092\n",
      "epoch:    5 | step:  8499 | train Sqc: 0.8619 | loss: 1.3092\n",
      "epoch:    5 | step:  8599 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  8699 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    5 | step:  8799 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  8899 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  8999 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  9099 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  9199 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    5 | step:  9299 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  9399 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  9499 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  9599 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  9699 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  9799 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  9899 | train Sqc: 0.8618 | loss: 1.3092\n",
      "epoch:    5 | step:  9999 | train Sqc: 0.8618 | loss: 1.3093\n",
      "epoch:    5 | step:  10099 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  10199 | train Sqc: 0.8617 | loss: 1.3093\n",
      "epoch:    5 | step:  10299 | train Sqc: 0.8618 | loss: 1.3093\n",
      "epoch:    5 | step:  10399 | train Sqc: 0.8618 | loss: 1.3093\n",
      "epoch:    5 | step:  10499 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    5 | step:  10599 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    5 | step:  10699 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  10799 | train Sqc: 0.8615 | loss: 1.3092\n",
      "epoch:    5 | step:  10899 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  10999 | train Sqc: 0.8618 | loss: 1.3093\n",
      "epoch:    5 | step:  11099 | train Sqc: 0.8618 | loss: 1.3093\n",
      "epoch:    5 | step:  11199 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    5 | step:  11299 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    5 | step:  11399 | train Sqc: 0.8622 | loss: 1.3093\n",
      "epoch:    5 | step:  11499 | train Sqc: 0.8621 | loss: 1.3093\n",
      "epoch:    5 | step:  11599 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    5 | step:  11699 | train Sqc: 0.8620 | loss: 1.3093\n",
      "epoch:    5 | step:  11799 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    5 | step:  11899 | train Sqc: 0.8619 | loss: 1.3093\n",
      "epoch:    5 | step:  11999 | train Sqc: 0.8618 | loss: 1.3092\n",
      "epoch:    5 | step:  12099 | train Sqc: 0.8617 | loss: 1.3092\n",
      "epoch:    5 | step:  12199 | train Sqc: 0.8619 | loss: 1.3092\n",
      "epoch:    5 | step:  12299 | train Sqc: 0.8619 | loss: 1.3092\n",
      "epoch:    5 | step:  12399 | train Sqc: 0.8618 | loss: 1.3092\n",
      "epoch:    5 | step:  12499 | train Sqc: 0.8616 | loss: 1.3092\n",
      "epoch:    5 | step:  12599 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    5 | step:  12699 | train Sqc: 0.8614 | loss: 1.3092\n",
      "epoch:    5 | step:  12799 | train Sqc: 0.8613 | loss: 1.3092\n",
      "epoch:    5 | step:  12899 | train Sqc: 0.8612 | loss: 1.3092\n",
      "epoch:    5 | step:  12999 | train Sqc: 0.8612 | loss: 1.3092\n",
      "epoch:    5 | step:  13099 | train Sqc: 0.8612 | loss: 1.3091\n",
      "epoch:    5 | step:  13199 | train Sqc: 0.8610 | loss: 1.3091\n",
      "epoch:    5 | step:  13299 | train Sqc: 0.8608 | loss: 1.3091\n",
      "epoch:    5 | step:  13399 | train Sqc: 0.8608 | loss: 1.3091\n",
      "epoch:    5 | step:  13499 | train Sqc: 0.8607 | loss: 1.3091\n",
      "epoch:    5 | step:  13599 | train Sqc: 0.8607 | loss: 1.3091\n",
      "epoch:    5 | step:  13699 | train Sqc: 0.8608 | loss: 1.3091\n",
      "epoch:    5 | step:  13799 | train Sqc: 0.8606 | loss: 1.3091\n",
      "epoch:    5 | step:  13899 | train Sqc: 0.8606 | loss: 1.3091\n",
      "epoch:    5 | step:  13999 | train Sqc: 0.8605 | loss: 1.3091\n",
      "epoch:    5 | step:  14099 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    5 | step:  14199 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    5 | step:  14299 | train Sqc: 0.8602 | loss: 1.3090\n",
      "epoch:    5 | step:  14399 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  14499 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  14599 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  14699 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  14799 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  14899 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  14999 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  15099 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  15199 | train Sqc: 0.8598 | loss: 1.3090\n",
      "epoch:    5 | step:  15299 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  15399 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  15499 | train Sqc: 0.8598 | loss: 1.3090\n",
      "epoch:    5 | step:  15599 | train Sqc: 0.8598 | loss: 1.3090\n",
      "epoch:    5 | step:  15699 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  15799 | train Sqc: 0.8598 | loss: 1.3090\n",
      "epoch:    5 | step:  15899 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  15999 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  16099 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    5 | step:  16199 | train Sqc: 0.8598 | loss: 1.3089\n",
      "epoch:    5 | step:  16299 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  16399 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  16499 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  16599 | train Sqc: 0.8600 | loss: 1.3090\n",
      "epoch:    5 | step:  16699 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    5 | step:  16799 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    5 | step:  16899 | train Sqc: 0.8601 | loss: 1.3090\n",
      "epoch:    5 | step:  16999 | train Sqc: 0.8599 | loss: 1.3090\n",
      "epoch:    5 | step:  17099 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  17199 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  17299 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  17399 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  17499 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  17599 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  17699 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  17799 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  17899 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  17999 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  18099 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  18199 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  18299 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  18399 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  18499 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  18599 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  18699 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  18799 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  18899 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  18999 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19099 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19199 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19299 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19399 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  19499 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19599 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19699 | train Sqc: 0.8597 | loss: 1.3089\n",
      "epoch:    5 | step:  19799 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  19899 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  19999 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  20099 | train Sqc: 0.8596 | loss: 1.3089\n",
      "epoch:    5 | step:  20199 | train Sqc: 0.8595 | loss: 1.3089\n",
      "epoch:    5 | step:  20299 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  20399 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  20499 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  20599 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  20699 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  20799 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  20899 | train Sqc: 0.8594 | loss: 1.3089\n",
      "epoch:    5 | step:  20999 | train Sqc: 0.8593 | loss: 1.3089\n",
      "epoch:    5 | step:  21099 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  21199 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  21299 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  21399 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  21499 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  21599 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  21699 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  21799 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  21899 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  21999 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  22099 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22199 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22299 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22399 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22499 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22599 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  22699 | train Sqc: 0.8592 | loss: 1.3089\n",
      "epoch:    5 | step:  22799 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  22899 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  22999 | train Sqc: 0.8591 | loss: 1.3089\n",
      "epoch:    5 | step:  23099 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  23199 | train Sqc: 0.8589 | loss: 1.3089\n",
      "epoch:    5 | step:  23299 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    5 | step:  23399 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    5 | step:  23499 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  23599 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  23699 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  23799 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  23899 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  23999 | train Sqc: 0.8588 | loss: 1.3088\n",
      "epoch:    5 | step:  24099 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    5 | step:  24199 | train Sqc: 0.8589 | loss: 1.3089\n",
      "epoch:    5 | step:  24299 | train Sqc: 0.8590 | loss: 1.3089\n",
      "epoch:    5 | step:  24399 | train Sqc: 0.8589 | loss: 1.3089\n",
      "epoch:    5 | step:  24499 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    5 | step:  24599 | train Sqc: 0.8588 | loss: 1.3089\n",
      "epoch:    5 | step:  24699 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  24799 | train Sqc: 0.8587 | loss: 1.3088\n",
      "epoch:    5 | step:  24899 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  24999 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  25099 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  25199 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  25299 | train Sqc: 0.8586 | loss: 1.3088\n",
      "epoch:    5 | step:  25399 | train Sqc: 0.8585 | loss: 1.3088\n",
      "epoch:    5 | step:  25499 | train Sqc: 0.8583 | loss: 1.3088\n",
      "epoch:    5 | step:  25599 | train Sqc: 0.8584 | loss: 1.3088\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | test Sqc: 0.3570 | test Neg: 0.4202\n",
      "epoch:    5 | step:  199 | N:  4 | test Sqc: 0.3730 | test Neg: 0.4146\n",
      "epoch:    5 | step:   99 | N:  6 | test Sqc: 0.4068 | test Neg: 0.4073\n",
      "epoch:    5 | step:  199 | N:  6 | test Sqc: 0.4193 | test Neg: 0.4027\n",
      "epoch:    5 | step:   99 | N:  8 | test Sqc: 0.5263 | test Neg: 0.3688\n",
      "epoch:    5 | step:  199 | N:  8 | test Sqc: 0.5247 | test Neg: 0.3688\n",
      "epoch:    5 | step:   99 | N:  10 | test Sqc: 0.6342 | test Neg: 0.3260\n",
      "epoch:    5 | step:  199 | N:  10 | test Sqc: 0.6385 | test Neg: 0.3246\n",
      "epoch:    5 | step:   99 | N:  12 | test Sqc: 0.7280 | test Neg: 0.2931\n",
      "epoch:    5 | step:  199 | N:  12 | test Sqc: 0.7281 | test Neg: 0.2913\n",
      "epoch:    5 | step:   99 | N:  14 | test Sqc: 0.7331 | test Neg: 0.2889\n",
      "epoch:    5 | step:  199 | N:  14 | test Sqc: 0.7502 | test Neg: 0.2808\n",
      "epoch:    5 | step:   99 | N:  16 | test Sqc: 0.7953 | test Neg: 0.2622\n",
      "epoch:    5 | step:  199 | N:  16 | test Sqc: 0.8060 | test Neg: 0.2568\n",
      "epoch:    5 | step:   99 | N:  18 | test Sqc: 0.8493 | test Neg: 0.2377\n",
      "epoch:    5 | step:  199 | N:  18 | test Sqc: 0.8506 | test Neg: 0.2351\n",
      "epoch:    5 | step:   99 | N:  20 | test Sqc: 0.9250 | test Neg: 0.1986\n",
      "epoch:    5 | step:  199 | N:  20 | test Sqc: 0.9237 | test Neg: 0.1986\n",
      "epoch:    5 | step:   99 | N:  22 | test Sqc: 0.9983 | test Neg: 0.1585\n",
      "epoch:    5 | step:  199 | N:  22 | test Sqc: 0.9933 | test Neg: 0.1609\n",
      "epoch:    5 | step:   99 | N:  24 | test Sqc: 1.0484 | test Neg: 0.1331\n",
      "epoch:    5 | step:  199 | N:  24 | test Sqc: 1.0473 | test Neg: 0.1332\n",
      "epoch:    5 | step:   99 | N:  26 | test Sqc: 1.0832 | test Neg: 0.1100\n",
      "epoch:    5 | step:  199 | N:  26 | test Sqc: 1.0805 | test Neg: 0.1108\n",
      "epoch:    5 | step:   99 | N:  28 | test Sqc: 1.0955 | test Neg: 0.1038\n",
      "epoch:    5 | step:  199 | N:  28 | test Sqc: 1.1000 | test Neg: 0.0992\n",
      "epoch:    5 | step:   99 | N:  30 | test Sqc: 1.1348 | test Neg: 0.0735\n",
      "epoch:    5 | step:  199 | N:  30 | test Sqc: 1.1313 | test Neg: 0.0770\n",
      "epoch:    5 | step:   99 | N:  32 | test Sqc: 1.1648 | test Neg: 0.0544\n",
      "epoch:    5 | step:  199 | N:  32 | test Sqc: 1.1655 | test Neg: 0.0524\n",
      "epoch:    5 | step:   99 | N:  34 | test Sqc: 1.2091 | test Neg: 0.0284\n",
      "epoch:    5 | step:  199 | N:  34 | test Sqc: 1.1979 | test Neg: 0.0309\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | train Sqc: 0.8624 | loss: 1.3094\n",
      "epoch:    6 | step:  199 | train Sqc: 0.8630 | loss: 1.3092\n",
      "epoch:    6 | step:  299 | train Sqc: 0.8514 | loss: 1.3080\n",
      "epoch:    6 | step:  399 | train Sqc: 0.8545 | loss: 1.3085\n",
      "epoch:    6 | step:  499 | train Sqc: 0.8553 | loss: 1.3086\n",
      "epoch:    6 | step:  599 | train Sqc: 0.8570 | loss: 1.3088\n",
      "epoch:    6 | step:  699 | train Sqc: 0.8594 | loss: 1.3090\n",
      "epoch:    6 | step:  799 | train Sqc: 0.8576 | loss: 1.3089\n",
      "epoch:    6 | step:  899 | train Sqc: 0.8548 | loss: 1.3085\n",
      "epoch:    6 | step:  999 | train Sqc: 0.8525 | loss: 1.3082\n",
      "epoch:    6 | step:  1099 | train Sqc: 0.8529 | loss: 1.3082\n",
      "epoch:    6 | step:  1199 | train Sqc: 0.8534 | loss: 1.3083\n",
      "epoch:    6 | step:  1299 | train Sqc: 0.8557 | loss: 1.3086\n",
      "epoch:    6 | step:  1399 | train Sqc: 0.8552 | loss: 1.3086\n",
      "epoch:    6 | step:  1499 | train Sqc: 0.8557 | loss: 1.3086\n",
      "epoch:    6 | step:  1599 | train Sqc: 0.8553 | loss: 1.3085\n",
      "epoch:    6 | step:  1699 | train Sqc: 0.8556 | loss: 1.3085\n",
      "epoch:    6 | step:  1799 | train Sqc: 0.8560 | loss: 1.3086\n",
      "epoch:    6 | step:  1899 | train Sqc: 0.8559 | loss: 1.3086\n",
      "epoch:    6 | step:  1999 | train Sqc: 0.8554 | loss: 1.3085\n",
      "epoch:    6 | step:  2099 | train Sqc: 0.8558 | loss: 1.3085\n",
      "epoch:    6 | step:  2199 | train Sqc: 0.8556 | loss: 1.3084\n",
      "epoch:    6 | step:  2299 | train Sqc: 0.8554 | loss: 1.3083\n",
      "epoch:    6 | step:  2399 | train Sqc: 0.8559 | loss: 1.3084\n",
      "epoch:    6 | step:  2499 | train Sqc: 0.8558 | loss: 1.3084\n",
      "epoch:    6 | step:  2599 | train Sqc: 0.8561 | loss: 1.3085\n",
      "epoch:    6 | step:  2699 | train Sqc: 0.8561 | loss: 1.3085\n",
      "epoch:    6 | step:  2799 | train Sqc: 0.8554 | loss: 1.3084\n",
      "epoch:    6 | step:  2899 | train Sqc: 0.8555 | loss: 1.3083\n",
      "epoch:    6 | step:  2999 | train Sqc: 0.8553 | loss: 1.3083\n",
      "epoch:    6 | step:  3099 | train Sqc: 0.8551 | loss: 1.3083\n",
      "epoch:    6 | step:  3199 | train Sqc: 0.8551 | loss: 1.3083\n",
      "epoch:    6 | step:  3299 | train Sqc: 0.8549 | loss: 1.3083\n",
      "epoch:    6 | step:  3399 | train Sqc: 0.8542 | loss: 1.3082\n",
      "epoch:    6 | step:  3499 | train Sqc: 0.8544 | loss: 1.3082\n",
      "epoch:    6 | step:  3599 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    6 | step:  3699 | train Sqc: 0.8544 | loss: 1.3082\n",
      "epoch:    6 | step:  3799 | train Sqc: 0.8549 | loss: 1.3083\n",
      "epoch:    6 | step:  3899 | train Sqc: 0.8555 | loss: 1.3084\n",
      "epoch:    6 | step:  3999 | train Sqc: 0.8558 | loss: 1.3084\n",
      "epoch:    6 | step:  4099 | train Sqc: 0.8552 | loss: 1.3083\n",
      "epoch:    6 | step:  4199 | train Sqc: 0.8545 | loss: 1.3082\n",
      "epoch:    6 | step:  4299 | train Sqc: 0.8546 | loss: 1.3082\n",
      "epoch:    6 | step:  4399 | train Sqc: 0.8553 | loss: 1.3083\n",
      "epoch:    6 | step:  4499 | train Sqc: 0.8549 | loss: 1.3082\n",
      "epoch:    6 | step:  4599 | train Sqc: 0.8552 | loss: 1.3083\n",
      "epoch:    6 | step:  4699 | train Sqc: 0.8549 | loss: 1.3083\n",
      "epoch:    6 | step:  4799 | train Sqc: 0.8547 | loss: 1.3082\n",
      "epoch:    6 | step:  4899 | train Sqc: 0.8547 | loss: 1.3082\n",
      "epoch:    6 | step:  4999 | train Sqc: 0.8549 | loss: 1.3082\n",
      "epoch:    6 | step:  5099 | train Sqc: 0.8548 | loss: 1.3082\n",
      "epoch:    6 | step:  5199 | train Sqc: 0.8549 | loss: 1.3082\n",
      "epoch:    6 | step:  5299 | train Sqc: 0.8549 | loss: 1.3083\n",
      "epoch:    6 | step:  5399 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    6 | step:  5499 | train Sqc: 0.8553 | loss: 1.3083\n",
      "epoch:    6 | step:  5599 | train Sqc: 0.8558 | loss: 1.3084\n",
      "epoch:    6 | step:  5699 | train Sqc: 0.8557 | loss: 1.3084\n",
      "epoch:    6 | step:  5799 | train Sqc: 0.8556 | loss: 1.3084\n",
      "epoch:    6 | step:  5899 | train Sqc: 0.8552 | loss: 1.3083\n",
      "epoch:    6 | step:  5999 | train Sqc: 0.8551 | loss: 1.3083\n",
      "epoch:    6 | step:  6099 | train Sqc: 0.8548 | loss: 1.3083\n",
      "epoch:    6 | step:  6199 | train Sqc: 0.8548 | loss: 1.3083\n",
      "epoch:    6 | step:  6299 | train Sqc: 0.8549 | loss: 1.3083\n",
      "epoch:    6 | step:  6399 | train Sqc: 0.8551 | loss: 1.3083\n",
      "epoch:    6 | step:  6499 | train Sqc: 0.8551 | loss: 1.3083\n",
      "epoch:    6 | step:  6599 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    6 | step:  6699 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    6 | step:  6799 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    6 | step:  6899 | train Sqc: 0.8548 | loss: 1.3082\n",
      "epoch:    6 | step:  6999 | train Sqc: 0.8544 | loss: 1.3082\n",
      "epoch:    6 | step:  7099 | train Sqc: 0.8545 | loss: 1.3082\n",
      "epoch:    6 | step:  7199 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    6 | step:  7299 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    6 | step:  7399 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    6 | step:  7499 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    6 | step:  7599 | train Sqc: 0.8542 | loss: 1.3081\n",
      "epoch:    6 | step:  7699 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    6 | step:  7799 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  7899 | train Sqc: 0.8538 | loss: 1.3081\n",
      "epoch:    6 | step:  7999 | train Sqc: 0.8538 | loss: 1.3081\n",
      "epoch:    6 | step:  8099 | train Sqc: 0.8536 | loss: 1.3081\n",
      "epoch:    6 | step:  8199 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  8299 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  8399 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    6 | step:  8499 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    6 | step:  8599 | train Sqc: 0.8538 | loss: 1.3081\n",
      "epoch:    6 | step:  8699 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  8799 | train Sqc: 0.8538 | loss: 1.3081\n",
      "epoch:    6 | step:  8899 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  8999 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  9099 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  9199 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  9299 | train Sqc: 0.8536 | loss: 1.3080\n",
      "epoch:    6 | step:  9399 | train Sqc: 0.8536 | loss: 1.3080\n",
      "epoch:    6 | step:  9499 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  9599 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  9699 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  9799 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  9899 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  9999 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  10099 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  10199 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  10299 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  10399 | train Sqc: 0.8535 | loss: 1.3081\n",
      "epoch:    6 | step:  10499 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  10599 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    6 | step:  10699 | train Sqc: 0.8535 | loss: 1.3081\n",
      "epoch:    6 | step:  10799 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  10899 | train Sqc: 0.8532 | loss: 1.3080\n",
      "epoch:    6 | step:  10999 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  11099 | train Sqc: 0.8532 | loss: 1.3080\n",
      "epoch:    6 | step:  11199 | train Sqc: 0.8532 | loss: 1.3080\n",
      "epoch:    6 | step:  11299 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  11399 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  11499 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  11599 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  11699 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  11799 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  11899 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  11999 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  12099 | train Sqc: 0.8532 | loss: 1.3080\n",
      "epoch:    6 | step:  12199 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  12299 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    6 | step:  12399 | train Sqc: 0.8534 | loss: 1.3080\n",
      "epoch:    6 | step:  12499 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    6 | step:  12599 | train Sqc: 0.8530 | loss: 1.3080\n",
      "epoch:    6 | step:  12699 | train Sqc: 0.8531 | loss: 1.3080\n",
      "epoch:    6 | step:  12799 | train Sqc: 0.8531 | loss: 1.3080\n",
      "epoch:    6 | step:  12899 | train Sqc: 0.8530 | loss: 1.3080\n",
      "epoch:    6 | step:  12999 | train Sqc: 0.8531 | loss: 1.3080\n",
      "epoch:    6 | step:  13099 | train Sqc: 0.8531 | loss: 1.3080\n",
      "epoch:    6 | step:  13199 | train Sqc: 0.8529 | loss: 1.3080\n",
      "epoch:    6 | step:  13299 | train Sqc: 0.8528 | loss: 1.3079\n",
      "epoch:    6 | step:  13399 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    6 | step:  13499 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    6 | step:  13599 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    6 | step:  13699 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    6 | step:  13799 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  13899 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    6 | step:  13999 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    6 | step:  14099 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  14199 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  14299 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  14399 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  14499 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  14599 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  14699 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  14799 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  14899 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  14999 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  15099 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  15199 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15299 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15399 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15499 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15599 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15699 | train Sqc: 0.8519 | loss: 1.3079\n",
      "epoch:    6 | step:  15799 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  15899 | train Sqc: 0.8519 | loss: 1.3078\n",
      "epoch:    6 | step:  15999 | train Sqc: 0.8520 | loss: 1.3078\n",
      "epoch:    6 | step:  16099 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  16199 | train Sqc: 0.8521 | loss: 1.3078\n",
      "epoch:    6 | step:  16299 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  16399 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  16499 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  16599 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  16699 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  16799 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  16899 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  16999 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  17099 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  17199 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  17299 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  17399 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  17499 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  17599 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  17699 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  17799 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  17899 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  17999 | train Sqc: 0.8520 | loss: 1.3078\n",
      "epoch:    6 | step:  18099 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  18199 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  18299 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  18399 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  18499 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  18599 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  18699 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  18799 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  18899 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  18999 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  19099 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  19199 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  19299 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  19399 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  19499 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  19599 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  19699 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  19799 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  19899 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  19999 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  20099 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  20199 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  20299 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  20399 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  20499 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  20599 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  20699 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  20799 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  20899 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  20999 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  21099 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21199 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21299 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21399 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21499 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  21599 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21699 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  21799 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  21899 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  21999 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  22099 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  22199 | train Sqc: 0.8525 | loss: 1.3080\n",
      "epoch:    6 | step:  22299 | train Sqc: 0.8525 | loss: 1.3080\n",
      "epoch:    6 | step:  22399 | train Sqc: 0.8525 | loss: 1.3080\n",
      "epoch:    6 | step:  22499 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  22599 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  22699 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    6 | step:  22799 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  22899 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  22999 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  23099 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  23199 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  23299 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  23399 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  23499 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  23599 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  23699 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  23799 | train Sqc: 0.8520 | loss: 1.3079\n",
      "epoch:    6 | step:  23899 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  23999 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  24099 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  24199 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  24299 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    6 | step:  24399 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  24499 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  24599 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    6 | step:  24699 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  24799 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  24899 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  24999 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  25099 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  25199 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  25299 | train Sqc: 0.8522 | loss: 1.3079\n",
      "epoch:    6 | step:  25399 | train Sqc: 0.8521 | loss: 1.3079\n",
      "epoch:    6 | step:  25499 | train Sqc: 0.8519 | loss: 1.3079\n",
      "epoch:    6 | step:  25599 | train Sqc: 0.8519 | loss: 1.3079\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | test Sqc: 0.3542 | test Neg: 0.4202\n",
      "epoch:    6 | step:  199 | N:  4 | test Sqc: 0.3693 | test Neg: 0.4146\n",
      "epoch:    6 | step:   99 | N:  6 | test Sqc: 0.4019 | test Neg: 0.4072\n",
      "epoch:    6 | step:  199 | N:  6 | test Sqc: 0.4156 | test Neg: 0.4027\n",
      "epoch:    6 | step:   99 | N:  8 | test Sqc: 0.5258 | test Neg: 0.3687\n",
      "epoch:    6 | step:  199 | N:  8 | test Sqc: 0.5263 | test Neg: 0.3686\n",
      "epoch:    6 | step:   99 | N:  10 | test Sqc: 0.6394 | test Neg: 0.3258\n",
      "epoch:    6 | step:  199 | N:  10 | test Sqc: 0.6406 | test Neg: 0.3244\n",
      "epoch:    6 | step:   99 | N:  12 | test Sqc: 0.7239 | test Neg: 0.2938\n",
      "epoch:    6 | step:  199 | N:  12 | test Sqc: 0.7263 | test Neg: 0.2920\n",
      "epoch:    6 | step:   99 | N:  14 | test Sqc: 0.7304 | test Neg: 0.2896\n",
      "epoch:    6 | step:  199 | N:  14 | test Sqc: 0.7469 | test Neg: 0.2815\n",
      "epoch:    6 | step:   99 | N:  16 | test Sqc: 0.7954 | test Neg: 0.2619\n",
      "epoch:    6 | step:  199 | N:  16 | test Sqc: 0.8065 | test Neg: 0.2565\n",
      "epoch:    6 | step:   99 | N:  18 | test Sqc: 0.8505 | test Neg: 0.2373\n",
      "epoch:    6 | step:  199 | N:  18 | test Sqc: 0.8512 | test Neg: 0.2349\n",
      "epoch:    6 | step:   99 | N:  20 | test Sqc: 0.9256 | test Neg: 0.1976\n",
      "epoch:    6 | step:  199 | N:  20 | test Sqc: 0.9244 | test Neg: 0.1978\n",
      "epoch:    6 | step:   99 | N:  22 | test Sqc: 0.9957 | test Neg: 0.1606\n",
      "epoch:    6 | step:  199 | N:  22 | test Sqc: 0.9907 | test Neg: 0.1624\n",
      "epoch:    6 | step:   99 | N:  24 | test Sqc: 1.0444 | test Neg: 0.1343\n",
      "epoch:    6 | step:  199 | N:  24 | test Sqc: 1.0431 | test Neg: 0.1343\n",
      "epoch:    6 | step:   99 | N:  26 | test Sqc: 1.0755 | test Neg: 0.1112\n",
      "epoch:    6 | step:  199 | N:  26 | test Sqc: 1.0759 | test Neg: 0.1109\n",
      "epoch:    6 | step:   99 | N:  28 | test Sqc: 1.0896 | test Neg: 0.1052\n",
      "epoch:    6 | step:  199 | N:  28 | test Sqc: 1.0954 | test Neg: 0.1003\n",
      "epoch:    6 | step:   99 | N:  30 | test Sqc: 1.1274 | test Neg: 0.0776\n",
      "epoch:    6 | step:  199 | N:  30 | test Sqc: 1.1215 | test Neg: 0.0819\n",
      "epoch:    6 | step:   99 | N:  32 | test Sqc: 1.1582 | test Neg: 0.0548\n",
      "epoch:    6 | step:  199 | N:  32 | test Sqc: 1.1587 | test Neg: 0.0537\n",
      "epoch:    6 | step:   99 | N:  34 | test Sqc: 1.1930 | test Neg: 0.0343\n",
      "epoch:    6 | step:  199 | N:  34 | test Sqc: 1.1843 | test Neg: 0.0364\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | train Sqc: 0.8562 | loss: 1.3081\n",
      "epoch:    7 | step:  199 | train Sqc: 0.8565 | loss: 1.3081\n",
      "epoch:    7 | step:  299 | train Sqc: 0.8443 | loss: 1.3068\n",
      "epoch:    7 | step:  399 | train Sqc: 0.8472 | loss: 1.3073\n",
      "epoch:    7 | step:  499 | train Sqc: 0.8474 | loss: 1.3074\n",
      "epoch:    7 | step:  599 | train Sqc: 0.8507 | loss: 1.3079\n",
      "epoch:    7 | step:  699 | train Sqc: 0.8548 | loss: 1.3084\n",
      "epoch:    7 | step:  799 | train Sqc: 0.8539 | loss: 1.3084\n",
      "epoch:    7 | step:  899 | train Sqc: 0.8518 | loss: 1.3081\n",
      "epoch:    7 | step:  999 | train Sqc: 0.8498 | loss: 1.3079\n",
      "epoch:    7 | step:  1099 | train Sqc: 0.8505 | loss: 1.3080\n",
      "epoch:    7 | step:  1199 | train Sqc: 0.8511 | loss: 1.3080\n",
      "epoch:    7 | step:  1299 | train Sqc: 0.8534 | loss: 1.3083\n",
      "epoch:    7 | step:  1399 | train Sqc: 0.8527 | loss: 1.3083\n",
      "epoch:    7 | step:  1499 | train Sqc: 0.8535 | loss: 1.3083\n",
      "epoch:    7 | step:  1599 | train Sqc: 0.8529 | loss: 1.3082\n",
      "epoch:    7 | step:  1699 | train Sqc: 0.8533 | loss: 1.3082\n",
      "epoch:    7 | step:  1799 | train Sqc: 0.8534 | loss: 1.3082\n",
      "epoch:    7 | step:  1899 | train Sqc: 0.8540 | loss: 1.3083\n",
      "epoch:    7 | step:  1999 | train Sqc: 0.8536 | loss: 1.3082\n",
      "epoch:    7 | step:  2099 | train Sqc: 0.8539 | loss: 1.3082\n",
      "epoch:    7 | step:  2199 | train Sqc: 0.8537 | loss: 1.3081\n",
      "epoch:    7 | step:  2299 | train Sqc: 0.8535 | loss: 1.3081\n",
      "epoch:    7 | step:  2399 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    7 | step:  2499 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    7 | step:  2599 | train Sqc: 0.8542 | loss: 1.3082\n",
      "epoch:    7 | step:  2699 | train Sqc: 0.8543 | loss: 1.3082\n",
      "epoch:    7 | step:  2799 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    7 | step:  2899 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    7 | step:  2999 | train Sqc: 0.8538 | loss: 1.3080\n",
      "epoch:    7 | step:  3099 | train Sqc: 0.8537 | loss: 1.3080\n",
      "epoch:    7 | step:  3199 | train Sqc: 0.8537 | loss: 1.3080\n",
      "epoch:    7 | step:  3299 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    7 | step:  3399 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    7 | step:  3499 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    7 | step:  3599 | train Sqc: 0.8531 | loss: 1.3080\n",
      "epoch:    7 | step:  3699 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    7 | step:  3799 | train Sqc: 0.8530 | loss: 1.3079\n",
      "epoch:    7 | step:  3899 | train Sqc: 0.8537 | loss: 1.3080\n",
      "epoch:    7 | step:  3999 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    7 | step:  4099 | train Sqc: 0.8536 | loss: 1.3080\n",
      "epoch:    7 | step:  4199 | train Sqc: 0.8535 | loss: 1.3080\n",
      "epoch:    7 | step:  4299 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    7 | step:  4399 | train Sqc: 0.8546 | loss: 1.3082\n",
      "epoch:    7 | step:  4499 | train Sqc: 0.8542 | loss: 1.3081\n",
      "epoch:    7 | step:  4599 | train Sqc: 0.8545 | loss: 1.3082\n",
      "epoch:    7 | step:  4699 | train Sqc: 0.8543 | loss: 1.3082\n",
      "epoch:    7 | step:  4799 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    7 | step:  4899 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    7 | step:  4999 | train Sqc: 0.8543 | loss: 1.3081\n",
      "epoch:    7 | step:  5099 | train Sqc: 0.8542 | loss: 1.3081\n",
      "epoch:    7 | step:  5199 | train Sqc: 0.8543 | loss: 1.3081\n",
      "epoch:    7 | step:  5299 | train Sqc: 0.8544 | loss: 1.3082\n",
      "epoch:    7 | step:  5399 | train Sqc: 0.8544 | loss: 1.3082\n",
      "epoch:    7 | step:  5499 | train Sqc: 0.8546 | loss: 1.3082\n",
      "epoch:    7 | step:  5599 | train Sqc: 0.8552 | loss: 1.3083\n",
      "epoch:    7 | step:  5699 | train Sqc: 0.8552 | loss: 1.3083\n",
      "epoch:    7 | step:  5799 | train Sqc: 0.8550 | loss: 1.3083\n",
      "epoch:    7 | step:  5899 | train Sqc: 0.8546 | loss: 1.3082\n",
      "epoch:    7 | step:  5999 | train Sqc: 0.8545 | loss: 1.3082\n",
      "epoch:    7 | step:  6099 | train Sqc: 0.8541 | loss: 1.3082\n",
      "epoch:    7 | step:  6199 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    7 | step:  6299 | train Sqc: 0.8541 | loss: 1.3081\n",
      "epoch:    7 | step:  6399 | train Sqc: 0.8542 | loss: 1.3081\n",
      "epoch:    7 | step:  6499 | train Sqc: 0.8542 | loss: 1.3082\n",
      "epoch:    7 | step:  6599 | train Sqc: 0.8540 | loss: 1.3081\n",
      "epoch:    7 | step:  6699 | train Sqc: 0.8539 | loss: 1.3081\n",
      "epoch:    7 | step:  6799 | train Sqc: 0.8538 | loss: 1.3081\n",
      "epoch:    7 | step:  6899 | train Sqc: 0.8537 | loss: 1.3080\n",
      "epoch:    7 | step:  6999 | train Sqc: 0.8532 | loss: 1.3080\n",
      "epoch:    7 | step:  7099 | train Sqc: 0.8533 | loss: 1.3080\n",
      "epoch:    7 | step:  7199 | train Sqc: 0.8528 | loss: 1.3079\n",
      "epoch:    7 | step:  7299 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    7 | step:  7399 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    7 | step:  7499 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    7 | step:  7599 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    7 | step:  7699 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  7799 | train Sqc: 0.8521 | loss: 1.3078\n",
      "epoch:    7 | step:  7899 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  7999 | train Sqc: 0.8526 | loss: 1.3079\n",
      "epoch:    7 | step:  8099 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  8199 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    7 | step:  8299 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  8399 | train Sqc: 0.8527 | loss: 1.3079\n",
      "epoch:    7 | step:  8499 | train Sqc: 0.8528 | loss: 1.3079\n",
      "epoch:    7 | step:  8599 | train Sqc: 0.8525 | loss: 1.3079\n",
      "epoch:    7 | step:  8699 | train Sqc: 0.8523 | loss: 1.3079\n",
      "epoch:    7 | step:  8799 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  8899 | train Sqc: 0.8524 | loss: 1.3079\n",
      "epoch:    7 | step:  8999 | train Sqc: 0.8522 | loss: 1.3078\n",
      "epoch:    7 | step:  9099 | train Sqc: 0.8522 | loss: 1.3078\n",
      "epoch:    7 | step:  9199 | train Sqc: 0.8519 | loss: 1.3078\n",
      "epoch:    7 | step:  9299 | train Sqc: 0.8521 | loss: 1.3078\n",
      "epoch:    7 | step:  9399 | train Sqc: 0.8521 | loss: 1.3078\n",
      "epoch:    7 | step:  9499 | train Sqc: 0.8518 | loss: 1.3078\n",
      "epoch:    7 | step:  9599 | train Sqc: 0.8517 | loss: 1.3078\n",
      "epoch:    7 | step:  9699 | train Sqc: 0.8517 | loss: 1.3078\n",
      "epoch:    7 | step:  9799 | train Sqc: 0.8517 | loss: 1.3078\n",
      "epoch:    7 | step:  9899 | train Sqc: 0.8517 | loss: 1.3078\n",
      "epoch:    7 | step:  9999 | train Sqc: 0.8518 | loss: 1.3078\n",
      "epoch:    7 | step:  10099 | train Sqc: 0.8516 | loss: 1.3078\n",
      "epoch:    7 | step:  10199 | train Sqc: 0.8516 | loss: 1.3078\n",
      "epoch:    7 | step:  10299 | train Sqc: 0.8516 | loss: 1.3078\n",
      "epoch:    7 | step:  10399 | train Sqc: 0.8516 | loss: 1.3078\n",
      "epoch:    7 | step:  10499 | train Sqc: 0.8518 | loss: 1.3078\n",
      "epoch:    7 | step:  10599 | train Sqc: 0.8516 | loss: 1.3078\n",
      "epoch:    7 | step:  10699 | train Sqc: 0.8514 | loss: 1.3078\n",
      "epoch:    7 | step:  10799 | train Sqc: 0.8512 | loss: 1.3077\n",
      "epoch:    7 | step:  10899 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  10999 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  11099 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  11199 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  11299 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  11399 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  11499 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  11599 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  11699 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  11799 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  11899 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  11999 | train Sqc: 0.8509 | loss: 1.3077\n",
      "epoch:    7 | step:  12099 | train Sqc: 0.8508 | loss: 1.3077\n",
      "epoch:    7 | step:  12199 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  12299 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  12399 | train Sqc: 0.8509 | loss: 1.3077\n",
      "epoch:    7 | step:  12499 | train Sqc: 0.8507 | loss: 1.3076\n",
      "epoch:    7 | step:  12599 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  12699 | train Sqc: 0.8506 | loss: 1.3076\n",
      "epoch:    7 | step:  12799 | train Sqc: 0.8506 | loss: 1.3076\n",
      "epoch:    7 | step:  12899 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  12999 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  13099 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  13199 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  13299 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  13399 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  13499 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  13599 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  13699 | train Sqc: 0.8506 | loss: 1.3076\n",
      "epoch:    7 | step:  13799 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  13899 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  13999 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  14099 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  14199 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  14299 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  14399 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  14499 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  14599 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  14699 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  14799 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  14899 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  14999 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  15099 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  15199 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  15299 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  15399 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  15499 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  15599 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  15699 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  15799 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  15899 | train Sqc: 0.8499 | loss: 1.3076\n",
      "epoch:    7 | step:  15999 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    7 | step:  16099 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  16199 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  16299 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  16399 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  16499 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  16599 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  16699 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  16799 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  16899 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  16999 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  17099 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  17199 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  17299 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  17399 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  17499 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  17599 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  17699 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  17799 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  17899 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  17999 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18099 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18199 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  18299 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  18399 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  18499 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18599 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18699 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  18799 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18899 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    7 | step:  18999 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    7 | step:  19099 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  19199 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  19299 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  19399 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  19499 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  19599 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  19699 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  19799 | train Sqc: 0.8505 | loss: 1.3076\n",
      "epoch:    7 | step:  19899 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  19999 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20099 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20199 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20299 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  20399 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  20499 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  20599 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  20699 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20799 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20899 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  20999 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  21099 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    7 | step:  21199 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21299 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21399 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21499 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21599 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21699 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    7 | step:  21799 | train Sqc: 0.8504 | loss: 1.3077\n",
      "epoch:    7 | step:  21899 | train Sqc: 0.8504 | loss: 1.3077\n",
      "epoch:    7 | step:  21999 | train Sqc: 0.8504 | loss: 1.3077\n",
      "epoch:    7 | step:  22099 | train Sqc: 0.8505 | loss: 1.3077\n",
      "epoch:    7 | step:  22199 | train Sqc: 0.8506 | loss: 1.3077\n",
      "epoch:    7 | step:  22299 | train Sqc: 0.8508 | loss: 1.3077\n",
      "epoch:    7 | step:  22399 | train Sqc: 0.8509 | loss: 1.3077\n",
      "epoch:    7 | step:  22499 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  22599 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  22699 | train Sqc: 0.8511 | loss: 1.3077\n",
      "epoch:    7 | step:  22799 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  22899 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  22999 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23099 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23199 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23299 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23399 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23499 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  23599 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  23699 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  23799 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    7 | step:  23899 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  23999 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  24099 | train Sqc: 0.8512 | loss: 1.3078\n",
      "epoch:    7 | step:  24199 | train Sqc: 0.8512 | loss: 1.3078\n",
      "epoch:    7 | step:  24299 | train Sqc: 0.8513 | loss: 1.3078\n",
      "epoch:    7 | step:  24399 | train Sqc: 0.8513 | loss: 1.3078\n",
      "epoch:    7 | step:  24499 | train Sqc: 0.8512 | loss: 1.3078\n",
      "epoch:    7 | step:  24599 | train Sqc: 0.8512 | loss: 1.3078\n",
      "epoch:    7 | step:  24699 | train Sqc: 0.8512 | loss: 1.3078\n",
      "epoch:    7 | step:  24799 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  24899 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  24999 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  25099 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  25199 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  25299 | train Sqc: 0.8511 | loss: 1.3078\n",
      "epoch:    7 | step:  25399 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    7 | step:  25499 | train Sqc: 0.8508 | loss: 1.3077\n",
      "epoch:    7 | step:  25599 | train Sqc: 0.8509 | loss: 1.3077\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | test Sqc: 0.3564 | test Neg: 0.4202\n",
      "epoch:    7 | step:  199 | N:  4 | test Sqc: 0.3708 | test Neg: 0.4146\n",
      "epoch:    7 | step:   99 | N:  6 | test Sqc: 0.4236 | test Neg: 0.4061\n",
      "epoch:    7 | step:  199 | N:  6 | test Sqc: 0.4373 | test Neg: 0.4014\n",
      "epoch:    7 | step:   99 | N:  8 | test Sqc: 0.5314 | test Neg: 0.3687\n",
      "epoch:    7 | step:  199 | N:  8 | test Sqc: 0.5298 | test Neg: 0.3685\n",
      "epoch:    7 | step:   99 | N:  10 | test Sqc: 0.6362 | test Neg: 0.3263\n",
      "epoch:    7 | step:  199 | N:  10 | test Sqc: 0.6392 | test Neg: 0.3249\n",
      "epoch:    7 | step:   99 | N:  12 | test Sqc: 0.7244 | test Neg: 0.2929\n",
      "epoch:    7 | step:  199 | N:  12 | test Sqc: 0.7270 | test Neg: 0.2912\n",
      "epoch:    7 | step:   99 | N:  14 | test Sqc: 0.7308 | test Neg: 0.2894\n",
      "epoch:    7 | step:  199 | N:  14 | test Sqc: 0.7471 | test Neg: 0.2815\n",
      "epoch:    7 | step:   99 | N:  16 | test Sqc: 0.7940 | test Neg: 0.2619\n",
      "epoch:    7 | step:  199 | N:  16 | test Sqc: 0.8051 | test Neg: 0.2568\n",
      "epoch:    7 | step:   99 | N:  18 | test Sqc: 0.8499 | test Neg: 0.2377\n",
      "epoch:    7 | step:  199 | N:  18 | test Sqc: 0.8500 | test Neg: 0.2353\n",
      "epoch:    7 | step:   99 | N:  20 | test Sqc: 0.9228 | test Neg: 0.1991\n",
      "epoch:    7 | step:  199 | N:  20 | test Sqc: 0.9218 | test Neg: 0.1992\n",
      "epoch:    7 | step:   99 | N:  22 | test Sqc: 0.9946 | test Neg: 0.1602\n",
      "epoch:    7 | step:  199 | N:  22 | test Sqc: 0.9902 | test Neg: 0.1626\n",
      "epoch:    7 | step:   99 | N:  24 | test Sqc: 1.0469 | test Neg: 0.1335\n",
      "epoch:    7 | step:  199 | N:  24 | test Sqc: 1.0446 | test Neg: 0.1339\n",
      "epoch:    7 | step:   99 | N:  26 | test Sqc: 1.0730 | test Neg: 0.1145\n",
      "epoch:    7 | step:  199 | N:  26 | test Sqc: 1.0708 | test Neg: 0.1144\n",
      "epoch:    7 | step:   99 | N:  28 | test Sqc: 1.0962 | test Neg: 0.1019\n",
      "epoch:    7 | step:  199 | N:  28 | test Sqc: 1.0997 | test Neg: 0.0978\n",
      "epoch:    7 | step:   99 | N:  30 | test Sqc: 1.1299 | test Neg: 0.0738\n",
      "epoch:    7 | step:  199 | N:  30 | test Sqc: 1.1242 | test Neg: 0.0795\n",
      "epoch:    7 | step:   99 | N:  32 | test Sqc: 1.1448 | test Neg: 0.0643\n",
      "epoch:    7 | step:  199 | N:  32 | test Sqc: 1.1439 | test Neg: 0.0644\n",
      "epoch:    7 | step:   99 | N:  34 | test Sqc: 1.1965 | test Neg: 0.0303\n",
      "epoch:    7 | step:  199 | N:  34 | test Sqc: 1.1868 | test Neg: 0.0332\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | train Sqc: 0.8573 | loss: 1.3082\n",
      "epoch:    8 | step:  199 | train Sqc: 0.8573 | loss: 1.3081\n",
      "epoch:    8 | step:  299 | train Sqc: 0.8445 | loss: 1.3068\n",
      "epoch:    8 | step:  399 | train Sqc: 0.8464 | loss: 1.3072\n",
      "epoch:    8 | step:  499 | train Sqc: 0.8471 | loss: 1.3073\n",
      "epoch:    8 | step:  599 | train Sqc: 0.8489 | loss: 1.3076\n",
      "epoch:    8 | step:  699 | train Sqc: 0.8510 | loss: 1.3079\n",
      "epoch:    8 | step:  799 | train Sqc: 0.8492 | loss: 1.3078\n",
      "epoch:    8 | step:  899 | train Sqc: 0.8480 | loss: 1.3075\n",
      "epoch:    8 | step:  999 | train Sqc: 0.8463 | loss: 1.3074\n",
      "epoch:    8 | step:  1099 | train Sqc: 0.8468 | loss: 1.3074\n",
      "epoch:    8 | step:  1199 | train Sqc: 0.8475 | loss: 1.3075\n",
      "epoch:    8 | step:  1299 | train Sqc: 0.8498 | loss: 1.3078\n",
      "epoch:    8 | step:  1399 | train Sqc: 0.8497 | loss: 1.3078\n",
      "epoch:    8 | step:  1499 | train Sqc: 0.8505 | loss: 1.3079\n",
      "epoch:    8 | step:  1599 | train Sqc: 0.8502 | loss: 1.3078\n",
      "epoch:    8 | step:  1699 | train Sqc: 0.8508 | loss: 1.3079\n",
      "epoch:    8 | step:  1799 | train Sqc: 0.8509 | loss: 1.3079\n",
      "epoch:    8 | step:  1899 | train Sqc: 0.8505 | loss: 1.3079\n",
      "epoch:    8 | step:  1999 | train Sqc: 0.8502 | loss: 1.3077\n",
      "epoch:    8 | step:  2099 | train Sqc: 0.8510 | loss: 1.3078\n",
      "epoch:    8 | step:  2199 | train Sqc: 0.8507 | loss: 1.3077\n",
      "epoch:    8 | step:  2299 | train Sqc: 0.8504 | loss: 1.3077\n",
      "epoch:    8 | step:  2399 | train Sqc: 0.8506 | loss: 1.3077\n",
      "epoch:    8 | step:  2499 | train Sqc: 0.8504 | loss: 1.3077\n",
      "epoch:    8 | step:  2599 | train Sqc: 0.8506 | loss: 1.3077\n",
      "epoch:    8 | step:  2699 | train Sqc: 0.8507 | loss: 1.3077\n",
      "epoch:    8 | step:  2799 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    8 | step:  2899 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    8 | step:  2999 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    8 | step:  3099 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    8 | step:  3199 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    8 | step:  3299 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    8 | step:  3399 | train Sqc: 0.8493 | loss: 1.3075\n",
      "epoch:    8 | step:  3499 | train Sqc: 0.8495 | loss: 1.3075\n",
      "epoch:    8 | step:  3599 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    8 | step:  3699 | train Sqc: 0.8496 | loss: 1.3075\n",
      "epoch:    8 | step:  3799 | train Sqc: 0.8500 | loss: 1.3075\n",
      "epoch:    8 | step:  3899 | train Sqc: 0.8506 | loss: 1.3076\n",
      "epoch:    8 | step:  3999 | train Sqc: 0.8510 | loss: 1.3077\n",
      "epoch:    8 | step:  4099 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    8 | step:  4199 | train Sqc: 0.8497 | loss: 1.3075\n",
      "epoch:    8 | step:  4299 | train Sqc: 0.8497 | loss: 1.3075\n",
      "epoch:    8 | step:  4399 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    8 | step:  4499 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  4599 | train Sqc: 0.8504 | loss: 1.3076\n",
      "epoch:    8 | step:  4699 | train Sqc: 0.8503 | loss: 1.3076\n",
      "epoch:    8 | step:  4799 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    8 | step:  4899 | train Sqc: 0.8499 | loss: 1.3075\n",
      "epoch:    8 | step:  4999 | train Sqc: 0.8500 | loss: 1.3075\n",
      "epoch:    8 | step:  5099 | train Sqc: 0.8499 | loss: 1.3075\n",
      "epoch:    8 | step:  5199 | train Sqc: 0.8499 | loss: 1.3075\n",
      "epoch:    8 | step:  5299 | train Sqc: 0.8499 | loss: 1.3076\n",
      "epoch:    8 | step:  5399 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  5499 | train Sqc: 0.8502 | loss: 1.3076\n",
      "epoch:    8 | step:  5599 | train Sqc: 0.8508 | loss: 1.3077\n",
      "epoch:    8 | step:  5699 | train Sqc: 0.8506 | loss: 1.3076\n",
      "epoch:    8 | step:  5799 | train Sqc: 0.8505 | loss: 1.3077\n",
      "epoch:    8 | step:  5899 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  5999 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    8 | step:  6099 | train Sqc: 0.8498 | loss: 1.3076\n",
      "epoch:    8 | step:  6199 | train Sqc: 0.8498 | loss: 1.3076\n",
      "epoch:    8 | step:  6299 | train Sqc: 0.8499 | loss: 1.3076\n",
      "epoch:    8 | step:  6399 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  6499 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  6599 | train Sqc: 0.8500 | loss: 1.3076\n",
      "epoch:    8 | step:  6699 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  6799 | train Sqc: 0.8501 | loss: 1.3076\n",
      "epoch:    8 | step:  6899 | train Sqc: 0.8499 | loss: 1.3075\n",
      "epoch:    8 | step:  6999 | train Sqc: 0.8494 | loss: 1.3075\n",
      "epoch:    8 | step:  7099 | train Sqc: 0.8495 | loss: 1.3075\n",
      "epoch:    8 | step:  7199 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    8 | step:  7299 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  7399 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    8 | step:  7499 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    8 | step:  7599 | train Sqc: 0.8495 | loss: 1.3075\n",
      "epoch:    8 | step:  7699 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    8 | step:  7799 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  7899 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    8 | step:  7999 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    8 | step:  8099 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  8199 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  8299 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  8399 | train Sqc: 0.8493 | loss: 1.3074\n",
      "epoch:    8 | step:  8499 | train Sqc: 0.8494 | loss: 1.3074\n",
      "epoch:    8 | step:  8599 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    8 | step:  8699 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    8 | step:  8799 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  8899 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    8 | step:  8999 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    8 | step:  9099 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    8 | step:  9199 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    8 | step:  9299 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    8 | step:  9399 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    8 | step:  9499 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  9599 | train Sqc: 0.8486 | loss: 1.3074\n",
      "epoch:    8 | step:  9699 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    8 | step:  9799 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    8 | step:  9899 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    8 | step:  9999 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  10099 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    8 | step:  10199 | train Sqc: 0.8486 | loss: 1.3074\n",
      "epoch:    8 | step:  10299 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  10399 | train Sqc: 0.8486 | loss: 1.3074\n",
      "epoch:    8 | step:  10499 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    8 | step:  10599 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  10699 | train Sqc: 0.8485 | loss: 1.3074\n",
      "epoch:    8 | step:  10799 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  10899 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    8 | step:  10999 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  11099 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    8 | step:  11199 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  11299 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  11399 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    8 | step:  11499 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    8 | step:  11599 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  11699 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    8 | step:  11799 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  11899 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    8 | step:  11999 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    8 | step:  12099 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    8 | step:  12199 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  12299 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    8 | step:  12399 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    8 | step:  12499 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    8 | step:  12599 | train Sqc: 0.8479 | loss: 1.3072\n",
      "epoch:    8 | step:  12699 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  12799 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  12899 | train Sqc: 0.8480 | loss: 1.3072\n",
      "epoch:    8 | step:  12999 | train Sqc: 0.8479 | loss: 1.3072\n",
      "epoch:    8 | step:  13099 | train Sqc: 0.8479 | loss: 1.3072\n",
      "epoch:    8 | step:  13199 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13299 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    8 | step:  13399 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13499 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13599 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13699 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13799 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    8 | step:  13899 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  13999 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    8 | step:  14099 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  14199 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  14299 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  14399 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  14499 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  14599 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  14699 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  14799 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  14899 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  14999 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  15099 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  15199 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  15299 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  15399 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  15499 | train Sqc: 0.8471 | loss: 1.3072\n",
      "epoch:    8 | step:  15599 | train Sqc: 0.8471 | loss: 1.3072\n",
      "epoch:    8 | step:  15699 | train Sqc: 0.8471 | loss: 1.3071\n",
      "epoch:    8 | step:  15799 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  15899 | train Sqc: 0.8470 | loss: 1.3071\n",
      "epoch:    8 | step:  15999 | train Sqc: 0.8471 | loss: 1.3071\n",
      "epoch:    8 | step:  16099 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  16199 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  16299 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  16399 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  16499 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    8 | step:  16599 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    8 | step:  16699 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  16799 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  16899 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    8 | step:  16999 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    8 | step:  17099 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  17199 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  17299 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  17399 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  17499 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  17599 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  17699 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  17799 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  17899 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  17999 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    8 | step:  18099 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  18199 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  18299 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  18399 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  18499 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  18599 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  18699 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  18799 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:    8 | step:  18899 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  18999 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  19099 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  19199 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    8 | step:  19299 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  19399 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    8 | step:  19499 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  19599 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  19699 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    8 | step:  19799 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    8 | step:  19899 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  19999 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    8 | step:  20099 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    8 | step:  20199 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  20299 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  20399 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  20499 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  20599 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  20699 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  20799 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  20899 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  20999 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  21099 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  21199 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  21299 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  21399 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  21499 | train Sqc: 0.8476 | loss: 1.3073\n",
      "epoch:    8 | step:  21599 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  21699 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:    8 | step:  21799 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  21899 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  21999 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    8 | step:  22099 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  22199 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  22299 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  22399 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  22499 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  22599 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  22699 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    8 | step:  22799 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    8 | step:  22899 | train Sqc: 0.8484 | loss: 1.3074\n",
      "epoch:    8 | step:  22999 | train Sqc: 0.8486 | loss: 1.3074\n",
      "epoch:    8 | step:  23099 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23199 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23299 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23399 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23499 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23599 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23699 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23799 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    8 | step:  23899 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    8 | step:  23999 | train Sqc: 0.8490 | loss: 1.3075\n",
      "epoch:    8 | step:  24099 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  24199 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  24299 | train Sqc: 0.8494 | loss: 1.3075\n",
      "epoch:    8 | step:  24399 | train Sqc: 0.8493 | loss: 1.3075\n",
      "epoch:    8 | step:  24499 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  24599 | train Sqc: 0.8493 | loss: 1.3075\n",
      "epoch:    8 | step:  24699 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  24799 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  24899 | train Sqc: 0.8491 | loss: 1.3075\n",
      "epoch:    8 | step:  24999 | train Sqc: 0.8491 | loss: 1.3075\n",
      "epoch:    8 | step:  25099 | train Sqc: 0.8491 | loss: 1.3075\n",
      "epoch:    8 | step:  25199 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  25299 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  25399 | train Sqc: 0.8492 | loss: 1.3075\n",
      "epoch:    8 | step:  25499 | train Sqc: 0.8490 | loss: 1.3075\n",
      "epoch:    8 | step:  25599 | train Sqc: 0.8491 | loss: 1.3075\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | test Sqc: 0.3563 | test Neg: 0.4204\n",
      "epoch:    8 | step:  199 | N:  4 | test Sqc: 0.3697 | test Neg: 0.4147\n",
      "epoch:    8 | step:   99 | N:  6 | test Sqc: 0.4062 | test Neg: 0.4075\n",
      "epoch:    8 | step:  199 | N:  6 | test Sqc: 0.4167 | test Neg: 0.4028\n",
      "epoch:    8 | step:   99 | N:  8 | test Sqc: 0.5388 | test Neg: 0.3679\n",
      "epoch:    8 | step:  199 | N:  8 | test Sqc: 0.5358 | test Neg: 0.3676\n",
      "epoch:    8 | step:   99 | N:  10 | test Sqc: 0.6315 | test Neg: 0.3266\n",
      "epoch:    8 | step:  199 | N:  10 | test Sqc: 0.6354 | test Neg: 0.3252\n",
      "epoch:    8 | step:   99 | N:  12 | test Sqc: 0.7228 | test Neg: 0.2934\n",
      "epoch:    8 | step:  199 | N:  12 | test Sqc: 0.7244 | test Neg: 0.2919\n",
      "epoch:    8 | step:   99 | N:  14 | test Sqc: 0.7307 | test Neg: 0.2895\n",
      "epoch:    8 | step:  199 | N:  14 | test Sqc: 0.7485 | test Neg: 0.2812\n",
      "epoch:    8 | step:   99 | N:  16 | test Sqc: 0.7922 | test Neg: 0.2625\n",
      "epoch:    8 | step:  199 | N:  16 | test Sqc: 0.8029 | test Neg: 0.2571\n",
      "epoch:    8 | step:   99 | N:  18 | test Sqc: 0.8499 | test Neg: 0.2378\n",
      "epoch:    8 | step:  199 | N:  18 | test Sqc: 0.8488 | test Neg: 0.2357\n",
      "epoch:    8 | step:   99 | N:  20 | test Sqc: 0.9233 | test Neg: 0.1988\n",
      "epoch:    8 | step:  199 | N:  20 | test Sqc: 0.9214 | test Neg: 0.1992\n",
      "epoch:    8 | step:   99 | N:  22 | test Sqc: 0.9928 | test Neg: 0.1612\n",
      "epoch:    8 | step:  199 | N:  22 | test Sqc: 0.9887 | test Neg: 0.1632\n",
      "epoch:    8 | step:   99 | N:  24 | test Sqc: 1.0450 | test Neg: 0.1350\n",
      "epoch:    8 | step:  199 | N:  24 | test Sqc: 1.0422 | test Neg: 0.1351\n",
      "epoch:    8 | step:   99 | N:  26 | test Sqc: 1.0744 | test Neg: 0.1142\n",
      "epoch:    8 | step:  199 | N:  26 | test Sqc: 1.0737 | test Neg: 0.1139\n",
      "epoch:    8 | step:   99 | N:  28 | test Sqc: 1.0877 | test Neg: 0.1073\n",
      "epoch:    8 | step:  199 | N:  28 | test Sqc: 1.0920 | test Neg: 0.1025\n",
      "epoch:    8 | step:   99 | N:  30 | test Sqc: 1.1226 | test Neg: 0.0790\n",
      "epoch:    8 | step:  199 | N:  30 | test Sqc: 1.1175 | test Neg: 0.0844\n",
      "epoch:    8 | step:   99 | N:  32 | test Sqc: 1.1539 | test Neg: 0.0583\n",
      "epoch:    8 | step:  199 | N:  32 | test Sqc: 1.1505 | test Neg: 0.0598\n",
      "epoch:    8 | step:   99 | N:  34 | test Sqc: 1.1913 | test Neg: 0.0356\n",
      "epoch:    8 | step:  199 | N:  34 | test Sqc: 1.1829 | test Neg: 0.0365\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | train Sqc: 0.8568 | loss: 1.3078\n",
      "epoch:    9 | step:  199 | train Sqc: 0.8568 | loss: 1.3079\n",
      "epoch:    9 | step:  299 | train Sqc: 0.8437 | loss: 1.3065\n",
      "epoch:    9 | step:  399 | train Sqc: 0.8447 | loss: 1.3068\n",
      "epoch:    9 | step:  499 | train Sqc: 0.8455 | loss: 1.3070\n",
      "epoch:    9 | step:  599 | train Sqc: 0.8472 | loss: 1.3074\n",
      "epoch:    9 | step:  699 | train Sqc: 0.8501 | loss: 1.3077\n",
      "epoch:    9 | step:  799 | train Sqc: 0.8485 | loss: 1.3076\n",
      "epoch:    9 | step:  899 | train Sqc: 0.8468 | loss: 1.3073\n",
      "epoch:    9 | step:  999 | train Sqc: 0.8445 | loss: 1.3070\n",
      "epoch:    9 | step:  1099 | train Sqc: 0.8450 | loss: 1.3071\n",
      "epoch:    9 | step:  1199 | train Sqc: 0.8456 | loss: 1.3072\n",
      "epoch:    9 | step:  1299 | train Sqc: 0.8477 | loss: 1.3075\n",
      "epoch:    9 | step:  1399 | train Sqc: 0.8475 | loss: 1.3075\n",
      "epoch:    9 | step:  1499 | train Sqc: 0.8484 | loss: 1.3075\n",
      "epoch:    9 | step:  1599 | train Sqc: 0.8483 | loss: 1.3075\n",
      "epoch:    9 | step:  1699 | train Sqc: 0.8489 | loss: 1.3075\n",
      "epoch:    9 | step:  1799 | train Sqc: 0.8489 | loss: 1.3075\n",
      "epoch:    9 | step:  1899 | train Sqc: 0.8489 | loss: 1.3075\n",
      "epoch:    9 | step:  1999 | train Sqc: 0.8484 | loss: 1.3074\n",
      "epoch:    9 | step:  2099 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    9 | step:  2199 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    9 | step:  2299 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  2399 | train Sqc: 0.8485 | loss: 1.3073\n",
      "epoch:    9 | step:  2499 | train Sqc: 0.8485 | loss: 1.3074\n",
      "epoch:    9 | step:  2599 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    9 | step:  2699 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    9 | step:  2799 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    9 | step:  2899 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    9 | step:  2999 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    9 | step:  3099 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    9 | step:  3199 | train Sqc: 0.8487 | loss: 1.3073\n",
      "epoch:    9 | step:  3299 | train Sqc: 0.8484 | loss: 1.3073\n",
      "epoch:    9 | step:  3399 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    9 | step:  3499 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    9 | step:  3599 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    9 | step:  3699 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  3799 | train Sqc: 0.8487 | loss: 1.3073\n",
      "epoch:    9 | step:  3899 | train Sqc: 0.8493 | loss: 1.3075\n",
      "epoch:    9 | step:  3999 | train Sqc: 0.8498 | loss: 1.3075\n",
      "epoch:    9 | step:  4099 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    9 | step:  4199 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    9 | step:  4299 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    9 | step:  4399 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    9 | step:  4499 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    9 | step:  4599 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    9 | step:  4699 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    9 | step:  4799 | train Sqc: 0.8487 | loss: 1.3074\n",
      "epoch:    9 | step:  4899 | train Sqc: 0.8487 | loss: 1.3073\n",
      "epoch:    9 | step:  4999 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    9 | step:  5099 | train Sqc: 0.8488 | loss: 1.3074\n",
      "epoch:    9 | step:  5199 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    9 | step:  5299 | train Sqc: 0.8489 | loss: 1.3074\n",
      "epoch:    9 | step:  5399 | train Sqc: 0.8493 | loss: 1.3074\n",
      "epoch:    9 | step:  5499 | train Sqc: 0.8495 | loss: 1.3075\n",
      "epoch:    9 | step:  5599 | train Sqc: 0.8501 | loss: 1.3075\n",
      "epoch:    9 | step:  5699 | train Sqc: 0.8500 | loss: 1.3075\n",
      "epoch:    9 | step:  5799 | train Sqc: 0.8499 | loss: 1.3075\n",
      "epoch:    9 | step:  5899 | train Sqc: 0.8495 | loss: 1.3075\n",
      "epoch:    9 | step:  5999 | train Sqc: 0.8494 | loss: 1.3075\n",
      "epoch:    9 | step:  6099 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    9 | step:  6199 | train Sqc: 0.8491 | loss: 1.3074\n",
      "epoch:    9 | step:  6299 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    9 | step:  6399 | train Sqc: 0.8494 | loss: 1.3074\n",
      "epoch:    9 | step:  6499 | train Sqc: 0.8493 | loss: 1.3075\n",
      "epoch:    9 | step:  6599 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    9 | step:  6699 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    9 | step:  6799 | train Sqc: 0.8492 | loss: 1.3074\n",
      "epoch:    9 | step:  6899 | train Sqc: 0.8490 | loss: 1.3074\n",
      "epoch:    9 | step:  6999 | train Sqc: 0.8486 | loss: 1.3073\n",
      "epoch:    9 | step:  7099 | train Sqc: 0.8487 | loss: 1.3073\n",
      "epoch:    9 | step:  7199 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  7299 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  7399 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:    9 | step:  7499 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  7599 | train Sqc: 0.8485 | loss: 1.3073\n",
      "epoch:    9 | step:  7699 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  7799 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    9 | step:  7899 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  7999 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8099 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  8199 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8299 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8399 | train Sqc: 0.8485 | loss: 1.3073\n",
      "epoch:    9 | step:  8499 | train Sqc: 0.8485 | loss: 1.3073\n",
      "epoch:    9 | step:  8599 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8699 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  8799 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8899 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:    9 | step:  8999 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  9099 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  9199 | train Sqc: 0.8479 | loss: 1.3072\n",
      "epoch:    9 | step:  9299 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:    9 | step:  9399 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    9 | step:  9499 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    9 | step:  9599 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:    9 | step:  9699 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  9799 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  9899 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  9999 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    9 | step:  10099 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  10199 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  10299 | train Sqc: 0.8478 | loss: 1.3072\n",
      "epoch:    9 | step:  10399 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    9 | step:  10499 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:    9 | step:  10599 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:    9 | step:  10699 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:    9 | step:  10799 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    9 | step:  10899 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  10999 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  11099 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  11199 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  11299 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  11399 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  11499 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    9 | step:  11599 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  11699 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  11799 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  11899 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:    9 | step:  11999 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  12099 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  12199 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  12299 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:    9 | step:  12399 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:    9 | step:  12499 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:    9 | step:  12599 | train Sqc: 0.8470 | loss: 1.3071\n",
      "epoch:    9 | step:  12699 | train Sqc: 0.8470 | loss: 1.3071\n",
      "epoch:    9 | step:  12799 | train Sqc: 0.8470 | loss: 1.3071\n",
      "epoch:    9 | step:  12899 | train Sqc: 0.8469 | loss: 1.3071\n",
      "epoch:    9 | step:  12999 | train Sqc: 0.8469 | loss: 1.3071\n",
      "epoch:    9 | step:  13099 | train Sqc: 0.8470 | loss: 1.3071\n",
      "epoch:    9 | step:  13199 | train Sqc: 0.8468 | loss: 1.3071\n",
      "epoch:    9 | step:  13299 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  13399 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  13499 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  13599 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  13699 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  13799 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  13899 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  13999 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  14099 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  14199 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  14299 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  14399 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  14499 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  14599 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  14699 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  14799 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  14899 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  14999 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  15099 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  15199 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  15299 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  15399 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  15499 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  15599 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  15699 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  15799 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  15899 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:    9 | step:  15999 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:    9 | step:  16099 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  16199 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  16299 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  16399 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  16499 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:    9 | step:  16599 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:    9 | step:  16699 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  16799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  16899 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  16999 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  17099 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  17199 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  17299 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:    9 | step:  17399 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  17499 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  17599 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  17699 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  17799 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  17899 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:    9 | step:  17999 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  18099 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:    9 | step:  18199 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  18299 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  18399 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  18499 | train Sqc: 0.8463 | loss: 1.3071\n",
      "epoch:    9 | step:  18599 | train Sqc: 0.8463 | loss: 1.3071\n",
      "epoch:    9 | step:  18699 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  18799 | train Sqc: 0.8463 | loss: 1.3071\n",
      "epoch:    9 | step:  18899 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  18999 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  19099 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19199 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19299 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19399 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  19499 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19599 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19699 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  19799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  19899 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  19999 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  20099 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  20199 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  20299 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  20399 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  20499 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  20599 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  20699 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  20799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  20899 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  20999 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  21099 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  21199 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  21299 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  21399 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  21499 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  21599 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  21699 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  21799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  21899 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  21999 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  22099 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  22199 | train Sqc: 0.8468 | loss: 1.3071\n",
      "epoch:    9 | step:  22299 | train Sqc: 0.8468 | loss: 1.3071\n",
      "epoch:    9 | step:  22399 | train Sqc: 0.8468 | loss: 1.3071\n",
      "epoch:    9 | step:  22499 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  22599 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  22699 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  22799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  22899 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  22999 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  23099 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  23199 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  23299 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  23399 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  23499 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  23599 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  23699 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  23799 | train Sqc: 0.8463 | loss: 1.3071\n",
      "epoch:    9 | step:  23899 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  23999 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  24099 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  24199 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24299 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:    9 | step:  24399 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24499 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24599 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24699 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24799 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  24899 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  24999 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  25099 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  25199 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  25299 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:    9 | step:  25399 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:    9 | step:  25499 | train Sqc: 0.8464 | loss: 1.3071\n",
      "epoch:    9 | step:  25599 | train Sqc: 0.8464 | loss: 1.3071\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | test Sqc: 0.3546 | test Neg: 0.4203\n",
      "epoch:    9 | step:  199 | N:  4 | test Sqc: 0.3684 | test Neg: 0.4147\n",
      "epoch:    9 | step:   99 | N:  6 | test Sqc: 0.3971 | test Neg: 0.4077\n",
      "epoch:    9 | step:  199 | N:  6 | test Sqc: 0.4098 | test Neg: 0.4030\n",
      "epoch:    9 | step:   99 | N:  8 | test Sqc: 0.5272 | test Neg: 0.3643\n",
      "epoch:    9 | step:  199 | N:  8 | test Sqc: 0.5271 | test Neg: 0.3630\n",
      "epoch:    9 | step:   99 | N:  10 | test Sqc: 0.6313 | test Neg: 0.3286\n",
      "epoch:    9 | step:  199 | N:  10 | test Sqc: 0.6341 | test Neg: 0.3273\n",
      "epoch:    9 | step:   99 | N:  12 | test Sqc: 0.7291 | test Neg: 0.2913\n",
      "epoch:    9 | step:  199 | N:  12 | test Sqc: 0.7301 | test Neg: 0.2899\n",
      "epoch:    9 | step:   99 | N:  14 | test Sqc: 0.7321 | test Neg: 0.2881\n",
      "epoch:    9 | step:  199 | N:  14 | test Sqc: 0.7508 | test Neg: 0.2791\n",
      "epoch:    9 | step:   99 | N:  16 | test Sqc: 0.7902 | test Neg: 0.2632\n",
      "epoch:    9 | step:  199 | N:  16 | test Sqc: 0.8021 | test Neg: 0.2577\n",
      "epoch:    9 | step:   99 | N:  18 | test Sqc: 0.8499 | test Neg: 0.2364\n",
      "epoch:    9 | step:  199 | N:  18 | test Sqc: 0.8502 | test Neg: 0.2341\n",
      "epoch:    9 | step:   99 | N:  20 | test Sqc: 0.9260 | test Neg: 0.1971\n",
      "epoch:    9 | step:  199 | N:  20 | test Sqc: 0.9237 | test Neg: 0.1975\n",
      "epoch:    9 | step:   99 | N:  22 | test Sqc: 0.9924 | test Neg: 0.1602\n",
      "epoch:    9 | step:  199 | N:  22 | test Sqc: 0.9882 | test Neg: 0.1621\n",
      "epoch:    9 | step:   99 | N:  24 | test Sqc: 1.0463 | test Neg: 0.1330\n",
      "epoch:    9 | step:  199 | N:  24 | test Sqc: 1.0450 | test Neg: 0.1327\n",
      "epoch:    9 | step:   99 | N:  26 | test Sqc: 1.0773 | test Neg: 0.1121\n",
      "epoch:    9 | step:  199 | N:  26 | test Sqc: 1.0749 | test Neg: 0.1120\n",
      "epoch:    9 | step:   99 | N:  28 | test Sqc: 1.0912 | test Neg: 0.1043\n",
      "epoch:    9 | step:  199 | N:  28 | test Sqc: 1.0949 | test Neg: 0.1002\n",
      "epoch:    9 | step:   99 | N:  30 | test Sqc: 1.1256 | test Neg: 0.0788\n",
      "epoch:    9 | step:  199 | N:  30 | test Sqc: 1.1181 | test Neg: 0.0842\n",
      "epoch:    9 | step:   99 | N:  32 | test Sqc: 1.1387 | test Neg: 0.0676\n",
      "epoch:    9 | step:  199 | N:  32 | test Sqc: 1.1382 | test Neg: 0.0682\n",
      "epoch:    9 | step:   99 | N:  34 | test Sqc: 1.1829 | test Neg: 0.0369\n",
      "epoch:    9 | step:  199 | N:  34 | test Sqc: 1.1770 | test Neg: 0.0367\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   10 | step:   99 | train Sqc: 0.8507 | loss: 1.3077\n",
      "epoch:   10 | step:  199 | train Sqc: 0.8533 | loss: 1.3077\n",
      "epoch:   10 | step:  299 | train Sqc: 0.8403 | loss: 1.3063\n",
      "epoch:   10 | step:  399 | train Sqc: 0.8419 | loss: 1.3066\n",
      "epoch:   10 | step:  499 | train Sqc: 0.8425 | loss: 1.3067\n",
      "epoch:   10 | step:  599 | train Sqc: 0.8446 | loss: 1.3071\n",
      "epoch:   10 | step:  699 | train Sqc: 0.8471 | loss: 1.3074\n",
      "epoch:   10 | step:  799 | train Sqc: 0.8457 | loss: 1.3073\n",
      "epoch:   10 | step:  899 | train Sqc: 0.8446 | loss: 1.3071\n",
      "epoch:   10 | step:  999 | train Sqc: 0.8433 | loss: 1.3070\n",
      "epoch:   10 | step:  1099 | train Sqc: 0.8439 | loss: 1.3070\n",
      "epoch:   10 | step:  1199 | train Sqc: 0.8449 | loss: 1.3072\n",
      "epoch:   10 | step:  1299 | train Sqc: 0.8473 | loss: 1.3075\n",
      "epoch:   10 | step:  1399 | train Sqc: 0.8470 | loss: 1.3075\n",
      "epoch:   10 | step:  1499 | train Sqc: 0.8479 | loss: 1.3075\n",
      "epoch:   10 | step:  1599 | train Sqc: 0.8475 | loss: 1.3074\n",
      "epoch:   10 | step:  1699 | train Sqc: 0.8483 | loss: 1.3076\n",
      "epoch:   10 | step:  1799 | train Sqc: 0.8485 | loss: 1.3076\n",
      "epoch:   10 | step:  1899 | train Sqc: 0.8484 | loss: 1.3076\n",
      "epoch:   10 | step:  1999 | train Sqc: 0.8480 | loss: 1.3074\n",
      "epoch:   10 | step:  2099 | train Sqc: 0.8486 | loss: 1.3075\n",
      "epoch:   10 | step:  2199 | train Sqc: 0.8484 | loss: 1.3074\n",
      "epoch:   10 | step:  2299 | train Sqc: 0.8482 | loss: 1.3074\n",
      "epoch:   10 | step:  2399 | train Sqc: 0.8486 | loss: 1.3074\n",
      "epoch:   10 | step:  2499 | train Sqc: 0.8482 | loss: 1.3074\n",
      "epoch:   10 | step:  2599 | train Sqc: 0.8487 | loss: 1.3075\n",
      "epoch:   10 | step:  2699 | train Sqc: 0.8489 | loss: 1.3075\n",
      "epoch:   10 | step:  2799 | train Sqc: 0.8484 | loss: 1.3074\n",
      "epoch:   10 | step:  2899 | train Sqc: 0.8484 | loss: 1.3074\n",
      "epoch:   10 | step:  2999 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:   10 | step:  3099 | train Sqc: 0.8482 | loss: 1.3073\n",
      "epoch:   10 | step:  3199 | train Sqc: 0.8483 | loss: 1.3074\n",
      "epoch:   10 | step:  3299 | train Sqc: 0.8479 | loss: 1.3073\n",
      "epoch:   10 | step:  3399 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  3499 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:   10 | step:  3599 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:   10 | step:  3699 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  3799 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:   10 | step:  3899 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:   10 | step:  3999 | train Sqc: 0.8485 | loss: 1.3073\n",
      "epoch:   10 | step:  4099 | train Sqc: 0.8478 | loss: 1.3073\n",
      "epoch:   10 | step:  4199 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  4299 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  4399 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:   10 | step:  4499 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:   10 | step:  4599 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:   10 | step:  4699 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:   10 | step:  4799 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  4899 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  4999 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  5099 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  5199 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  5299 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:   10 | step:  5399 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:   10 | step:  5499 | train Sqc: 0.8477 | loss: 1.3072\n",
      "epoch:   10 | step:  5599 | train Sqc: 0.8483 | loss: 1.3073\n",
      "epoch:   10 | step:  5699 | train Sqc: 0.8481 | loss: 1.3073\n",
      "epoch:   10 | step:  5799 | train Sqc: 0.8480 | loss: 1.3073\n",
      "epoch:   10 | step:  5899 | train Sqc: 0.8476 | loss: 1.3073\n",
      "epoch:   10 | step:  5999 | train Sqc: 0.8476 | loss: 1.3073\n",
      "epoch:   10 | step:  6099 | train Sqc: 0.8473 | loss: 1.3072\n",
      "epoch:   10 | step:  6199 | train Sqc: 0.8472 | loss: 1.3072\n",
      "epoch:   10 | step:  6299 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:   10 | step:  6399 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:   10 | step:  6499 | train Sqc: 0.8477 | loss: 1.3073\n",
      "epoch:   10 | step:  6599 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:   10 | step:  6699 | train Sqc: 0.8475 | loss: 1.3072\n",
      "epoch:   10 | step:  6799 | train Sqc: 0.8476 | loss: 1.3072\n",
      "epoch:   10 | step:  6899 | train Sqc: 0.8474 | loss: 1.3072\n",
      "epoch:   10 | step:  6999 | train Sqc: 0.8469 | loss: 1.3071\n",
      "epoch:   10 | step:  7099 | train Sqc: 0.8471 | loss: 1.3071\n",
      "epoch:   10 | step:  7199 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:   10 | step:  7299 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:   10 | step:  7399 | train Sqc: 0.8466 | loss: 1.3071\n",
      "epoch:   10 | step:  7499 | train Sqc: 0.8465 | loss: 1.3070\n",
      "epoch:   10 | step:  7599 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:   10 | step:  7699 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:   10 | step:  7799 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:   10 | step:  7899 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:   10 | step:  7999 | train Sqc: 0.8465 | loss: 1.3071\n",
      "epoch:   10 | step:  8099 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:   10 | step:  8199 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:   10 | step:  8299 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:   10 | step:  8399 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:   10 | step:  8499 | train Sqc: 0.8467 | loss: 1.3071\n",
      "epoch:   10 | step:  8599 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:   10 | step:  8699 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:   10 | step:  8799 | train Sqc: 0.8464 | loss: 1.3070\n",
      "epoch:   10 | step:  8899 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:   10 | step:  8999 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:   10 | step:  9099 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:   10 | step:  9199 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  9299 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:   10 | step:  9399 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:   10 | step:  9499 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  9599 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  9699 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  9799 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  9899 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  9999 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:   10 | step:  10099 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  10199 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  10299 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:   10 | step:  10399 | train Sqc: 0.8461 | loss: 1.3070\n",
      "epoch:   10 | step:  10499 | train Sqc: 0.8463 | loss: 1.3070\n",
      "epoch:   10 | step:  10599 | train Sqc: 0.8462 | loss: 1.3070\n",
      "epoch:   10 | step:  10699 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  10799 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  10899 | train Sqc: 0.8457 | loss: 1.3070\n",
      "epoch:   10 | step:  10999 | train Sqc: 0.8458 | loss: 1.3070\n",
      "epoch:   10 | step:  11099 | train Sqc: 0.8457 | loss: 1.3070\n",
      "epoch:   10 | step:  11199 | train Sqc: 0.8458 | loss: 1.3070\n",
      "epoch:   10 | step:  11299 | train Sqc: 0.8458 | loss: 1.3070\n",
      "epoch:   10 | step:  11399 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  11499 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  11599 | train Sqc: 0.8458 | loss: 1.3070\n",
      "epoch:   10 | step:  11699 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  11799 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  11899 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  11999 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  12099 | train Sqc: 0.8458 | loss: 1.3070\n",
      "epoch:   10 | step:  12199 | train Sqc: 0.8459 | loss: 1.3070\n",
      "epoch:   10 | step:  12299 | train Sqc: 0.8460 | loss: 1.3070\n",
      "epoch:   10 | step:  12399 | train Sqc: 0.8459 | loss: 1.3069\n",
      "epoch:   10 | step:  12499 | train Sqc: 0.8457 | loss: 1.3069\n",
      "epoch:   10 | step:  12599 | train Sqc: 0.8455 | loss: 1.3069\n",
      "epoch:   10 | step:  12699 | train Sqc: 0.8456 | loss: 1.3069\n",
      "epoch:   10 | step:  12799 | train Sqc: 0.8456 | loss: 1.3069\n",
      "epoch:   10 | step:  12899 | train Sqc: 0.8455 | loss: 1.3069\n",
      "epoch:   10 | step:  12999 | train Sqc: 0.8455 | loss: 1.3069\n",
      "epoch:   10 | step:  13099 | train Sqc: 0.8455 | loss: 1.3069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(torch\u001b[38;5;241m.\u001b[39mbmm(shadow_state_train[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), rhoC), shadow_state_train[i]\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mprobs\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m l[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    # Train\n",
    "    print('='*50+'   Train   '+'='*50)\n",
    "    mdl.train()\n",
    "    for i in range(prepseq_train.shape[0]):\n",
    "        rhoC = mdl(prepseq_train[i])\n",
    "        l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "        optimizer.zero_grad()\n",
    "        probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "        loss = -probs.log().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l['loss'].append(loss.item())\n",
    "        if (i+1)%100 == 0:\n",
    "            trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "            loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "            print('epoch:  %3d | step:  %3d | train Sqc: %.4f | loss: %.4f' %(epoch, i, trainS, loss))\n",
    "    # Test\n",
    "    if test:\n",
    "        with torch.no_grad():\n",
    "            print('='*50+'   Test   '+'='*50)\n",
    "            mdl.eval()\n",
    "            for n in range(prepseq_test.shape[0]):\n",
    "                for i in range(prepseq_test.shape[1]):\n",
    "                    N = n*2+4\n",
    "                    rhoC = mdl(prepseq_test[n,i])\n",
    "                    l['test Sqc'].append([N,bSqc(rhoS_test[n,i], rhoC).mean().item()])\n",
    "                    l['test Neg'].append([N,Neg(rhoS_test[n,i], rhoC).mean().item()])\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean(0)[-1].item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean(0)[-1].item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "    torch.save(l, f'{file}/record/gpt_na.pt')\n",
    "    torch.save(mdl.state_dict(), f'{file}/models/gpt_na.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1421c9-26b0-4331-9280-6489d25e1de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
