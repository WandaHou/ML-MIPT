{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg, Sa\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "test = True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 5/6\n",
    "batch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae16959-337c-480b-84ec-cf28828f186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepseq_train = torch.load(f'../data/post_selected/prepseq_train.pt')\n",
    "shadow_state_train = torch.load(f'../data/post_selected/shadow_state_train.pt')\n",
    "rhoS_train = torch.load(f'../data/post_selected/rhoS_train.pt')\n",
    "\n",
    "prepseq_test = torch.load(f'../data/post_selected/prepseq_test.pt')\n",
    "shadow_state_test = torch.load(f'../data/post_selected/shadow_state_test.pt')\n",
    "rhoS_test = torch.load(f'../data/post_selected/rhoS_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3999e5c4-d433-4385-8c15-945a8acaf5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7000, 500, 15]), torch.Size([7, 200, 500, 15]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split in batches\n",
    "prepseq_train = prepseq_train.view(-1, batch, 15)\n",
    "shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "prepseq_test = prepseq_test.view(7, -1, batch, 15)\n",
    "shadow_state_test = shadow_state_test.view(7, -1, batch, 4)\n",
    "rhoS_test = rhoS_test.view(7, -1, batch, 4, 4)\n",
    "    \n",
    "prepseq_train.shape, prepseq_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64d1744-dd15-481b-a263-c9bfe3155a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LlamaPredictor(L_max=17,\n",
    "                     n_embd=24, \n",
    "                     n_layer=12, \n",
    "                     n_head=6, \n",
    "                     vocab_size=4, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'test Sa':[], 'loss':[]}\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb7ad23-b1b6-494a-8ef4-b66d63367a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = LlamaPredictor(L_max=17,\n",
    "#                      n_embd=12, \n",
    "#                      n_layer=6, \n",
    "#                      n_head=6, \n",
    "#                      vocab_size=4, \n",
    "#                      dropout_prob=0.0).to(device)\n",
    "# optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "# l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'test Sa':[], 'loss':[]}\n",
    "# total=0 # find size of the model\n",
    "# for p in mdl.parameters():\n",
    "#     total+=prod(p.shape)\n",
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | train Sqc: 1.4100 | loss: 1.3901\n",
      "epoch:    0 | step:  199 | train Sqc: 1.3880 | loss: 1.3867\n",
      "epoch:    0 | step:  299 | train Sqc: 1.3704 | loss: 1.3838\n",
      "epoch:    0 | step:  399 | train Sqc: 1.3475 | loss: 1.3803\n",
      "epoch:    0 | step:  499 | train Sqc: 1.3246 | loss: 1.3766\n",
      "epoch:    0 | step:  599 | train Sqc: 1.3085 | loss: 1.3737\n",
      "epoch:    0 | step:  699 | train Sqc: 1.2942 | loss: 1.3714\n",
      "epoch:    0 | step:  799 | train Sqc: 1.2841 | loss: 1.3698\n",
      "epoch:    0 | step:  899 | train Sqc: 1.2737 | loss: 1.3683\n",
      "epoch:    0 | step:  999 | train Sqc: 1.2639 | loss: 1.3670\n",
      "epoch:    0 | step:  1099 | train Sqc: 1.2570 | loss: 1.3658\n",
      "epoch:    0 | step:  1199 | train Sqc: 1.2504 | loss: 1.3648\n",
      "epoch:    0 | step:  1299 | train Sqc: 1.2430 | loss: 1.3638\n",
      "epoch:    0 | step:  1399 | train Sqc: 1.2382 | loss: 1.3629\n",
      "epoch:    0 | step:  1499 | train Sqc: 1.2328 | loss: 1.3619\n",
      "epoch:    0 | step:  1599 | train Sqc: 1.2267 | loss: 1.3609\n",
      "epoch:    0 | step:  1699 | train Sqc: 1.2231 | loss: 1.3604\n",
      "epoch:    0 | step:  1799 | train Sqc: 1.2209 | loss: 1.3599\n",
      "epoch:    0 | step:  1899 | train Sqc: 1.2168 | loss: 1.3593\n",
      "epoch:    0 | step:  1999 | train Sqc: 1.2138 | loss: 1.3587\n",
      "epoch:    0 | step:  2099 | train Sqc: 1.2093 | loss: 1.3580\n",
      "epoch:    0 | step:  2199 | train Sqc: 1.2050 | loss: 1.3574\n",
      "epoch:    0 | step:  2299 | train Sqc: 1.2019 | loss: 1.3569\n",
      "epoch:    0 | step:  2399 | train Sqc: 1.1983 | loss: 1.3563\n",
      "epoch:    0 | step:  2499 | train Sqc: 1.1940 | loss: 1.3557\n",
      "epoch:    0 | step:  2599 | train Sqc: 1.1886 | loss: 1.3549\n",
      "epoch:    0 | step:  2699 | train Sqc: 1.1838 | loss: 1.3543\n",
      "epoch:    0 | step:  2799 | train Sqc: 1.1805 | loss: 1.3537\n",
      "epoch:    0 | step:  2899 | train Sqc: 1.1761 | loss: 1.3531\n",
      "epoch:    0 | step:  2999 | train Sqc: 1.1720 | loss: 1.3524\n",
      "epoch:    0 | step:  3099 | train Sqc: 1.1668 | loss: 1.3517\n",
      "epoch:    0 | step:  3199 | train Sqc: 1.1614 | loss: 1.3509\n",
      "epoch:    0 | step:  3299 | train Sqc: 1.1568 | loss: 1.3502\n",
      "epoch:    0 | step:  3399 | train Sqc: 1.1531 | loss: 1.3497\n",
      "epoch:    0 | step:  3499 | train Sqc: 1.1493 | loss: 1.3491\n",
      "epoch:    0 | step:  3599 | train Sqc: 1.1452 | loss: 1.3484\n",
      "epoch:    0 | step:  3699 | train Sqc: 1.1405 | loss: 1.3477\n",
      "epoch:    0 | step:  3799 | train Sqc: 1.1356 | loss: 1.3470\n",
      "epoch:    0 | step:  3899 | train Sqc: 1.1315 | loss: 1.3463\n",
      "epoch:    0 | step:  3999 | train Sqc: 1.1273 | loss: 1.3457\n",
      "epoch:    0 | step:  4099 | train Sqc: 1.1242 | loss: 1.3453\n",
      "epoch:    0 | step:  4199 | train Sqc: 1.1204 | loss: 1.3447\n",
      "epoch:    0 | step:  4299 | train Sqc: 1.1161 | loss: 1.3441\n",
      "epoch:    0 | step:  4399 | train Sqc: 1.1125 | loss: 1.3435\n",
      "epoch:    0 | step:  4499 | train Sqc: 1.1094 | loss: 1.3430\n",
      "epoch:    0 | step:  4599 | train Sqc: 1.1058 | loss: 1.3425\n",
      "epoch:    0 | step:  4699 | train Sqc: 1.1017 | loss: 1.3419\n",
      "epoch:    0 | step:  4799 | train Sqc: 1.0981 | loss: 1.3413\n",
      "epoch:    0 | step:  4899 | train Sqc: 1.0940 | loss: 1.3407\n",
      "epoch:    0 | step:  4999 | train Sqc: 1.0902 | loss: 1.3401\n",
      "epoch:    0 | step:  5099 | train Sqc: 1.0870 | loss: 1.3396\n",
      "epoch:    0 | step:  5199 | train Sqc: 1.0832 | loss: 1.3391\n",
      "epoch:    0 | step:  5299 | train Sqc: 1.0794 | loss: 1.3385\n",
      "epoch:    0 | step:  5399 | train Sqc: 1.0758 | loss: 1.3380\n",
      "epoch:    0 | step:  5499 | train Sqc: 1.0726 | loss: 1.3375\n",
      "epoch:    0 | step:  5599 | train Sqc: 1.0688 | loss: 1.3369\n",
      "epoch:    0 | step:  5699 | train Sqc: 1.0658 | loss: 1.3364\n",
      "epoch:    0 | step:  5799 | train Sqc: 1.0629 | loss: 1.3360\n",
      "epoch:    0 | step:  5899 | train Sqc: 1.0597 | loss: 1.3355\n",
      "epoch:    0 | step:  5999 | train Sqc: 1.0565 | loss: 1.3350\n",
      "epoch:    0 | step:  6099 | train Sqc: 1.0534 | loss: 1.3346\n",
      "epoch:    0 | step:  6199 | train Sqc: 1.0502 | loss: 1.3341\n",
      "epoch:    0 | step:  6299 | train Sqc: 1.0472 | loss: 1.3336\n",
      "epoch:    0 | step:  6399 | train Sqc: 1.0443 | loss: 1.3332\n",
      "epoch:    0 | step:  6499 | train Sqc: 1.0414 | loss: 1.3328\n",
      "epoch:    0 | step:  6599 | train Sqc: 1.0383 | loss: 1.3323\n",
      "epoch:    0 | step:  6699 | train Sqc: 1.0354 | loss: 1.3319\n",
      "epoch:    0 | step:  6799 | train Sqc: 1.0330 | loss: 1.3315\n",
      "epoch:    0 | step:  6899 | train Sqc: 1.0303 | loss: 1.3311\n",
      "epoch:    0 | step:  6999 | train Sqc: 1.0273 | loss: 1.3307\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3733 | test Neg: 0.4149\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3801 | test Neg: 0.4136\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4436 | test Neg: 0.3965\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4242 | test Neg: 0.4021\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.5138 | test Neg: 0.3744\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.5106 | test Neg: 0.3751\n",
      "epoch:    0 | step:   99 | N:  10 | test Sqc: 0.6250 | test Neg: 0.3187\n",
      "epoch:    0 | step:  199 | N:  10 | test Sqc: 0.6404 | test Neg: 0.3144\n",
      "epoch:    0 | step:   99 | N:  12 | test Sqc: 1.1424 | test Neg: 0.0965\n",
      "epoch:    0 | step:  199 | N:  12 | test Sqc: 1.1482 | test Neg: 0.0959\n",
      "epoch:    0 | step:   99 | N:  14 | test Sqc: 1.3289 | test Neg: 0.0240\n",
      "epoch:    0 | step:  199 | N:  14 | test Sqc: 1.3350 | test Neg: 0.0217\n",
      "epoch:    0 | step:   99 | N:  16 | test Sqc: 1.3881 | test Neg: 0.0031\n",
      "epoch:    0 | step:  199 | N:  16 | test Sqc: 1.3887 | test Neg: 0.0027\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | train Sqc: 0.8448 | loss: 1.3022\n",
      "epoch:    1 | step:  199 | train Sqc: 0.8429 | loss: 1.3021\n",
      "epoch:    1 | step:  299 | train Sqc: 0.8388 | loss: 1.3018\n",
      "epoch:    1 | step:  399 | train Sqc: 0.8350 | loss: 1.3015\n",
      "epoch:    1 | step:  499 | train Sqc: 0.8394 | loss: 1.3016\n",
      "epoch:    1 | step:  599 | train Sqc: 0.8399 | loss: 1.3012\n",
      "epoch:    1 | step:  699 | train Sqc: 0.8391 | loss: 1.3012\n",
      "epoch:    1 | step:  799 | train Sqc: 0.8366 | loss: 1.3010\n",
      "epoch:    1 | step:  899 | train Sqc: 0.8358 | loss: 1.3009\n",
      "epoch:    1 | step:  999 | train Sqc: 0.8323 | loss: 1.3003\n",
      "epoch:    1 | step:  1099 | train Sqc: 0.8310 | loss: 1.3000\n",
      "epoch:    1 | step:  1199 | train Sqc: 0.8306 | loss: 1.3000\n",
      "epoch:    1 | step:  1299 | train Sqc: 0.8279 | loss: 1.2996\n",
      "epoch:    1 | step:  1399 | train Sqc: 0.8260 | loss: 1.2993\n",
      "epoch:    1 | step:  1499 | train Sqc: 0.8248 | loss: 1.2991\n",
      "epoch:    1 | step:  1599 | train Sqc: 0.8241 | loss: 1.2991\n",
      "epoch:    1 | step:  1699 | train Sqc: 0.8227 | loss: 1.2989\n",
      "epoch:    1 | step:  1799 | train Sqc: 0.8207 | loss: 1.2987\n",
      "epoch:    1 | step:  1899 | train Sqc: 0.8200 | loss: 1.2987\n",
      "epoch:    1 | step:  1999 | train Sqc: 0.8201 | loss: 1.2986\n",
      "epoch:    1 | step:  2099 | train Sqc: 0.8200 | loss: 1.2986\n",
      "epoch:    1 | step:  2199 | train Sqc: 0.8193 | loss: 1.2986\n",
      "epoch:    1 | step:  2299 | train Sqc: 0.8186 | loss: 1.2985\n",
      "epoch:    1 | step:  2399 | train Sqc: 0.8189 | loss: 1.2986\n",
      "epoch:    1 | step:  2499 | train Sqc: 0.8183 | loss: 1.2985\n",
      "epoch:    1 | step:  2599 | train Sqc: 0.8164 | loss: 1.2983\n",
      "epoch:    1 | step:  2699 | train Sqc: 0.8152 | loss: 1.2982\n",
      "epoch:    1 | step:  2799 | train Sqc: 0.8152 | loss: 1.2981\n",
      "epoch:    1 | step:  2899 | train Sqc: 0.8146 | loss: 1.2981\n",
      "epoch:    1 | step:  2999 | train Sqc: 0.8136 | loss: 1.2979\n",
      "epoch:    1 | step:  3099 | train Sqc: 0.8120 | loss: 1.2977\n",
      "epoch:    1 | step:  3199 | train Sqc: 0.8099 | loss: 1.2975\n",
      "epoch:    1 | step:  3299 | train Sqc: 0.8092 | loss: 1.2974\n",
      "epoch:    1 | step:  3399 | train Sqc: 0.8094 | loss: 1.2974\n",
      "epoch:    1 | step:  3499 | train Sqc: 0.8085 | loss: 1.2973\n",
      "epoch:    1 | step:  3599 | train Sqc: 0.8073 | loss: 1.2971\n",
      "epoch:    1 | step:  3699 | train Sqc: 0.8055 | loss: 1.2968\n",
      "epoch:    1 | step:  3799 | train Sqc: 0.8041 | loss: 1.2966\n",
      "epoch:    1 | step:  3899 | train Sqc: 0.8028 | loss: 1.2964\n",
      "epoch:    1 | step:  3999 | train Sqc: 0.8016 | loss: 1.2963\n",
      "epoch:    1 | step:  4099 | train Sqc: 0.8005 | loss: 1.2961\n",
      "epoch:    1 | step:  4199 | train Sqc: 0.7993 | loss: 1.2960\n",
      "epoch:    1 | step:  4299 | train Sqc: 0.7979 | loss: 1.2958\n",
      "epoch:    1 | step:  4399 | train Sqc: 0.7966 | loss: 1.2956\n",
      "epoch:    1 | step:  4499 | train Sqc: 0.7955 | loss: 1.2954\n",
      "epoch:    1 | step:  4599 | train Sqc: 0.7940 | loss: 1.2952\n",
      "epoch:    1 | step:  4699 | train Sqc: 0.7922 | loss: 1.2949\n",
      "epoch:    1 | step:  4799 | train Sqc: 0.7913 | loss: 1.2948\n",
      "epoch:    1 | step:  4899 | train Sqc: 0.7893 | loss: 1.2945\n",
      "epoch:    1 | step:  4999 | train Sqc: 0.7879 | loss: 1.2943\n",
      "epoch:    1 | step:  5099 | train Sqc: 0.7869 | loss: 1.2942\n",
      "epoch:    1 | step:  5199 | train Sqc: 0.7851 | loss: 1.2939\n",
      "epoch:    1 | step:  5299 | train Sqc: 0.7834 | loss: 1.2937\n",
      "epoch:    1 | step:  5399 | train Sqc: 0.7819 | loss: 1.2935\n",
      "epoch:    1 | step:  5499 | train Sqc: 0.7809 | loss: 1.2934\n",
      "epoch:    1 | step:  5599 | train Sqc: 0.7795 | loss: 1.2932\n",
      "epoch:    1 | step:  5699 | train Sqc: 0.7788 | loss: 1.2931\n",
      "epoch:    1 | step:  5799 | train Sqc: 0.7781 | loss: 1.2929\n",
      "epoch:    1 | step:  5899 | train Sqc: 0.7772 | loss: 1.2928\n",
      "epoch:    1 | step:  5999 | train Sqc: 0.7760 | loss: 1.2926\n",
      "epoch:    1 | step:  6099 | train Sqc: 0.7747 | loss: 1.2924\n",
      "epoch:    1 | step:  6199 | train Sqc: 0.7734 | loss: 1.2922\n",
      "epoch:    1 | step:  6299 | train Sqc: 0.7722 | loss: 1.2920\n",
      "epoch:    1 | step:  6399 | train Sqc: 0.7710 | loss: 1.2919\n",
      "epoch:    1 | step:  6499 | train Sqc: 0.7701 | loss: 1.2917\n",
      "epoch:    1 | step:  6599 | train Sqc: 0.7687 | loss: 1.2915\n",
      "epoch:    1 | step:  6699 | train Sqc: 0.7675 | loss: 1.2914\n",
      "epoch:    1 | step:  6799 | train Sqc: 0.7664 | loss: 1.2912\n",
      "epoch:    1 | step:  6899 | train Sqc: 0.7646 | loss: 1.2910\n",
      "epoch:    1 | step:  6999 | train Sqc: 0.7629 | loss: 1.2907\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3721 | test Neg: 0.4152\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3788 | test Neg: 0.4138\n",
      "epoch:    1 | step:   99 | N:  6 | test Sqc: 0.4370 | test Neg: 0.3977\n",
      "epoch:    1 | step:  199 | N:  6 | test Sqc: 0.4145 | test Neg: 0.4035\n",
      "epoch:    1 | step:   99 | N:  8 | test Sqc: 0.5047 | test Neg: 0.3805\n",
      "epoch:    1 | step:  199 | N:  8 | test Sqc: 0.5005 | test Neg: 0.3809\n",
      "epoch:    1 | step:   99 | N:  10 | test Sqc: 0.4974 | test Neg: 0.3796\n",
      "epoch:    1 | step:  199 | N:  10 | test Sqc: 0.5075 | test Neg: 0.3750\n",
      "epoch:    1 | step:   99 | N:  12 | test Sqc: 0.6222 | test Neg: 0.3387\n",
      "epoch:    1 | step:  199 | N:  12 | test Sqc: 0.6117 | test Neg: 0.3368\n",
      "epoch:    1 | step:   99 | N:  14 | test Sqc: 1.0097 | test Neg: 0.1316\n",
      "epoch:    1 | step:  199 | N:  14 | test Sqc: 1.0080 | test Neg: 0.1296\n",
      "epoch:    1 | step:   99 | N:  16 | test Sqc: 1.2165 | test Neg: 0.0405\n",
      "epoch:    1 | step:  199 | N:  16 | test Sqc: 1.2225 | test Neg: 0.0403\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | train Sqc: 0.6766 | loss: 1.2789\n",
      "epoch:    2 | step:  199 | train Sqc: 0.6821 | loss: 1.2793\n",
      "epoch:    2 | step:  299 | train Sqc: 0.6793 | loss: 1.2787\n",
      "epoch:    2 | step:  399 | train Sqc: 0.6765 | loss: 1.2786\n",
      "epoch:    2 | step:  499 | train Sqc: 0.6801 | loss: 1.2785\n",
      "epoch:    2 | step:  599 | train Sqc: 0.6789 | loss: 1.2780\n",
      "epoch:    2 | step:  699 | train Sqc: 0.6776 | loss: 1.2777\n",
      "epoch:    2 | step:  799 | train Sqc: 0.6740 | loss: 1.2776\n",
      "epoch:    2 | step:  899 | train Sqc: 0.6765 | loss: 1.2776\n",
      "epoch:    2 | step:  999 | train Sqc: 0.6722 | loss: 1.2771\n",
      "epoch:    2 | step:  1099 | train Sqc: 0.6712 | loss: 1.2769\n",
      "epoch:    2 | step:  1199 | train Sqc: 0.6699 | loss: 1.2766\n",
      "epoch:    2 | step:  1299 | train Sqc: 0.6665 | loss: 1.2762\n",
      "epoch:    2 | step:  1399 | train Sqc: 0.6651 | loss: 1.2759\n",
      "epoch:    2 | step:  1499 | train Sqc: 0.6639 | loss: 1.2757\n",
      "epoch:    2 | step:  1599 | train Sqc: 0.6620 | loss: 1.2754\n",
      "epoch:    2 | step:  1699 | train Sqc: 0.6612 | loss: 1.2753\n",
      "epoch:    2 | step:  1799 | train Sqc: 0.6603 | loss: 1.2753\n",
      "epoch:    2 | step:  1899 | train Sqc: 0.6599 | loss: 1.2754\n",
      "epoch:    2 | step:  1999 | train Sqc: 0.6601 | loss: 1.2753\n",
      "epoch:    2 | step:  2099 | train Sqc: 0.6588 | loss: 1.2751\n",
      "epoch:    2 | step:  2199 | train Sqc: 0.6575 | loss: 1.2750\n",
      "epoch:    2 | step:  2299 | train Sqc: 0.6570 | loss: 1.2749\n",
      "epoch:    2 | step:  2399 | train Sqc: 0.6560 | loss: 1.2749\n",
      "epoch:    2 | step:  2499 | train Sqc: 0.6556 | loss: 1.2748\n",
      "epoch:    2 | step:  2599 | train Sqc: 0.6540 | loss: 1.2747\n",
      "epoch:    2 | step:  2699 | train Sqc: 0.6527 | loss: 1.2745\n",
      "epoch:    2 | step:  2799 | train Sqc: 0.6525 | loss: 1.2744\n",
      "epoch:    2 | step:  2899 | train Sqc: 0.6510 | loss: 1.2743\n",
      "epoch:    2 | step:  2999 | train Sqc: 0.6508 | loss: 1.2741\n",
      "epoch:    2 | step:  3099 | train Sqc: 0.6491 | loss: 1.2739\n",
      "epoch:    2 | step:  3199 | train Sqc: 0.6471 | loss: 1.2737\n",
      "epoch:    2 | step:  3299 | train Sqc: 0.6461 | loss: 1.2735\n",
      "epoch:    2 | step:  3399 | train Sqc: 0.6449 | loss: 1.2734\n",
      "epoch:    2 | step:  3499 | train Sqc: 0.6435 | loss: 1.2731\n",
      "epoch:    2 | step:  3599 | train Sqc: 0.6428 | loss: 1.2730\n",
      "epoch:    2 | step:  3699 | train Sqc: 0.6421 | loss: 1.2729\n",
      "epoch:    2 | step:  3799 | train Sqc: 0.6406 | loss: 1.2726\n",
      "epoch:    2 | step:  3899 | train Sqc: 0.6393 | loss: 1.2725\n",
      "epoch:    2 | step:  3999 | train Sqc: 0.6385 | loss: 1.2724\n",
      "epoch:    2 | step:  4099 | train Sqc: 0.6377 | loss: 1.2723\n",
      "epoch:    2 | step:  4199 | train Sqc: 0.6367 | loss: 1.2722\n",
      "epoch:    2 | step:  4299 | train Sqc: 0.6357 | loss: 1.2720\n",
      "epoch:    2 | step:  4399 | train Sqc: 0.6349 | loss: 1.2719\n",
      "epoch:    2 | step:  4499 | train Sqc: 0.6341 | loss: 1.2718\n",
      "epoch:    2 | step:  4599 | train Sqc: 0.6329 | loss: 1.2716\n",
      "epoch:    2 | step:  4699 | train Sqc: 0.6318 | loss: 1.2715\n",
      "epoch:    2 | step:  4799 | train Sqc: 0.6317 | loss: 1.2715\n",
      "epoch:    2 | step:  4899 | train Sqc: 0.6310 | loss: 1.2713\n",
      "epoch:    2 | step:  4999 | train Sqc: 0.6302 | loss: 1.2712\n",
      "epoch:    2 | step:  5099 | train Sqc: 0.6300 | loss: 1.2712\n",
      "epoch:    2 | step:  5199 | train Sqc: 0.6286 | loss: 1.2710\n",
      "epoch:    2 | step:  5299 | train Sqc: 0.6277 | loss: 1.2709\n",
      "epoch:    2 | step:  5399 | train Sqc: 0.6270 | loss: 1.2708\n",
      "epoch:    2 | step:  5499 | train Sqc: 0.6265 | loss: 1.2707\n",
      "epoch:    2 | step:  5599 | train Sqc: 0.6253 | loss: 1.2706\n",
      "epoch:    2 | step:  5699 | train Sqc: 0.6247 | loss: 1.2705\n",
      "epoch:    2 | step:  5799 | train Sqc: 0.6245 | loss: 1.2704\n",
      "epoch:    2 | step:  5899 | train Sqc: 0.6239 | loss: 1.2703\n",
      "epoch:    2 | step:  5999 | train Sqc: 0.6228 | loss: 1.2702\n",
      "epoch:    2 | step:  6099 | train Sqc: 0.6225 | loss: 1.2701\n",
      "epoch:    2 | step:  6199 | train Sqc: 0.6215 | loss: 1.2700\n",
      "epoch:    2 | step:  6299 | train Sqc: 0.6211 | loss: 1.2699\n",
      "epoch:    2 | step:  6399 | train Sqc: 0.6204 | loss: 1.2698\n",
      "epoch:    2 | step:  6499 | train Sqc: 0.6200 | loss: 1.2697\n",
      "epoch:    2 | step:  6599 | train Sqc: 0.6191 | loss: 1.2695\n",
      "epoch:    2 | step:  6699 | train Sqc: 0.6181 | loss: 1.2694\n",
      "epoch:    2 | step:  6799 | train Sqc: 0.6175 | loss: 1.2693\n",
      "epoch:    2 | step:  6899 | train Sqc: 0.6163 | loss: 1.2691\n",
      "epoch:    2 | step:  6999 | train Sqc: 0.6151 | loss: 1.2690\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3698 | test Neg: 0.4155\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3766 | test Neg: 0.4141\n",
      "epoch:    2 | step:   99 | N:  6 | test Sqc: 0.4338 | test Neg: 0.3982\n",
      "epoch:    2 | step:  199 | N:  6 | test Sqc: 0.4126 | test Neg: 0.4039\n",
      "epoch:    2 | step:   99 | N:  8 | test Sqc: 0.5016 | test Neg: 0.3806\n",
      "epoch:    2 | step:  199 | N:  8 | test Sqc: 0.4925 | test Neg: 0.3812\n",
      "epoch:    2 | step:   99 | N:  10 | test Sqc: 0.5068 | test Neg: 0.3771\n",
      "epoch:    2 | step:  199 | N:  10 | test Sqc: 0.5142 | test Neg: 0.3726\n",
      "epoch:    2 | step:   99 | N:  12 | test Sqc: 0.5796 | test Neg: 0.3548\n",
      "epoch:    2 | step:  199 | N:  12 | test Sqc: 0.5767 | test Neg: 0.3539\n",
      "epoch:    2 | step:   99 | N:  14 | test Sqc: 0.7087 | test Neg: 0.3062\n",
      "epoch:    2 | step:  199 | N:  14 | test Sqc: 0.7129 | test Neg: 0.3026\n",
      "epoch:    2 | step:   99 | N:  16 | test Sqc: 0.8659 | test Neg: 0.2019\n",
      "epoch:    2 | step:  199 | N:  16 | test Sqc: 0.8742 | test Neg: 0.2026\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | train Sqc: 0.5677 | loss: 1.2617\n",
      "epoch:    3 | step:  199 | train Sqc: 0.5733 | loss: 1.2625\n",
      "epoch:    3 | step:  299 | train Sqc: 0.5690 | loss: 1.2618\n",
      "epoch:    3 | step:  399 | train Sqc: 0.5664 | loss: 1.2616\n",
      "epoch:    3 | step:  499 | train Sqc: 0.5720 | loss: 1.2619\n",
      "epoch:    3 | step:  599 | train Sqc: 0.5733 | loss: 1.2618\n",
      "epoch:    3 | step:  699 | train Sqc: 0.5713 | loss: 1.2616\n",
      "epoch:    3 | step:  799 | train Sqc: 0.5689 | loss: 1.2614\n",
      "epoch:    3 | step:  899 | train Sqc: 0.5692 | loss: 1.2613\n",
      "epoch:    3 | step:  999 | train Sqc: 0.5679 | loss: 1.2610\n",
      "epoch:    3 | step:  1099 | train Sqc: 0.5674 | loss: 1.2609\n",
      "epoch:    3 | step:  1199 | train Sqc: 0.5676 | loss: 1.2609\n",
      "epoch:    3 | step:  1299 | train Sqc: 0.5650 | loss: 1.2606\n",
      "epoch:    3 | step:  1399 | train Sqc: 0.5646 | loss: 1.2605\n",
      "epoch:    3 | step:  1499 | train Sqc: 0.5645 | loss: 1.2606\n",
      "epoch:    3 | step:  1599 | train Sqc: 0.5628 | loss: 1.2603\n",
      "epoch:    3 | step:  1699 | train Sqc: 0.5620 | loss: 1.2602\n",
      "epoch:    3 | step:  1799 | train Sqc: 0.5611 | loss: 1.2601\n",
      "epoch:    3 | step:  1899 | train Sqc: 0.5607 | loss: 1.2601\n",
      "epoch:    3 | step:  1999 | train Sqc: 0.5610 | loss: 1.2600\n",
      "epoch:    3 | step:  2099 | train Sqc: 0.5612 | loss: 1.2601\n",
      "epoch:    3 | step:  2199 | train Sqc: 0.5610 | loss: 1.2600\n",
      "epoch:    3 | step:  2299 | train Sqc: 0.5612 | loss: 1.2601\n",
      "epoch:    3 | step:  2399 | train Sqc: 0.5612 | loss: 1.2602\n",
      "epoch:    3 | step:  2499 | train Sqc: 0.5607 | loss: 1.2601\n",
      "epoch:    3 | step:  2599 | train Sqc: 0.5598 | loss: 1.2600\n",
      "epoch:    3 | step:  2699 | train Sqc: 0.5595 | loss: 1.2600\n",
      "epoch:    3 | step:  2799 | train Sqc: 0.5598 | loss: 1.2600\n",
      "epoch:    3 | step:  2899 | train Sqc: 0.5594 | loss: 1.2600\n",
      "epoch:    3 | step:  2999 | train Sqc: 0.5593 | loss: 1.2599\n",
      "epoch:    3 | step:  3099 | train Sqc: 0.5581 | loss: 1.2598\n",
      "epoch:    3 | step:  3199 | train Sqc: 0.5570 | loss: 1.2596\n",
      "epoch:    3 | step:  3299 | train Sqc: 0.5564 | loss: 1.2596\n",
      "epoch:    3 | step:  3399 | train Sqc: 0.5560 | loss: 1.2595\n",
      "epoch:    3 | step:  3499 | train Sqc: 0.5558 | loss: 1.2595\n",
      "epoch:    3 | step:  3599 | train Sqc: 0.5556 | loss: 1.2594\n",
      "epoch:    3 | step:  3699 | train Sqc: 0.5555 | loss: 1.2594\n",
      "epoch:    3 | step:  3799 | train Sqc: 0.5547 | loss: 1.2592\n",
      "epoch:    3 | step:  3899 | train Sqc: 0.5544 | loss: 1.2592\n",
      "epoch:    3 | step:  3999 | train Sqc: 0.5544 | loss: 1.2592\n",
      "epoch:    3 | step:  4099 | train Sqc: 0.5543 | loss: 1.2591\n",
      "epoch:    3 | step:  4199 | train Sqc: 0.5538 | loss: 1.2591\n",
      "epoch:    3 | step:  4299 | train Sqc: 0.5535 | loss: 1.2591\n",
      "epoch:    3 | step:  4399 | train Sqc: 0.5536 | loss: 1.2591\n",
      "epoch:    3 | step:  4499 | train Sqc: 0.5540 | loss: 1.2591\n",
      "epoch:    3 | step:  4599 | train Sqc: 0.5537 | loss: 1.2591\n",
      "epoch:    3 | step:  4699 | train Sqc: 0.5533 | loss: 1.2591\n",
      "epoch:    3 | step:  4799 | train Sqc: 0.5543 | loss: 1.2592\n",
      "epoch:    3 | step:  4899 | train Sqc: 0.5541 | loss: 1.2591\n",
      "epoch:    3 | step:  4999 | train Sqc: 0.5538 | loss: 1.2591\n",
      "epoch:    3 | step:  5099 | train Sqc: 0.5541 | loss: 1.2591\n",
      "epoch:    3 | step:  5199 | train Sqc: 0.5534 | loss: 1.2591\n",
      "epoch:    3 | step:  5299 | train Sqc: 0.5532 | loss: 1.2590\n",
      "epoch:    3 | step:  5399 | train Sqc: 0.5529 | loss: 1.2590\n",
      "epoch:    3 | step:  5499 | train Sqc: 0.5530 | loss: 1.2590\n",
      "epoch:    3 | step:  5599 | train Sqc: 0.5524 | loss: 1.2590\n",
      "epoch:    3 | step:  5699 | train Sqc: 0.5523 | loss: 1.2590\n",
      "epoch:    3 | step:  5799 | train Sqc: 0.5525 | loss: 1.2590\n",
      "epoch:    3 | step:  5899 | train Sqc: 0.5524 | loss: 1.2590\n",
      "epoch:    3 | step:  5999 | train Sqc: 0.5521 | loss: 1.2589\n",
      "epoch:    3 | step:  6099 | train Sqc: 0.5522 | loss: 1.2589\n",
      "epoch:    3 | step:  6199 | train Sqc: 0.5517 | loss: 1.2589\n",
      "epoch:    3 | step:  6299 | train Sqc: 0.5517 | loss: 1.2588\n",
      "epoch:    3 | step:  6399 | train Sqc: 0.5516 | loss: 1.2588\n",
      "epoch:    3 | step:  6499 | train Sqc: 0.5518 | loss: 1.2588\n",
      "epoch:    3 | step:  6599 | train Sqc: 0.5514 | loss: 1.2588\n",
      "epoch:    3 | step:  6699 | train Sqc: 0.5510 | loss: 1.2587\n",
      "epoch:    3 | step:  6799 | train Sqc: 0.5509 | loss: 1.2587\n",
      "epoch:    3 | step:  6899 | train Sqc: 0.5503 | loss: 1.2587\n",
      "epoch:    3 | step:  6999 | train Sqc: 0.5496 | loss: 1.2586\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | test Sqc: 0.3697 | test Neg: 0.4155\n",
      "epoch:    3 | step:  199 | N:  4 | test Sqc: 0.3762 | test Neg: 0.4142\n",
      "epoch:    3 | step:   99 | N:  6 | test Sqc: 0.4302 | test Neg: 0.3987\n",
      "epoch:    3 | step:  199 | N:  6 | test Sqc: 0.4112 | test Neg: 0.4043\n",
      "epoch:    3 | step:   99 | N:  8 | test Sqc: 0.4987 | test Neg: 0.3810\n",
      "epoch:    3 | step:  199 | N:  8 | test Sqc: 0.4934 | test Neg: 0.3814\n",
      "epoch:    3 | step:   99 | N:  10 | test Sqc: 0.4956 | test Neg: 0.3805\n",
      "epoch:    3 | step:  199 | N:  10 | test Sqc: 0.5030 | test Neg: 0.3758\n",
      "epoch:    3 | step:   99 | N:  12 | test Sqc: 0.5702 | test Neg: 0.3561\n",
      "epoch:    3 | step:  199 | N:  12 | test Sqc: 0.5671 | test Neg: 0.3554\n",
      "epoch:    3 | step:   99 | N:  14 | test Sqc: 0.6651 | test Neg: 0.3208\n",
      "epoch:    3 | step:  199 | N:  14 | test Sqc: 0.6709 | test Neg: 0.3190\n",
      "epoch:    3 | step:   99 | N:  16 | test Sqc: 0.7438 | test Neg: 0.2885\n",
      "epoch:    3 | step:  199 | N:  16 | test Sqc: 0.7440 | test Neg: 0.2872\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | train Sqc: 0.5560 | loss: 1.2597\n",
      "epoch:    4 | step:  199 | train Sqc: 0.5784 | loss: 1.2616\n",
      "epoch:    4 | step:  299 | train Sqc: 0.5672 | loss: 1.2603\n",
      "epoch:    4 | step:  399 | train Sqc: 0.5629 | loss: 1.2598\n",
      "epoch:    4 | step:  499 | train Sqc: 0.5646 | loss: 1.2596\n",
      "epoch:    4 | step:  599 | train Sqc: 0.5625 | loss: 1.2592\n",
      "epoch:    4 | step:  699 | train Sqc: 0.5587 | loss: 1.2588\n",
      "epoch:    4 | step:  799 | train Sqc: 0.5570 | loss: 1.2588\n",
      "epoch:    4 | step:  899 | train Sqc: 0.5567 | loss: 1.2587\n",
      "epoch:    4 | step:  999 | train Sqc: 0.5548 | loss: 1.2584\n",
      "epoch:    4 | step:  1099 | train Sqc: 0.5544 | loss: 1.2583\n",
      "epoch:    4 | step:  1199 | train Sqc: 0.5540 | loss: 1.2583\n",
      "epoch:    4 | step:  1299 | train Sqc: 0.5506 | loss: 1.2579\n",
      "epoch:    4 | step:  1399 | train Sqc: 0.5494 | loss: 1.2577\n",
      "epoch:    4 | step:  1499 | train Sqc: 0.5487 | loss: 1.2576\n",
      "epoch:    4 | step:  1599 | train Sqc: 0.5469 | loss: 1.2574\n",
      "epoch:    4 | step:  1699 | train Sqc: 0.5466 | loss: 1.2573\n",
      "epoch:    4 | step:  1799 | train Sqc: 0.5457 | loss: 1.2573\n",
      "epoch:    4 | step:  1899 | train Sqc: 0.5452 | loss: 1.2573\n",
      "epoch:    4 | step:  1999 | train Sqc: 0.5458 | loss: 1.2573\n",
      "epoch:    4 | step:  2099 | train Sqc: 0.5456 | loss: 1.2573\n",
      "epoch:    4 | step:  2199 | train Sqc: 0.5455 | loss: 1.2573\n",
      "epoch:    4 | step:  2299 | train Sqc: 0.5459 | loss: 1.2574\n",
      "epoch:    4 | step:  2399 | train Sqc: 0.5463 | loss: 1.2575\n",
      "epoch:    4 | step:  2499 | train Sqc: 0.5461 | loss: 1.2575\n",
      "epoch:    4 | step:  2599 | train Sqc: 0.5450 | loss: 1.2574\n",
      "epoch:    4 | step:  2699 | train Sqc: 0.5448 | loss: 1.2574\n",
      "epoch:    4 | step:  2799 | train Sqc: 0.5451 | loss: 1.2574\n",
      "epoch:    4 | step:  2899 | train Sqc: 0.5449 | loss: 1.2574\n",
      "epoch:    4 | step:  2999 | train Sqc: 0.5448 | loss: 1.2574\n",
      "epoch:    4 | step:  3099 | train Sqc: 0.5437 | loss: 1.2573\n",
      "epoch:    4 | step:  3199 | train Sqc: 0.5428 | loss: 1.2571\n",
      "epoch:    4 | step:  3299 | train Sqc: 0.5438 | loss: 1.2573\n",
      "epoch:    4 | step:  3399 | train Sqc: 0.5441 | loss: 1.2574\n",
      "epoch:    4 | step:  3499 | train Sqc: 0.5442 | loss: 1.2574\n",
      "epoch:    4 | step:  3599 | train Sqc: 0.5441 | loss: 1.2573\n",
      "epoch:    4 | step:  3699 | train Sqc: 0.5442 | loss: 1.2573\n",
      "epoch:    4 | step:  3799 | train Sqc: 0.5434 | loss: 1.2572\n",
      "epoch:    4 | step:  3899 | train Sqc: 0.5430 | loss: 1.2572\n",
      "epoch:    4 | step:  3999 | train Sqc: 0.5431 | loss: 1.2572\n",
      "epoch:    4 | step:  4099 | train Sqc: 0.5431 | loss: 1.2572\n",
      "epoch:    4 | step:  4199 | train Sqc: 0.5426 | loss: 1.2572\n",
      "epoch:    4 | step:  4299 | train Sqc: 0.5424 | loss: 1.2571\n",
      "epoch:    4 | step:  4399 | train Sqc: 0.5424 | loss: 1.2571\n",
      "epoch:    4 | step:  4499 | train Sqc: 0.5425 | loss: 1.2571\n",
      "epoch:    4 | step:  4599 | train Sqc: 0.5420 | loss: 1.2571\n",
      "epoch:    4 | step:  4699 | train Sqc: 0.5413 | loss: 1.2570\n",
      "epoch:    4 | step:  4799 | train Sqc: 0.5422 | loss: 1.2571\n",
      "epoch:    4 | step:  4899 | train Sqc: 0.5419 | loss: 1.2570\n",
      "epoch:    4 | step:  4999 | train Sqc: 0.5416 | loss: 1.2570\n",
      "epoch:    4 | step:  5099 | train Sqc: 0.5418 | loss: 1.2570\n",
      "epoch:    4 | step:  5199 | train Sqc: 0.5413 | loss: 1.2570\n",
      "epoch:    4 | step:  5299 | train Sqc: 0.5411 | loss: 1.2570\n",
      "epoch:    4 | step:  5399 | train Sqc: 0.5409 | loss: 1.2570\n",
      "epoch:    4 | step:  5499 | train Sqc: 0.5411 | loss: 1.2570\n",
      "epoch:    4 | step:  5599 | train Sqc: 0.5408 | loss: 1.2570\n",
      "epoch:    4 | step:  5699 | train Sqc: 0.5408 | loss: 1.2570\n",
      "epoch:    4 | step:  5799 | train Sqc: 0.5411 | loss: 1.2570\n",
      "epoch:    4 | step:  5899 | train Sqc: 0.5411 | loss: 1.2570\n",
      "epoch:    4 | step:  5999 | train Sqc: 0.5409 | loss: 1.2570\n",
      "epoch:    4 | step:  6099 | train Sqc: 0.5410 | loss: 1.2570\n",
      "epoch:    4 | step:  6199 | train Sqc: 0.5407 | loss: 1.2569\n",
      "epoch:    4 | step:  6299 | train Sqc: 0.5407 | loss: 1.2569\n",
      "epoch:    4 | step:  6399 | train Sqc: 0.5406 | loss: 1.2569\n",
      "epoch:    4 | step:  6499 | train Sqc: 0.5409 | loss: 1.2569\n",
      "epoch:    4 | step:  6599 | train Sqc: 0.5406 | loss: 1.2569\n",
      "epoch:    4 | step:  6699 | train Sqc: 0.5402 | loss: 1.2569\n",
      "epoch:    4 | step:  6799 | train Sqc: 0.5403 | loss: 1.2569\n",
      "epoch:    4 | step:  6899 | train Sqc: 0.5398 | loss: 1.2568\n",
      "epoch:    4 | step:  6999 | train Sqc: 0.5392 | loss: 1.2568\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | test Sqc: 0.3697 | test Neg: 0.4156\n",
      "epoch:    4 | step:  199 | N:  4 | test Sqc: 0.3748 | test Neg: 0.4142\n",
      "epoch:    4 | step:   99 | N:  6 | test Sqc: 0.4305 | test Neg: 0.3991\n",
      "epoch:    4 | step:  199 | N:  6 | test Sqc: 0.4099 | test Neg: 0.4046\n",
      "epoch:    4 | step:   99 | N:  8 | test Sqc: 0.4984 | test Neg: 0.3813\n",
      "epoch:    4 | step:  199 | N:  8 | test Sqc: 0.4933 | test Neg: 0.3818\n",
      "epoch:    4 | step:   99 | N:  10 | test Sqc: 0.4930 | test Neg: 0.3805\n",
      "epoch:    4 | step:  199 | N:  10 | test Sqc: 0.5013 | test Neg: 0.3759\n",
      "epoch:    4 | step:   99 | N:  12 | test Sqc: 0.5709 | test Neg: 0.3571\n",
      "epoch:    4 | step:  199 | N:  12 | test Sqc: 0.5663 | test Neg: 0.3563\n",
      "epoch:    4 | step:   99 | N:  14 | test Sqc: 0.6608 | test Neg: 0.3219\n",
      "epoch:    4 | step:  199 | N:  14 | test Sqc: 0.6694 | test Neg: 0.3196\n",
      "epoch:    4 | step:   99 | N:  16 | test Sqc: 0.7340 | test Neg: 0.2918\n",
      "epoch:    4 | step:  199 | N:  16 | test Sqc: 0.7326 | test Neg: 0.2918\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | train Sqc: 0.5389 | loss: 1.2570\n",
      "epoch:    5 | step:  199 | train Sqc: 0.5477 | loss: 1.2579\n",
      "epoch:    5 | step:  299 | train Sqc: 0.5392 | loss: 1.2567\n",
      "epoch:    5 | step:  399 | train Sqc: 0.5361 | loss: 1.2564\n",
      "epoch:    5 | step:  499 | train Sqc: 0.5418 | loss: 1.2567\n",
      "epoch:    5 | step:  599 | train Sqc: 0.5411 | loss: 1.2564\n",
      "epoch:    5 | step:  699 | train Sqc: 0.5390 | loss: 1.2561\n",
      "epoch:    5 | step:  799 | train Sqc: 0.5372 | loss: 1.2561\n",
      "epoch:    5 | step:  899 | train Sqc: 0.5383 | loss: 1.2562\n",
      "epoch:    5 | step:  999 | train Sqc: 0.5369 | loss: 1.2559\n",
      "epoch:    5 | step:  1099 | train Sqc: 0.5373 | loss: 1.2559\n",
      "epoch:    5 | step:  1199 | train Sqc: 0.5377 | loss: 1.2560\n",
      "epoch:    5 | step:  1299 | train Sqc: 0.5354 | loss: 1.2557\n",
      "epoch:    5 | step:  1399 | train Sqc: 0.5348 | loss: 1.2556\n",
      "epoch:    5 | step:  1499 | train Sqc: 0.5346 | loss: 1.2556\n",
      "epoch:    5 | step:  1599 | train Sqc: 0.5335 | loss: 1.2555\n",
      "epoch:    5 | step:  1699 | train Sqc: 0.5337 | loss: 1.2555\n",
      "epoch:    5 | step:  1799 | train Sqc: 0.5336 | loss: 1.2555\n",
      "epoch:    5 | step:  1899 | train Sqc: 0.5336 | loss: 1.2556\n",
      "epoch:    5 | step:  1999 | train Sqc: 0.5344 | loss: 1.2556\n",
      "epoch:    5 | step:  2099 | train Sqc: 0.5346 | loss: 1.2557\n",
      "epoch:    5 | step:  2199 | train Sqc: 0.5349 | loss: 1.2557\n",
      "epoch:    5 | step:  2299 | train Sqc: 0.5357 | loss: 1.2559\n",
      "epoch:    5 | step:  2399 | train Sqc: 0.5381 | loss: 1.2563\n",
      "epoch:    5 | step:  2499 | train Sqc: 0.5392 | loss: 1.2564\n",
      "epoch:    5 | step:  2599 | train Sqc: 0.5385 | loss: 1.2564\n",
      "epoch:    5 | step:  2699 | train Sqc: 0.5391 | loss: 1.2566\n",
      "epoch:    5 | step:  2799 | train Sqc: 0.5396 | loss: 1.2566\n",
      "epoch:    5 | step:  2899 | train Sqc: 0.5395 | loss: 1.2566\n",
      "epoch:    5 | step:  2999 | train Sqc: 0.5394 | loss: 1.2565\n",
      "epoch:    5 | step:  3099 | train Sqc: 0.5382 | loss: 1.2564\n",
      "epoch:    5 | step:  3199 | train Sqc: 0.5372 | loss: 1.2563\n",
      "epoch:    5 | step:  3299 | train Sqc: 0.5370 | loss: 1.2563\n",
      "epoch:    5 | step:  3399 | train Sqc: 0.5368 | loss: 1.2563\n",
      "epoch:    5 | step:  3499 | train Sqc: 0.5367 | loss: 1.2563\n",
      "epoch:    5 | step:  3599 | train Sqc: 0.5367 | loss: 1.2562\n",
      "epoch:    5 | step:  3699 | train Sqc: 0.5367 | loss: 1.2562\n",
      "epoch:    5 | step:  3799 | train Sqc: 0.5359 | loss: 1.2561\n",
      "epoch:    5 | step:  3899 | train Sqc: 0.5355 | loss: 1.2561\n",
      "epoch:    5 | step:  3999 | train Sqc: 0.5357 | loss: 1.2561\n",
      "epoch:    5 | step:  4099 | train Sqc: 0.5357 | loss: 1.2561\n",
      "epoch:    5 | step:  4199 | train Sqc: 0.5355 | loss: 1.2561\n",
      "epoch:    5 | step:  4299 | train Sqc: 0.5353 | loss: 1.2561\n",
      "epoch:    5 | step:  4399 | train Sqc: 0.5355 | loss: 1.2561\n",
      "epoch:    5 | step:  4499 | train Sqc: 0.5358 | loss: 1.2562\n",
      "epoch:    5 | step:  4599 | train Sqc: 0.5353 | loss: 1.2561\n",
      "epoch:    5 | step:  4699 | train Sqc: 0.5348 | loss: 1.2560\n",
      "epoch:    5 | step:  4799 | train Sqc: 0.5357 | loss: 1.2561\n",
      "epoch:    5 | step:  4899 | train Sqc: 0.5355 | loss: 1.2561\n",
      "epoch:    5 | step:  4999 | train Sqc: 0.5352 | loss: 1.2561\n",
      "epoch:    5 | step:  5099 | train Sqc: 0.5356 | loss: 1.2561\n",
      "epoch:    5 | step:  5199 | train Sqc: 0.5351 | loss: 1.2561\n",
      "epoch:    5 | step:  5299 | train Sqc: 0.5349 | loss: 1.2561\n",
      "epoch:    5 | step:  5399 | train Sqc: 0.5349 | loss: 1.2561\n",
      "epoch:    5 | step:  5499 | train Sqc: 0.5351 | loss: 1.2561\n",
      "epoch:    5 | step:  5599 | train Sqc: 0.5348 | loss: 1.2561\n",
      "epoch:    5 | step:  5699 | train Sqc: 0.5347 | loss: 1.2561\n",
      "epoch:    5 | step:  5799 | train Sqc: 0.5350 | loss: 1.2561\n",
      "epoch:    5 | step:  5899 | train Sqc: 0.5350 | loss: 1.2561\n",
      "epoch:    5 | step:  5999 | train Sqc: 0.5348 | loss: 1.2561\n",
      "epoch:    5 | step:  6099 | train Sqc: 0.5350 | loss: 1.2561\n",
      "epoch:    5 | step:  6199 | train Sqc: 0.5347 | loss: 1.2561\n",
      "epoch:    5 | step:  6299 | train Sqc: 0.5348 | loss: 1.2561\n",
      "epoch:    5 | step:  6399 | train Sqc: 0.5349 | loss: 1.2561\n",
      "epoch:    5 | step:  6499 | train Sqc: 0.5352 | loss: 1.2561\n",
      "epoch:    5 | step:  6599 | train Sqc: 0.5348 | loss: 1.2561\n",
      "epoch:    5 | step:  6699 | train Sqc: 0.5346 | loss: 1.2560\n",
      "epoch:    5 | step:  6799 | train Sqc: 0.5347 | loss: 1.2560\n",
      "epoch:    5 | step:  6899 | train Sqc: 0.5342 | loss: 1.2560\n",
      "epoch:    5 | step:  6999 | train Sqc: 0.5335 | loss: 1.2559\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | test Sqc: 0.3681 | test Neg: 0.4155\n",
      "epoch:    5 | step:  199 | N:  4 | test Sqc: 0.3738 | test Neg: 0.4142\n",
      "epoch:    5 | step:   99 | N:  6 | test Sqc: 0.4300 | test Neg: 0.3990\n",
      "epoch:    5 | step:  199 | N:  6 | test Sqc: 0.4096 | test Neg: 0.4046\n",
      "epoch:    5 | step:   99 | N:  8 | test Sqc: 0.4948 | test Neg: 0.3815\n",
      "epoch:    5 | step:  199 | N:  8 | test Sqc: 0.4909 | test Neg: 0.3819\n",
      "epoch:    5 | step:   99 | N:  10 | test Sqc: 0.4881 | test Neg: 0.3806\n",
      "epoch:    5 | step:  199 | N:  10 | test Sqc: 0.4982 | test Neg: 0.3759\n",
      "epoch:    5 | step:   99 | N:  12 | test Sqc: 0.5679 | test Neg: 0.3578\n",
      "epoch:    5 | step:  199 | N:  12 | test Sqc: 0.5645 | test Neg: 0.3567\n",
      "epoch:    5 | step:   99 | N:  14 | test Sqc: 0.6634 | test Neg: 0.3206\n",
      "epoch:    5 | step:  199 | N:  14 | test Sqc: 0.6698 | test Neg: 0.3184\n",
      "epoch:    5 | step:   99 | N:  16 | test Sqc: 0.7186 | test Neg: 0.2986\n",
      "epoch:    5 | step:  199 | N:  16 | test Sqc: 0.7221 | test Neg: 0.2976\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | train Sqc: 0.5375 | loss: 1.2574\n",
      "epoch:    6 | step:  199 | train Sqc: 0.5457 | loss: 1.2579\n",
      "epoch:    6 | step:  299 | train Sqc: 0.5376 | loss: 1.2566\n",
      "epoch:    6 | step:  399 | train Sqc: 0.5342 | loss: 1.2561\n",
      "epoch:    6 | step:  499 | train Sqc: 0.5400 | loss: 1.2564\n",
      "epoch:    6 | step:  599 | train Sqc: 0.5403 | loss: 1.2563\n",
      "epoch:    6 | step:  699 | train Sqc: 0.5386 | loss: 1.2561\n",
      "epoch:    6 | step:  799 | train Sqc: 0.5369 | loss: 1.2561\n",
      "epoch:    6 | step:  899 | train Sqc: 0.5377 | loss: 1.2561\n",
      "epoch:    6 | step:  999 | train Sqc: 0.5359 | loss: 1.2558\n",
      "epoch:    6 | step:  1099 | train Sqc: 0.5361 | loss: 1.2558\n",
      "epoch:    6 | step:  1199 | train Sqc: 0.5360 | loss: 1.2557\n",
      "epoch:    6 | step:  1299 | train Sqc: 0.5337 | loss: 1.2554\n",
      "epoch:    6 | step:  1399 | train Sqc: 0.5330 | loss: 1.2553\n",
      "epoch:    6 | step:  1499 | train Sqc: 0.5326 | loss: 1.2553\n",
      "epoch:    6 | step:  1599 | train Sqc: 0.5315 | loss: 1.2552\n",
      "epoch:    6 | step:  1699 | train Sqc: 0.5315 | loss: 1.2552\n",
      "epoch:    6 | step:  1799 | train Sqc: 0.5315 | loss: 1.2552\n",
      "epoch:    6 | step:  1899 | train Sqc: 0.5317 | loss: 1.2553\n",
      "epoch:    6 | step:  1999 | train Sqc: 0.5327 | loss: 1.2554\n",
      "epoch:    6 | step:  2099 | train Sqc: 0.5328 | loss: 1.2554\n",
      "epoch:    6 | step:  2199 | train Sqc: 0.5330 | loss: 1.2555\n",
      "epoch:    6 | step:  2299 | train Sqc: 0.5337 | loss: 1.2556\n",
      "epoch:    6 | step:  2399 | train Sqc: 0.5342 | loss: 1.2558\n",
      "epoch:    6 | step:  2499 | train Sqc: 0.5346 | loss: 1.2558\n",
      "epoch:    6 | step:  2599 | train Sqc: 0.5338 | loss: 1.2558\n",
      "epoch:    6 | step:  2699 | train Sqc: 0.5339 | loss: 1.2558\n",
      "epoch:    6 | step:  2799 | train Sqc: 0.5344 | loss: 1.2558\n",
      "epoch:    6 | step:  2899 | train Sqc: 0.5344 | loss: 1.2559\n",
      "epoch:    6 | step:  2999 | train Sqc: 0.5344 | loss: 1.2558\n",
      "epoch:    6 | step:  3099 | train Sqc: 0.5332 | loss: 1.2557\n",
      "epoch:    6 | step:  3199 | train Sqc: 0.5322 | loss: 1.2556\n",
      "epoch:    6 | step:  3299 | train Sqc: 0.5321 | loss: 1.2557\n",
      "epoch:    6 | step:  3399 | train Sqc: 0.5320 | loss: 1.2557\n",
      "epoch:    6 | step:  3499 | train Sqc: 0.5319 | loss: 1.2556\n",
      "epoch:    6 | step:  3599 | train Sqc: 0.5319 | loss: 1.2556\n",
      "epoch:    6 | step:  3699 | train Sqc: 0.5320 | loss: 1.2556\n",
      "epoch:    6 | step:  3799 | train Sqc: 0.5313 | loss: 1.2555\n",
      "epoch:    6 | step:  3899 | train Sqc: 0.5309 | loss: 1.2555\n",
      "epoch:    6 | step:  3999 | train Sqc: 0.5312 | loss: 1.2555\n",
      "epoch:    6 | step:  4099 | train Sqc: 0.5313 | loss: 1.2555\n",
      "epoch:    6 | step:  4199 | train Sqc: 0.5310 | loss: 1.2556\n",
      "epoch:    6 | step:  4299 | train Sqc: 0.5310 | loss: 1.2555\n",
      "epoch:    6 | step:  4399 | train Sqc: 0.5311 | loss: 1.2555\n",
      "epoch:    6 | step:  4499 | train Sqc: 0.5314 | loss: 1.2556\n",
      "epoch:    6 | step:  4599 | train Sqc: 0.5311 | loss: 1.2555\n",
      "epoch:    6 | step:  4699 | train Sqc: 0.5306 | loss: 1.2555\n",
      "epoch:    6 | step:  4799 | train Sqc: 0.5316 | loss: 1.2556\n",
      "epoch:    6 | step:  4899 | train Sqc: 0.5314 | loss: 1.2555\n",
      "epoch:    6 | step:  4999 | train Sqc: 0.5312 | loss: 1.2555\n",
      "epoch:    6 | step:  5099 | train Sqc: 0.5316 | loss: 1.2556\n",
      "epoch:    6 | step:  5199 | train Sqc: 0.5313 | loss: 1.2556\n",
      "epoch:    6 | step:  5299 | train Sqc: 0.5312 | loss: 1.2556\n",
      "epoch:    6 | step:  5399 | train Sqc: 0.5312 | loss: 1.2556\n",
      "epoch:    6 | step:  5499 | train Sqc: 0.5315 | loss: 1.2556\n",
      "epoch:    6 | step:  5599 | train Sqc: 0.5321 | loss: 1.2557\n",
      "epoch:    6 | step:  5699 | train Sqc: 0.5321 | loss: 1.2557\n",
      "epoch:    6 | step:  5799 | train Sqc: 0.5325 | loss: 1.2558\n",
      "epoch:    6 | step:  5899 | train Sqc: 0.5325 | loss: 1.2558\n",
      "epoch:    6 | step:  5999 | train Sqc: 0.5322 | loss: 1.2557\n",
      "epoch:    6 | step:  6099 | train Sqc: 0.5324 | loss: 1.2558\n",
      "epoch:    6 | step:  6199 | train Sqc: 0.5322 | loss: 1.2557\n",
      "epoch:    6 | step:  6299 | train Sqc: 0.5322 | loss: 1.2557\n",
      "epoch:    6 | step:  6399 | train Sqc: 0.5322 | loss: 1.2557\n",
      "epoch:    6 | step:  6499 | train Sqc: 0.5325 | loss: 1.2558\n",
      "epoch:    6 | step:  6599 | train Sqc: 0.5322 | loss: 1.2557\n",
      "epoch:    6 | step:  6699 | train Sqc: 0.5320 | loss: 1.2557\n",
      "epoch:    6 | step:  6799 | train Sqc: 0.5321 | loss: 1.2557\n",
      "epoch:    6 | step:  6899 | train Sqc: 0.5316 | loss: 1.2557\n",
      "epoch:    6 | step:  6999 | train Sqc: 0.5310 | loss: 1.2556\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | test Sqc: 0.3675 | test Neg: 0.4155\n",
      "epoch:    6 | step:  199 | N:  4 | test Sqc: 0.3737 | test Neg: 0.4142\n",
      "epoch:    6 | step:   99 | N:  6 | test Sqc: 0.4294 | test Neg: 0.3993\n",
      "epoch:    6 | step:  199 | N:  6 | test Sqc: 0.4092 | test Neg: 0.4047\n",
      "epoch:    6 | step:   99 | N:  8 | test Sqc: 0.4955 | test Neg: 0.3815\n",
      "epoch:    6 | step:  199 | N:  8 | test Sqc: 0.4912 | test Neg: 0.3819\n",
      "epoch:    6 | step:   99 | N:  10 | test Sqc: 0.4925 | test Neg: 0.3791\n",
      "epoch:    6 | step:  199 | N:  10 | test Sqc: 0.5065 | test Neg: 0.3743\n",
      "epoch:    6 | step:   99 | N:  12 | test Sqc: 0.5681 | test Neg: 0.3576\n",
      "epoch:    6 | step:  199 | N:  12 | test Sqc: 0.5628 | test Neg: 0.3569\n",
      "epoch:    6 | step:   99 | N:  14 | test Sqc: 0.6562 | test Neg: 0.3232\n",
      "epoch:    6 | step:  199 | N:  14 | test Sqc: 0.6628 | test Neg: 0.3207\n",
      "epoch:    6 | step:   99 | N:  16 | test Sqc: 0.7268 | test Neg: 0.2923\n",
      "epoch:    6 | step:  199 | N:  16 | test Sqc: 0.7322 | test Neg: 0.2904\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | train Sqc: 0.5334 | loss: 1.2566\n",
      "epoch:    7 | step:  199 | train Sqc: 0.5418 | loss: 1.2573\n",
      "epoch:    7 | step:  299 | train Sqc: 0.5338 | loss: 1.2560\n",
      "epoch:    7 | step:  399 | train Sqc: 0.5305 | loss: 1.2556\n",
      "epoch:    7 | step:  499 | train Sqc: 0.5368 | loss: 1.2560\n",
      "epoch:    7 | step:  599 | train Sqc: 0.5362 | loss: 1.2556\n",
      "epoch:    7 | step:  699 | train Sqc: 0.5342 | loss: 1.2554\n",
      "epoch:    7 | step:  799 | train Sqc: 0.5326 | loss: 1.2555\n",
      "epoch:    7 | step:  899 | train Sqc: 0.5340 | loss: 1.2555\n",
      "epoch:    7 | step:  999 | train Sqc: 0.5326 | loss: 1.2553\n",
      "epoch:    7 | step:  1099 | train Sqc: 0.5331 | loss: 1.2554\n",
      "epoch:    7 | step:  1199 | train Sqc: 0.5333 | loss: 1.2554\n",
      "epoch:    7 | step:  1299 | train Sqc: 0.5308 | loss: 1.2551\n",
      "epoch:    7 | step:  1399 | train Sqc: 0.5314 | loss: 1.2552\n",
      "epoch:    7 | step:  1499 | train Sqc: 0.5317 | loss: 1.2552\n",
      "epoch:    7 | step:  1599 | train Sqc: 0.5302 | loss: 1.2551\n",
      "epoch:    7 | step:  1699 | train Sqc: 0.5302 | loss: 1.2550\n",
      "epoch:    7 | step:  1799 | train Sqc: 0.5299 | loss: 1.2550\n",
      "epoch:    7 | step:  1899 | train Sqc: 0.5298 | loss: 1.2551\n",
      "epoch:    7 | step:  1999 | train Sqc: 0.5312 | loss: 1.2552\n",
      "epoch:    7 | step:  2099 | train Sqc: 0.5311 | loss: 1.2552\n",
      "epoch:    7 | step:  2199 | train Sqc: 0.5312 | loss: 1.2553\n",
      "epoch:    7 | step:  2299 | train Sqc: 0.5316 | loss: 1.2554\n",
      "epoch:    7 | step:  2399 | train Sqc: 0.5322 | loss: 1.2555\n",
      "epoch:    7 | step:  2499 | train Sqc: 0.5323 | loss: 1.2556\n",
      "epoch:    7 | step:  2599 | train Sqc: 0.5314 | loss: 1.2555\n",
      "epoch:    7 | step:  2699 | train Sqc: 0.5315 | loss: 1.2555\n",
      "epoch:    7 | step:  2799 | train Sqc: 0.5320 | loss: 1.2555\n",
      "epoch:    7 | step:  2899 | train Sqc: 0.5319 | loss: 1.2556\n",
      "epoch:    7 | step:  2999 | train Sqc: 0.5320 | loss: 1.2555\n",
      "epoch:    7 | step:  3099 | train Sqc: 0.5310 | loss: 1.2554\n",
      "epoch:    7 | step:  3199 | train Sqc: 0.5299 | loss: 1.2553\n",
      "epoch:    7 | step:  3299 | train Sqc: 0.5298 | loss: 1.2553\n",
      "epoch:    7 | step:  3399 | train Sqc: 0.5297 | loss: 1.2553\n",
      "epoch:    7 | step:  3499 | train Sqc: 0.5297 | loss: 1.2553\n",
      "epoch:    7 | step:  3599 | train Sqc: 0.5297 | loss: 1.2553\n",
      "epoch:    7 | step:  3699 | train Sqc: 0.5299 | loss: 1.2553\n",
      "epoch:    7 | step:  3799 | train Sqc: 0.5291 | loss: 1.2552\n",
      "epoch:    7 | step:  3899 | train Sqc: 0.5288 | loss: 1.2552\n",
      "epoch:    7 | step:  3999 | train Sqc: 0.5290 | loss: 1.2552\n",
      "epoch:    7 | step:  4099 | train Sqc: 0.5291 | loss: 1.2552\n",
      "epoch:    7 | step:  4199 | train Sqc: 0.5288 | loss: 1.2552\n",
      "epoch:    7 | step:  4299 | train Sqc: 0.5287 | loss: 1.2552\n",
      "epoch:    7 | step:  4399 | train Sqc: 0.5289 | loss: 1.2552\n",
      "epoch:    7 | step:  4499 | train Sqc: 0.5293 | loss: 1.2553\n",
      "epoch:    7 | step:  4599 | train Sqc: 0.5290 | loss: 1.2552\n",
      "epoch:    7 | step:  4699 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:    7 | step:  4799 | train Sqc: 0.5295 | loss: 1.2553\n",
      "epoch:    7 | step:  4899 | train Sqc: 0.5293 | loss: 1.2552\n",
      "epoch:    7 | step:  4999 | train Sqc: 0.5291 | loss: 1.2552\n",
      "epoch:    7 | step:  5099 | train Sqc: 0.5295 | loss: 1.2553\n",
      "epoch:    7 | step:  5199 | train Sqc: 0.5292 | loss: 1.2553\n",
      "epoch:    7 | step:  5299 | train Sqc: 0.5292 | loss: 1.2553\n",
      "epoch:    7 | step:  5399 | train Sqc: 0.5292 | loss: 1.2553\n",
      "epoch:    7 | step:  5499 | train Sqc: 0.5296 | loss: 1.2553\n",
      "epoch:    7 | step:  5599 | train Sqc: 0.5293 | loss: 1.2553\n",
      "epoch:    7 | step:  5699 | train Sqc: 0.5293 | loss: 1.2553\n",
      "epoch:    7 | step:  5799 | train Sqc: 0.5296 | loss: 1.2554\n",
      "epoch:    7 | step:  5899 | train Sqc: 0.5297 | loss: 1.2554\n",
      "epoch:    7 | step:  5999 | train Sqc: 0.5295 | loss: 1.2553\n",
      "epoch:    7 | step:  6099 | train Sqc: 0.5297 | loss: 1.2554\n",
      "epoch:    7 | step:  6199 | train Sqc: 0.5295 | loss: 1.2553\n",
      "epoch:    7 | step:  6299 | train Sqc: 0.5298 | loss: 1.2554\n",
      "epoch:    7 | step:  6399 | train Sqc: 0.5299 | loss: 1.2554\n",
      "epoch:    7 | step:  6499 | train Sqc: 0.5302 | loss: 1.2554\n",
      "epoch:    7 | step:  6599 | train Sqc: 0.5299 | loss: 1.2554\n",
      "epoch:    7 | step:  6699 | train Sqc: 0.5297 | loss: 1.2554\n",
      "epoch:    7 | step:  6799 | train Sqc: 0.5302 | loss: 1.2554\n",
      "epoch:    7 | step:  6899 | train Sqc: 0.5304 | loss: 1.2555\n",
      "epoch:    7 | step:  6999 | train Sqc: 0.5300 | loss: 1.2555\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | test Sqc: 0.3700 | test Neg: 0.4155\n",
      "epoch:    7 | step:  199 | N:  4 | test Sqc: 0.3757 | test Neg: 0.4142\n",
      "epoch:    7 | step:   99 | N:  6 | test Sqc: 0.4294 | test Neg: 0.3995\n",
      "epoch:    7 | step:  199 | N:  6 | test Sqc: 0.4087 | test Neg: 0.4048\n",
      "epoch:    7 | step:   99 | N:  8 | test Sqc: 0.4929 | test Neg: 0.3816\n",
      "epoch:    7 | step:  199 | N:  8 | test Sqc: 0.4906 | test Neg: 0.3820\n",
      "epoch:    7 | step:   99 | N:  10 | test Sqc: 0.4930 | test Neg: 0.3776\n",
      "epoch:    7 | step:  199 | N:  10 | test Sqc: 0.5060 | test Neg: 0.3713\n",
      "epoch:    7 | step:   99 | N:  12 | test Sqc: 0.5699 | test Neg: 0.3546\n",
      "epoch:    7 | step:  199 | N:  12 | test Sqc: 0.5683 | test Neg: 0.3533\n",
      "epoch:    7 | step:   99 | N:  14 | test Sqc: 0.6775 | test Neg: 0.3147\n",
      "epoch:    7 | step:  199 | N:  14 | test Sqc: 0.6870 | test Neg: 0.3109\n",
      "epoch:    7 | step:   99 | N:  16 | test Sqc: 0.7452 | test Neg: 0.2838\n",
      "epoch:    7 | step:  199 | N:  16 | test Sqc: 0.7468 | test Neg: 0.2838\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | train Sqc: 0.5416 | loss: 1.2580\n",
      "epoch:    8 | step:  199 | train Sqc: 0.5472 | loss: 1.2581\n",
      "epoch:    8 | step:  299 | train Sqc: 0.5378 | loss: 1.2567\n",
      "epoch:    8 | step:  399 | train Sqc: 0.5354 | loss: 1.2564\n",
      "epoch:    8 | step:  499 | train Sqc: 0.5410 | loss: 1.2567\n",
      "epoch:    8 | step:  599 | train Sqc: 0.5414 | loss: 1.2566\n",
      "epoch:    8 | step:  699 | train Sqc: 0.5396 | loss: 1.2564\n",
      "epoch:    8 | step:  799 | train Sqc: 0.5377 | loss: 1.2564\n",
      "epoch:    8 | step:  899 | train Sqc: 0.5388 | loss: 1.2564\n",
      "epoch:    8 | step:  999 | train Sqc: 0.5369 | loss: 1.2561\n",
      "epoch:    8 | step:  1099 | train Sqc: 0.5369 | loss: 1.2561\n",
      "epoch:    8 | step:  1199 | train Sqc: 0.5369 | loss: 1.2561\n",
      "epoch:    8 | step:  1299 | train Sqc: 0.5344 | loss: 1.2558\n",
      "epoch:    8 | step:  1399 | train Sqc: 0.5341 | loss: 1.2557\n",
      "epoch:    8 | step:  1499 | train Sqc: 0.5340 | loss: 1.2557\n",
      "epoch:    8 | step:  1599 | train Sqc: 0.5326 | loss: 1.2556\n",
      "epoch:    8 | step:  1699 | train Sqc: 0.5327 | loss: 1.2556\n",
      "epoch:    8 | step:  1799 | train Sqc: 0.5324 | loss: 1.2556\n",
      "epoch:    8 | step:  1899 | train Sqc: 0.5324 | loss: 1.2557\n",
      "epoch:    8 | step:  1999 | train Sqc: 0.5334 | loss: 1.2557\n",
      "epoch:    8 | step:  2099 | train Sqc: 0.5333 | loss: 1.2557\n",
      "epoch:    8 | step:  2199 | train Sqc: 0.5333 | loss: 1.2558\n",
      "epoch:    8 | step:  2299 | train Sqc: 0.5339 | loss: 1.2559\n",
      "epoch:    8 | step:  2399 | train Sqc: 0.5344 | loss: 1.2560\n",
      "epoch:    8 | step:  2499 | train Sqc: 0.5346 | loss: 1.2561\n",
      "epoch:    8 | step:  2599 | train Sqc: 0.5337 | loss: 1.2560\n",
      "epoch:    8 | step:  2699 | train Sqc: 0.5339 | loss: 1.2560\n",
      "epoch:    8 | step:  2799 | train Sqc: 0.5344 | loss: 1.2561\n",
      "epoch:    8 | step:  2899 | train Sqc: 0.5342 | loss: 1.2561\n",
      "epoch:    8 | step:  2999 | train Sqc: 0.5343 | loss: 1.2560\n",
      "epoch:    8 | step:  3099 | train Sqc: 0.5332 | loss: 1.2559\n",
      "epoch:    8 | step:  3199 | train Sqc: 0.5323 | loss: 1.2558\n",
      "epoch:    8 | step:  3299 | train Sqc: 0.5321 | loss: 1.2559\n",
      "epoch:    8 | step:  3399 | train Sqc: 0.5321 | loss: 1.2559\n",
      "epoch:    8 | step:  3499 | train Sqc: 0.5319 | loss: 1.2558\n",
      "epoch:    8 | step:  3599 | train Sqc: 0.5320 | loss: 1.2558\n",
      "epoch:    8 | step:  3699 | train Sqc: 0.5321 | loss: 1.2558\n",
      "epoch:    8 | step:  3799 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  3899 | train Sqc: 0.5310 | loss: 1.2557\n",
      "epoch:    8 | step:  3999 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  4099 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  4199 | train Sqc: 0.5311 | loss: 1.2558\n",
      "epoch:    8 | step:  4299 | train Sqc: 0.5310 | loss: 1.2557\n",
      "epoch:    8 | step:  4399 | train Sqc: 0.5311 | loss: 1.2557\n",
      "epoch:    8 | step:  4499 | train Sqc: 0.5314 | loss: 1.2558\n",
      "epoch:    8 | step:  4599 | train Sqc: 0.5310 | loss: 1.2557\n",
      "epoch:    8 | step:  4699 | train Sqc: 0.5305 | loss: 1.2557\n",
      "epoch:    8 | step:  4799 | train Sqc: 0.5314 | loss: 1.2558\n",
      "epoch:    8 | step:  4899 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  4999 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  5099 | train Sqc: 0.5316 | loss: 1.2558\n",
      "epoch:    8 | step:  5199 | train Sqc: 0.5311 | loss: 1.2557\n",
      "epoch:    8 | step:  5299 | train Sqc: 0.5310 | loss: 1.2557\n",
      "epoch:    8 | step:  5399 | train Sqc: 0.5309 | loss: 1.2557\n",
      "epoch:    8 | step:  5499 | train Sqc: 0.5313 | loss: 1.2558\n",
      "epoch:    8 | step:  5599 | train Sqc: 0.5309 | loss: 1.2557\n",
      "epoch:    8 | step:  5699 | train Sqc: 0.5309 | loss: 1.2557\n",
      "epoch:    8 | step:  5799 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  5899 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  5999 | train Sqc: 0.5311 | loss: 1.2557\n",
      "epoch:    8 | step:  6099 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  6199 | train Sqc: 0.5310 | loss: 1.2557\n",
      "epoch:    8 | step:  6299 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  6399 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  6499 | train Sqc: 0.5316 | loss: 1.2558\n",
      "epoch:    8 | step:  6599 | train Sqc: 0.5313 | loss: 1.2557\n",
      "epoch:    8 | step:  6699 | train Sqc: 0.5311 | loss: 1.2557\n",
      "epoch:    8 | step:  6799 | train Sqc: 0.5312 | loss: 1.2557\n",
      "epoch:    8 | step:  6899 | train Sqc: 0.5309 | loss: 1.2557\n",
      "epoch:    8 | step:  6999 | train Sqc: 0.5302 | loss: 1.2556\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | test Sqc: 0.3672 | test Neg: 0.4155\n",
      "epoch:    8 | step:  199 | N:  4 | test Sqc: 0.3745 | test Neg: 0.4142\n",
      "epoch:    8 | step:   99 | N:  6 | test Sqc: 0.4302 | test Neg: 0.3992\n",
      "epoch:    8 | step:  199 | N:  6 | test Sqc: 0.4112 | test Neg: 0.4046\n",
      "epoch:    8 | step:   99 | N:  8 | test Sqc: 0.4922 | test Neg: 0.3817\n",
      "epoch:    8 | step:  199 | N:  8 | test Sqc: 0.4876 | test Neg: 0.3822\n",
      "epoch:    8 | step:   99 | N:  10 | test Sqc: 0.4818 | test Neg: 0.3843\n",
      "epoch:    8 | step:  199 | N:  10 | test Sqc: 0.4924 | test Neg: 0.3795\n",
      "epoch:    8 | step:   99 | N:  12 | test Sqc: 0.5627 | test Neg: 0.3582\n",
      "epoch:    8 | step:  199 | N:  12 | test Sqc: 0.5603 | test Neg: 0.3570\n",
      "epoch:    8 | step:   99 | N:  14 | test Sqc: 0.6588 | test Neg: 0.3217\n",
      "epoch:    8 | step:  199 | N:  14 | test Sqc: 0.6671 | test Neg: 0.3191\n",
      "epoch:    8 | step:   99 | N:  16 | test Sqc: 0.7117 | test Neg: 0.3000\n",
      "epoch:    8 | step:  199 | N:  16 | test Sqc: 0.7146 | test Neg: 0.2986\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | train Sqc: 0.5370 | loss: 1.2566\n",
      "epoch:    9 | step:  199 | train Sqc: 0.5429 | loss: 1.2571\n",
      "epoch:    9 | step:  299 | train Sqc: 0.5343 | loss: 1.2559\n",
      "epoch:    9 | step:  399 | train Sqc: 0.5310 | loss: 1.2555\n",
      "epoch:    9 | step:  499 | train Sqc: 0.5379 | loss: 1.2560\n",
      "epoch:    9 | step:  599 | train Sqc: 0.5370 | loss: 1.2556\n",
      "epoch:    9 | step:  699 | train Sqc: 0.5350 | loss: 1.2554\n",
      "epoch:    9 | step:  799 | train Sqc: 0.5335 | loss: 1.2555\n",
      "epoch:    9 | step:  899 | train Sqc: 0.5345 | loss: 1.2555\n",
      "epoch:    9 | step:  999 | train Sqc: 0.5328 | loss: 1.2552\n",
      "epoch:    9 | step:  1099 | train Sqc: 0.5329 | loss: 1.2552\n",
      "epoch:    9 | step:  1199 | train Sqc: 0.5329 | loss: 1.2552\n",
      "epoch:    9 | step:  1299 | train Sqc: 0.5306 | loss: 1.2549\n",
      "epoch:    9 | step:  1399 | train Sqc: 0.5301 | loss: 1.2549\n",
      "epoch:    9 | step:  1499 | train Sqc: 0.5299 | loss: 1.2549\n",
      "epoch:    9 | step:  1599 | train Sqc: 0.5283 | loss: 1.2547\n",
      "epoch:    9 | step:  1699 | train Sqc: 0.5283 | loss: 1.2547\n",
      "epoch:    9 | step:  1799 | train Sqc: 0.5283 | loss: 1.2547\n",
      "epoch:    9 | step:  1899 | train Sqc: 0.5284 | loss: 1.2548\n",
      "epoch:    9 | step:  1999 | train Sqc: 0.5293 | loss: 1.2549\n",
      "epoch:    9 | step:  2099 | train Sqc: 0.5291 | loss: 1.2549\n",
      "epoch:    9 | step:  2199 | train Sqc: 0.5294 | loss: 1.2550\n",
      "epoch:    9 | step:  2299 | train Sqc: 0.5299 | loss: 1.2551\n",
      "epoch:    9 | step:  2399 | train Sqc: 0.5306 | loss: 1.2553\n",
      "epoch:    9 | step:  2499 | train Sqc: 0.5310 | loss: 1.2553\n",
      "epoch:    9 | step:  2599 | train Sqc: 0.5302 | loss: 1.2553\n",
      "epoch:    9 | step:  2699 | train Sqc: 0.5305 | loss: 1.2553\n",
      "epoch:    9 | step:  2799 | train Sqc: 0.5309 | loss: 1.2553\n",
      "epoch:    9 | step:  2899 | train Sqc: 0.5307 | loss: 1.2554\n",
      "epoch:    9 | step:  2999 | train Sqc: 0.5307 | loss: 1.2553\n",
      "epoch:    9 | step:  3099 | train Sqc: 0.5296 | loss: 1.2553\n",
      "epoch:    9 | step:  3199 | train Sqc: 0.5288 | loss: 1.2552\n",
      "epoch:    9 | step:  3299 | train Sqc: 0.5288 | loss: 1.2552\n",
      "epoch:    9 | step:  3399 | train Sqc: 0.5287 | loss: 1.2552\n",
      "epoch:    9 | step:  3499 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:    9 | step:  3599 | train Sqc: 0.5286 | loss: 1.2551\n",
      "epoch:    9 | step:  3699 | train Sqc: 0.5287 | loss: 1.2551\n",
      "epoch:    9 | step:  3799 | train Sqc: 0.5280 | loss: 1.2550\n",
      "epoch:    9 | step:  3899 | train Sqc: 0.5278 | loss: 1.2550\n",
      "epoch:    9 | step:  3999 | train Sqc: 0.5280 | loss: 1.2551\n",
      "epoch:    9 | step:  4099 | train Sqc: 0.5281 | loss: 1.2551\n",
      "epoch:    9 | step:  4199 | train Sqc: 0.5279 | loss: 1.2551\n",
      "epoch:    9 | step:  4299 | train Sqc: 0.5279 | loss: 1.2551\n",
      "epoch:    9 | step:  4399 | train Sqc: 0.5280 | loss: 1.2551\n",
      "epoch:    9 | step:  4499 | train Sqc: 0.5283 | loss: 1.2551\n",
      "epoch:    9 | step:  4599 | train Sqc: 0.5281 | loss: 1.2551\n",
      "epoch:    9 | step:  4699 | train Sqc: 0.5276 | loss: 1.2551\n",
      "epoch:    9 | step:  4799 | train Sqc: 0.5285 | loss: 1.2552\n",
      "epoch:    9 | step:  4899 | train Sqc: 0.5283 | loss: 1.2551\n",
      "epoch:    9 | step:  4999 | train Sqc: 0.5282 | loss: 1.2551\n",
      "epoch:    9 | step:  5099 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:    9 | step:  5199 | train Sqc: 0.5282 | loss: 1.2551\n",
      "epoch:    9 | step:  5299 | train Sqc: 0.5280 | loss: 1.2551\n",
      "epoch:    9 | step:  5399 | train Sqc: 0.5279 | loss: 1.2551\n",
      "epoch:    9 | step:  5499 | train Sqc: 0.5284 | loss: 1.2552\n",
      "epoch:    9 | step:  5599 | train Sqc: 0.5281 | loss: 1.2552\n",
      "epoch:    9 | step:  5699 | train Sqc: 0.5281 | loss: 1.2552\n",
      "epoch:    9 | step:  5799 | train Sqc: 0.5285 | loss: 1.2552\n",
      "epoch:    9 | step:  5899 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:    9 | step:  5999 | train Sqc: 0.5284 | loss: 1.2552\n",
      "epoch:    9 | step:  6099 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:    9 | step:  6199 | train Sqc: 0.5283 | loss: 1.2552\n",
      "epoch:    9 | step:  6299 | train Sqc: 0.5285 | loss: 1.2552\n",
      "epoch:    9 | step:  6399 | train Sqc: 0.5285 | loss: 1.2552\n",
      "epoch:    9 | step:  6499 | train Sqc: 0.5289 | loss: 1.2553\n",
      "epoch:    9 | step:  6599 | train Sqc: 0.5287 | loss: 1.2552\n",
      "epoch:    9 | step:  6699 | train Sqc: 0.5284 | loss: 1.2552\n",
      "epoch:    9 | step:  6799 | train Sqc: 0.5285 | loss: 1.2552\n",
      "epoch:    9 | step:  6899 | train Sqc: 0.5281 | loss: 1.2552\n",
      "epoch:    9 | step:  6999 | train Sqc: 0.5275 | loss: 1.2551\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | test Sqc: 0.3669 | test Neg: 0.4155\n",
      "epoch:    9 | step:  199 | N:  4 | test Sqc: 0.3731 | test Neg: 0.4142\n",
      "epoch:    9 | step:   99 | N:  6 | test Sqc: 0.4306 | test Neg: 0.3994\n",
      "epoch:    9 | step:  199 | N:  6 | test Sqc: 0.4103 | test Neg: 0.4047\n",
      "epoch:    9 | step:   99 | N:  8 | test Sqc: 0.4922 | test Neg: 0.3817\n",
      "epoch:    9 | step:  199 | N:  8 | test Sqc: 0.4890 | test Neg: 0.3823\n",
      "epoch:    9 | step:   99 | N:  10 | test Sqc: 0.4817 | test Neg: 0.3844\n",
      "epoch:    9 | step:  199 | N:  10 | test Sqc: 0.4925 | test Neg: 0.3795\n",
      "epoch:    9 | step:   99 | N:  12 | test Sqc: 0.5612 | test Neg: 0.3582\n",
      "epoch:    9 | step:  199 | N:  12 | test Sqc: 0.5599 | test Neg: 0.3569\n",
      "epoch:    9 | step:   99 | N:  14 | test Sqc: 0.6575 | test Neg: 0.3207\n",
      "epoch:    9 | step:  199 | N:  14 | test Sqc: 0.6657 | test Neg: 0.3184\n",
      "epoch:    9 | step:   99 | N:  16 | test Sqc: 0.7167 | test Neg: 0.2981\n",
      "epoch:    9 | step:  199 | N:  16 | test Sqc: 0.7168 | test Neg: 0.2977\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   10 | step:   99 | train Sqc: 0.5348 | loss: 1.2563\n",
      "epoch:   10 | step:  199 | train Sqc: 0.5401 | loss: 1.2569\n",
      "epoch:   10 | step:  299 | train Sqc: 0.5314 | loss: 1.2556\n",
      "epoch:   10 | step:  399 | train Sqc: 0.5286 | loss: 1.2552\n",
      "epoch:   10 | step:  499 | train Sqc: 0.5345 | loss: 1.2556\n",
      "epoch:   10 | step:  599 | train Sqc: 0.5335 | loss: 1.2553\n",
      "epoch:   10 | step:  699 | train Sqc: 0.5318 | loss: 1.2551\n",
      "epoch:   10 | step:  799 | train Sqc: 0.5303 | loss: 1.2551\n",
      "epoch:   10 | step:  899 | train Sqc: 0.5314 | loss: 1.2552\n",
      "epoch:   10 | step:  999 | train Sqc: 0.5302 | loss: 1.2550\n",
      "epoch:   10 | step:  1099 | train Sqc: 0.5305 | loss: 1.2550\n",
      "epoch:   10 | step:  1199 | train Sqc: 0.5309 | loss: 1.2550\n",
      "epoch:   10 | step:  1299 | train Sqc: 0.5289 | loss: 1.2547\n",
      "epoch:   10 | step:  1399 | train Sqc: 0.5287 | loss: 1.2547\n",
      "epoch:   10 | step:  1499 | train Sqc: 0.5285 | loss: 1.2547\n",
      "epoch:   10 | step:  1599 | train Sqc: 0.5270 | loss: 1.2546\n",
      "epoch:   10 | step:  1699 | train Sqc: 0.5271 | loss: 1.2545\n",
      "epoch:   10 | step:  1799 | train Sqc: 0.5269 | loss: 1.2545\n",
      "epoch:   10 | step:  1899 | train Sqc: 0.5269 | loss: 1.2546\n",
      "epoch:   10 | step:  1999 | train Sqc: 0.5278 | loss: 1.2546\n",
      "epoch:   10 | step:  2099 | train Sqc: 0.5275 | loss: 1.2547\n",
      "epoch:   10 | step:  2199 | train Sqc: 0.5278 | loss: 1.2547\n",
      "epoch:   10 | step:  2299 | train Sqc: 0.5283 | loss: 1.2549\n",
      "epoch:   10 | step:  2399 | train Sqc: 0.5289 | loss: 1.2550\n",
      "epoch:   10 | step:  2499 | train Sqc: 0.5293 | loss: 1.2551\n",
      "epoch:   10 | step:  2599 | train Sqc: 0.5285 | loss: 1.2550\n",
      "epoch:   10 | step:  2699 | train Sqc: 0.5287 | loss: 1.2550\n",
      "epoch:   10 | step:  2799 | train Sqc: 0.5291 | loss: 1.2551\n",
      "epoch:   10 | step:  2899 | train Sqc: 0.5290 | loss: 1.2551\n",
      "epoch:   10 | step:  2999 | train Sqc: 0.5291 | loss: 1.2551\n",
      "epoch:   10 | step:  3099 | train Sqc: 0.5280 | loss: 1.2550\n",
      "epoch:   10 | step:  3199 | train Sqc: 0.5270 | loss: 1.2549\n",
      "epoch:   10 | step:  3299 | train Sqc: 0.5270 | loss: 1.2549\n",
      "epoch:   10 | step:  3399 | train Sqc: 0.5269 | loss: 1.2549\n",
      "epoch:   10 | step:  3499 | train Sqc: 0.5268 | loss: 1.2549\n",
      "epoch:   10 | step:  3599 | train Sqc: 0.5268 | loss: 1.2549\n",
      "epoch:   10 | step:  3699 | train Sqc: 0.5269 | loss: 1.2549\n",
      "epoch:   10 | step:  3799 | train Sqc: 0.5262 | loss: 1.2548\n",
      "epoch:   10 | step:  3899 | train Sqc: 0.5259 | loss: 1.2548\n",
      "epoch:   10 | step:  3999 | train Sqc: 0.5261 | loss: 1.2548\n",
      "epoch:   10 | step:  4099 | train Sqc: 0.5266 | loss: 1.2549\n",
      "epoch:   10 | step:  4199 | train Sqc: 0.5266 | loss: 1.2549\n",
      "epoch:   10 | step:  4299 | train Sqc: 0.5266 | loss: 1.2549\n",
      "epoch:   10 | step:  4399 | train Sqc: 0.5267 | loss: 1.2549\n",
      "epoch:   10 | step:  4499 | train Sqc: 0.5270 | loss: 1.2549\n",
      "epoch:   10 | step:  4599 | train Sqc: 0.5266 | loss: 1.2549\n",
      "epoch:   10 | step:  4699 | train Sqc: 0.5262 | loss: 1.2549\n",
      "epoch:   10 | step:  4799 | train Sqc: 0.5271 | loss: 1.2550\n",
      "epoch:   10 | step:  4899 | train Sqc: 0.5269 | loss: 1.2549\n",
      "epoch:   10 | step:  4999 | train Sqc: 0.5268 | loss: 1.2549\n",
      "epoch:   10 | step:  5099 | train Sqc: 0.5272 | loss: 1.2550\n",
      "epoch:   10 | step:  5199 | train Sqc: 0.5269 | loss: 1.2549\n",
      "epoch:   10 | step:  5299 | train Sqc: 0.5267 | loss: 1.2549\n",
      "epoch:   10 | step:  5399 | train Sqc: 0.5267 | loss: 1.2550\n",
      "epoch:   10 | step:  5499 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   10 | step:  5599 | train Sqc: 0.5267 | loss: 1.2550\n",
      "epoch:   10 | step:  5699 | train Sqc: 0.5267 | loss: 1.2550\n",
      "epoch:   10 | step:  5799 | train Sqc: 0.5271 | loss: 1.2550\n",
      "epoch:   10 | step:  5899 | train Sqc: 0.5272 | loss: 1.2550\n",
      "epoch:   10 | step:  5999 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   10 | step:  6099 | train Sqc: 0.5272 | loss: 1.2550\n",
      "epoch:   10 | step:  6199 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   10 | step:  6299 | train Sqc: 0.5271 | loss: 1.2550\n",
      "epoch:   10 | step:  6399 | train Sqc: 0.5272 | loss: 1.2550\n",
      "epoch:   10 | step:  6499 | train Sqc: 0.5276 | loss: 1.2551\n",
      "epoch:   10 | step:  6599 | train Sqc: 0.5274 | loss: 1.2550\n",
      "epoch:   10 | step:  6699 | train Sqc: 0.5271 | loss: 1.2550\n",
      "epoch:   10 | step:  6799 | train Sqc: 0.5273 | loss: 1.2550\n",
      "epoch:   10 | step:  6899 | train Sqc: 0.5269 | loss: 1.2550\n",
      "epoch:   10 | step:  6999 | train Sqc: 0.5264 | loss: 1.2550\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   10 | step:   99 | N:  4 | test Sqc: 0.3672 | test Neg: 0.4155\n",
      "epoch:   10 | step:  199 | N:  4 | test Sqc: 0.3730 | test Neg: 0.4143\n",
      "epoch:   10 | step:   99 | N:  6 | test Sqc: 0.4315 | test Neg: 0.3994\n",
      "epoch:   10 | step:  199 | N:  6 | test Sqc: 0.4089 | test Neg: 0.4048\n",
      "epoch:   10 | step:   99 | N:  8 | test Sqc: 0.4883 | test Neg: 0.3818\n",
      "epoch:   10 | step:  199 | N:  8 | test Sqc: 0.4858 | test Neg: 0.3822\n",
      "epoch:   10 | step:   99 | N:  10 | test Sqc: 0.4806 | test Neg: 0.3843\n",
      "epoch:   10 | step:  199 | N:  10 | test Sqc: 0.4920 | test Neg: 0.3795\n",
      "epoch:   10 | step:   99 | N:  12 | test Sqc: 0.5600 | test Neg: 0.3574\n",
      "epoch:   10 | step:  199 | N:  12 | test Sqc: 0.5608 | test Neg: 0.3553\n",
      "epoch:   10 | step:   99 | N:  14 | test Sqc: 0.6555 | test Neg: 0.3212\n",
      "epoch:   10 | step:  199 | N:  14 | test Sqc: 0.6641 | test Neg: 0.3191\n",
      "epoch:   10 | step:   99 | N:  16 | test Sqc: 0.7145 | test Neg: 0.2998\n",
      "epoch:   10 | step:  199 | N:  16 | test Sqc: 0.7135 | test Neg: 0.2994\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   11 | step:   99 | train Sqc: 0.5331 | loss: 1.2563\n",
      "epoch:   11 | step:  199 | train Sqc: 0.5401 | loss: 1.2570\n",
      "epoch:   11 | step:  299 | train Sqc: 0.5303 | loss: 1.2556\n",
      "epoch:   11 | step:  399 | train Sqc: 0.5272 | loss: 1.2552\n",
      "epoch:   11 | step:  499 | train Sqc: 0.5330 | loss: 1.2555\n",
      "epoch:   11 | step:  599 | train Sqc: 0.5318 | loss: 1.2551\n",
      "epoch:   11 | step:  699 | train Sqc: 0.5297 | loss: 1.2548\n",
      "epoch:   11 | step:  799 | train Sqc: 0.5285 | loss: 1.2549\n",
      "epoch:   11 | step:  899 | train Sqc: 0.5298 | loss: 1.2550\n",
      "epoch:   11 | step:  999 | train Sqc: 0.5281 | loss: 1.2547\n",
      "epoch:   11 | step:  1099 | train Sqc: 0.5283 | loss: 1.2547\n",
      "epoch:   11 | step:  1199 | train Sqc: 0.5285 | loss: 1.2547\n",
      "epoch:   11 | step:  1299 | train Sqc: 0.5265 | loss: 1.2545\n",
      "epoch:   11 | step:  1399 | train Sqc: 0.5261 | loss: 1.2544\n",
      "epoch:   11 | step:  1499 | train Sqc: 0.5264 | loss: 1.2544\n",
      "epoch:   11 | step:  1599 | train Sqc: 0.5251 | loss: 1.2543\n",
      "epoch:   11 | step:  1699 | train Sqc: 0.5255 | loss: 1.2543\n",
      "epoch:   11 | step:  1799 | train Sqc: 0.5255 | loss: 1.2543\n",
      "epoch:   11 | step:  1899 | train Sqc: 0.5256 | loss: 1.2544\n",
      "epoch:   11 | step:  1999 | train Sqc: 0.5265 | loss: 1.2545\n",
      "epoch:   11 | step:  2099 | train Sqc: 0.5263 | loss: 1.2545\n",
      "epoch:   11 | step:  2199 | train Sqc: 0.5266 | loss: 1.2546\n",
      "epoch:   11 | step:  2299 | train Sqc: 0.5271 | loss: 1.2547\n",
      "epoch:   11 | step:  2399 | train Sqc: 0.5277 | loss: 1.2549\n",
      "epoch:   11 | step:  2499 | train Sqc: 0.5278 | loss: 1.2549\n",
      "epoch:   11 | step:  2599 | train Sqc: 0.5269 | loss: 1.2548\n",
      "epoch:   11 | step:  2699 | train Sqc: 0.5271 | loss: 1.2549\n",
      "epoch:   11 | step:  2799 | train Sqc: 0.5275 | loss: 1.2549\n",
      "epoch:   11 | step:  2899 | train Sqc: 0.5274 | loss: 1.2549\n",
      "epoch:   11 | step:  2999 | train Sqc: 0.5276 | loss: 1.2549\n",
      "epoch:   11 | step:  3099 | train Sqc: 0.5267 | loss: 1.2549\n",
      "epoch:   11 | step:  3199 | train Sqc: 0.5258 | loss: 1.2547\n",
      "epoch:   11 | step:  3299 | train Sqc: 0.5257 | loss: 1.2548\n",
      "epoch:   11 | step:  3399 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   11 | step:  3499 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   11 | step:  3599 | train Sqc: 0.5257 | loss: 1.2547\n",
      "epoch:   11 | step:  3699 | train Sqc: 0.5257 | loss: 1.2547\n",
      "epoch:   11 | step:  3799 | train Sqc: 0.5250 | loss: 1.2546\n",
      "epoch:   11 | step:  3899 | train Sqc: 0.5248 | loss: 1.2546\n",
      "epoch:   11 | step:  3999 | train Sqc: 0.5251 | loss: 1.2547\n",
      "epoch:   11 | step:  4099 | train Sqc: 0.5252 | loss: 1.2547\n",
      "epoch:   11 | step:  4199 | train Sqc: 0.5250 | loss: 1.2547\n",
      "epoch:   11 | step:  4299 | train Sqc: 0.5254 | loss: 1.2548\n",
      "epoch:   11 | step:  4399 | train Sqc: 0.5263 | loss: 1.2549\n",
      "epoch:   11 | step:  4499 | train Sqc: 0.5274 | loss: 1.2551\n",
      "epoch:   11 | step:  4599 | train Sqc: 0.5272 | loss: 1.2551\n",
      "epoch:   11 | step:  4699 | train Sqc: 0.5267 | loss: 1.2550\n",
      "epoch:   11 | step:  4799 | train Sqc: 0.5275 | loss: 1.2551\n",
      "epoch:   11 | step:  4899 | train Sqc: 0.5273 | loss: 1.2550\n",
      "epoch:   11 | step:  4999 | train Sqc: 0.5271 | loss: 1.2550\n",
      "epoch:   11 | step:  5099 | train Sqc: 0.5275 | loss: 1.2551\n",
      "epoch:   11 | step:  5199 | train Sqc: 0.5271 | loss: 1.2551\n",
      "epoch:   11 | step:  5299 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   11 | step:  5399 | train Sqc: 0.5269 | loss: 1.2551\n",
      "epoch:   11 | step:  5499 | train Sqc: 0.5272 | loss: 1.2551\n",
      "epoch:   11 | step:  5599 | train Sqc: 0.5268 | loss: 1.2550\n",
      "epoch:   11 | step:  5699 | train Sqc: 0.5268 | loss: 1.2550\n",
      "epoch:   11 | step:  5799 | train Sqc: 0.5271 | loss: 1.2551\n",
      "epoch:   11 | step:  5899 | train Sqc: 0.5272 | loss: 1.2551\n",
      "epoch:   11 | step:  5999 | train Sqc: 0.5270 | loss: 1.2551\n",
      "epoch:   11 | step:  6099 | train Sqc: 0.5271 | loss: 1.2551\n",
      "epoch:   11 | step:  6199 | train Sqc: 0.5268 | loss: 1.2551\n",
      "epoch:   11 | step:  6299 | train Sqc: 0.5269 | loss: 1.2550\n",
      "epoch:   11 | step:  6399 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   11 | step:  6499 | train Sqc: 0.5273 | loss: 1.2551\n",
      "epoch:   11 | step:  6599 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   11 | step:  6699 | train Sqc: 0.5268 | loss: 1.2550\n",
      "epoch:   11 | step:  6799 | train Sqc: 0.5269 | loss: 1.2550\n",
      "epoch:   11 | step:  6899 | train Sqc: 0.5264 | loss: 1.2550\n",
      "epoch:   11 | step:  6999 | train Sqc: 0.5258 | loss: 1.2549\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   11 | step:   99 | N:  4 | test Sqc: 0.3668 | test Neg: 0.4156\n",
      "epoch:   11 | step:  199 | N:  4 | test Sqc: 0.3731 | test Neg: 0.4143\n",
      "epoch:   11 | step:   99 | N:  6 | test Sqc: 0.4306 | test Neg: 0.3994\n",
      "epoch:   11 | step:  199 | N:  6 | test Sqc: 0.4097 | test Neg: 0.4047\n",
      "epoch:   11 | step:   99 | N:  8 | test Sqc: 0.4894 | test Neg: 0.3819\n",
      "epoch:   11 | step:  199 | N:  8 | test Sqc: 0.4850 | test Neg: 0.3822\n",
      "epoch:   11 | step:   99 | N:  10 | test Sqc: 0.4786 | test Neg: 0.3845\n",
      "epoch:   11 | step:  199 | N:  10 | test Sqc: 0.4902 | test Neg: 0.3797\n",
      "epoch:   11 | step:   99 | N:  12 | test Sqc: 0.5612 | test Neg: 0.3582\n",
      "epoch:   11 | step:  199 | N:  12 | test Sqc: 0.5595 | test Neg: 0.3571\n",
      "epoch:   11 | step:   99 | N:  14 | test Sqc: 0.6538 | test Neg: 0.3242\n",
      "epoch:   11 | step:  199 | N:  14 | test Sqc: 0.6610 | test Neg: 0.3222\n",
      "epoch:   11 | step:   99 | N:  16 | test Sqc: 0.7105 | test Neg: 0.3035\n",
      "epoch:   11 | step:  199 | N:  16 | test Sqc: 0.7102 | test Neg: 0.3027\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   12 | step:   99 | train Sqc: 0.5321 | loss: 1.2559\n",
      "epoch:   12 | step:  199 | train Sqc: 0.5379 | loss: 1.2566\n",
      "epoch:   12 | step:  299 | train Sqc: 0.5293 | loss: 1.2554\n",
      "epoch:   12 | step:  399 | train Sqc: 0.5265 | loss: 1.2550\n",
      "epoch:   12 | step:  499 | train Sqc: 0.5320 | loss: 1.2553\n",
      "epoch:   12 | step:  599 | train Sqc: 0.5310 | loss: 1.2550\n",
      "epoch:   12 | step:  699 | train Sqc: 0.5296 | loss: 1.2548\n",
      "epoch:   12 | step:  799 | train Sqc: 0.5285 | loss: 1.2548\n",
      "epoch:   12 | step:  899 | train Sqc: 0.5299 | loss: 1.2549\n",
      "epoch:   12 | step:  999 | train Sqc: 0.5287 | loss: 1.2547\n",
      "epoch:   12 | step:  1099 | train Sqc: 0.5295 | loss: 1.2548\n",
      "epoch:   12 | step:  1199 | train Sqc: 0.5300 | loss: 1.2549\n",
      "epoch:   12 | step:  1299 | train Sqc: 0.5280 | loss: 1.2546\n",
      "epoch:   12 | step:  1399 | train Sqc: 0.5276 | loss: 1.2546\n",
      "epoch:   12 | step:  1499 | train Sqc: 0.5274 | loss: 1.2545\n",
      "epoch:   12 | step:  1599 | train Sqc: 0.5260 | loss: 1.2544\n",
      "epoch:   12 | step:  1699 | train Sqc: 0.5260 | loss: 1.2543\n",
      "epoch:   12 | step:  1799 | train Sqc: 0.5258 | loss: 1.2543\n",
      "epoch:   12 | step:  1899 | train Sqc: 0.5259 | loss: 1.2544\n",
      "epoch:   12 | step:  1999 | train Sqc: 0.5269 | loss: 1.2545\n",
      "epoch:   12 | step:  2099 | train Sqc: 0.5265 | loss: 1.2545\n",
      "epoch:   12 | step:  2199 | train Sqc: 0.5268 | loss: 1.2546\n",
      "epoch:   12 | step:  2299 | train Sqc: 0.5272 | loss: 1.2547\n",
      "epoch:   12 | step:  2399 | train Sqc: 0.5278 | loss: 1.2548\n",
      "epoch:   12 | step:  2499 | train Sqc: 0.5279 | loss: 1.2549\n",
      "epoch:   12 | step:  2599 | train Sqc: 0.5270 | loss: 1.2548\n",
      "epoch:   12 | step:  2699 | train Sqc: 0.5272 | loss: 1.2548\n",
      "epoch:   12 | step:  2799 | train Sqc: 0.5276 | loss: 1.2549\n",
      "epoch:   12 | step:  2899 | train Sqc: 0.5275 | loss: 1.2549\n",
      "epoch:   12 | step:  2999 | train Sqc: 0.5275 | loss: 1.2549\n",
      "epoch:   12 | step:  3099 | train Sqc: 0.5265 | loss: 1.2548\n",
      "epoch:   12 | step:  3199 | train Sqc: 0.5255 | loss: 1.2546\n",
      "epoch:   12 | step:  3299 | train Sqc: 0.5255 | loss: 1.2547\n",
      "epoch:   12 | step:  3399 | train Sqc: 0.5254 | loss: 1.2547\n",
      "epoch:   12 | step:  3499 | train Sqc: 0.5255 | loss: 1.2547\n",
      "epoch:   12 | step:  3599 | train Sqc: 0.5256 | loss: 1.2547\n",
      "epoch:   12 | step:  3699 | train Sqc: 0.5257 | loss: 1.2547\n",
      "epoch:   12 | step:  3799 | train Sqc: 0.5249 | loss: 1.2546\n",
      "epoch:   12 | step:  3899 | train Sqc: 0.5247 | loss: 1.2546\n",
      "epoch:   12 | step:  3999 | train Sqc: 0.5250 | loss: 1.2546\n",
      "epoch:   12 | step:  4099 | train Sqc: 0.5251 | loss: 1.2546\n",
      "epoch:   12 | step:  4199 | train Sqc: 0.5249 | loss: 1.2547\n",
      "epoch:   12 | step:  4299 | train Sqc: 0.5249 | loss: 1.2547\n",
      "epoch:   12 | step:  4399 | train Sqc: 0.5250 | loss: 1.2547\n",
      "epoch:   12 | step:  4499 | train Sqc: 0.5253 | loss: 1.2547\n",
      "epoch:   12 | step:  4599 | train Sqc: 0.5250 | loss: 1.2547\n",
      "epoch:   12 | step:  4699 | train Sqc: 0.5245 | loss: 1.2546\n",
      "epoch:   12 | step:  4799 | train Sqc: 0.5255 | loss: 1.2547\n",
      "epoch:   12 | step:  4899 | train Sqc: 0.5253 | loss: 1.2547\n",
      "epoch:   12 | step:  4999 | train Sqc: 0.5251 | loss: 1.2547\n",
      "epoch:   12 | step:  5099 | train Sqc: 0.5256 | loss: 1.2547\n",
      "epoch:   12 | step:  5199 | train Sqc: 0.5252 | loss: 1.2547\n",
      "epoch:   12 | step:  5299 | train Sqc: 0.5251 | loss: 1.2547\n",
      "epoch:   12 | step:  5399 | train Sqc: 0.5250 | loss: 1.2547\n",
      "epoch:   12 | step:  5499 | train Sqc: 0.5254 | loss: 1.2548\n",
      "epoch:   12 | step:  5599 | train Sqc: 0.5250 | loss: 1.2547\n",
      "epoch:   12 | step:  5699 | train Sqc: 0.5251 | loss: 1.2548\n",
      "epoch:   12 | step:  5799 | train Sqc: 0.5255 | loss: 1.2548\n",
      "epoch:   12 | step:  5899 | train Sqc: 0.5255 | loss: 1.2548\n",
      "epoch:   12 | step:  5999 | train Sqc: 0.5254 | loss: 1.2548\n",
      "epoch:   12 | step:  6099 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   12 | step:  6199 | train Sqc: 0.5254 | loss: 1.2548\n",
      "epoch:   12 | step:  6299 | train Sqc: 0.5255 | loss: 1.2548\n",
      "epoch:   12 | step:  6399 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   12 | step:  6499 | train Sqc: 0.5259 | loss: 1.2548\n",
      "epoch:   12 | step:  6599 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   12 | step:  6699 | train Sqc: 0.5254 | loss: 1.2548\n",
      "epoch:   12 | step:  6799 | train Sqc: 0.5255 | loss: 1.2548\n",
      "epoch:   12 | step:  6899 | train Sqc: 0.5250 | loss: 1.2548\n",
      "epoch:   12 | step:  6999 | train Sqc: 0.5244 | loss: 1.2547\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   12 | step:   99 | N:  4 | test Sqc: 0.3674 | test Neg: 0.4156\n",
      "epoch:   12 | step:  199 | N:  4 | test Sqc: 0.3729 | test Neg: 0.4143\n",
      "epoch:   12 | step:   99 | N:  6 | test Sqc: 0.4299 | test Neg: 0.3996\n",
      "epoch:   12 | step:  199 | N:  6 | test Sqc: 0.4099 | test Neg: 0.4049\n",
      "epoch:   12 | step:   99 | N:  8 | test Sqc: 0.4890 | test Neg: 0.3820\n",
      "epoch:   12 | step:  199 | N:  8 | test Sqc: 0.4853 | test Neg: 0.3823\n",
      "epoch:   12 | step:   99 | N:  10 | test Sqc: 0.4821 | test Neg: 0.3811\n",
      "epoch:   12 | step:  199 | N:  10 | test Sqc: 0.4938 | test Neg: 0.3763\n",
      "epoch:   12 | step:   99 | N:  12 | test Sqc: 0.5610 | test Neg: 0.3580\n",
      "epoch:   12 | step:  199 | N:  12 | test Sqc: 0.5608 | test Neg: 0.3572\n",
      "epoch:   12 | step:   99 | N:  14 | test Sqc: 0.6582 | test Neg: 0.3215\n",
      "epoch:   12 | step:  199 | N:  14 | test Sqc: 0.6662 | test Neg: 0.3194\n",
      "epoch:   12 | step:   99 | N:  16 | test Sqc: 0.7046 | test Neg: 0.3043\n",
      "epoch:   12 | step:  199 | N:  16 | test Sqc: 0.7066 | test Neg: 0.3040\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   13 | step:   99 | train Sqc: 0.5323 | loss: 1.2562\n",
      "epoch:   13 | step:  199 | train Sqc: 0.5376 | loss: 1.2567\n",
      "epoch:   13 | step:  299 | train Sqc: 0.5282 | loss: 1.2553\n",
      "epoch:   13 | step:  399 | train Sqc: 0.5250 | loss: 1.2549\n",
      "epoch:   13 | step:  499 | train Sqc: 0.5312 | loss: 1.2553\n",
      "epoch:   13 | step:  599 | train Sqc: 0.5296 | loss: 1.2549\n",
      "epoch:   13 | step:  699 | train Sqc: 0.5279 | loss: 1.2546\n",
      "epoch:   13 | step:  799 | train Sqc: 0.5266 | loss: 1.2547\n",
      "epoch:   13 | step:  899 | train Sqc: 0.5279 | loss: 1.2548\n",
      "epoch:   13 | step:  999 | train Sqc: 0.5263 | loss: 1.2545\n",
      "epoch:   13 | step:  1099 | train Sqc: 0.5267 | loss: 1.2545\n",
      "epoch:   13 | step:  1199 | train Sqc: 0.5272 | loss: 1.2545\n",
      "epoch:   13 | step:  1299 | train Sqc: 0.5253 | loss: 1.2543\n",
      "epoch:   13 | step:  1399 | train Sqc: 0.5249 | loss: 1.2542\n",
      "epoch:   13 | step:  1499 | train Sqc: 0.5248 | loss: 1.2542\n",
      "epoch:   13 | step:  1599 | train Sqc: 0.5233 | loss: 1.2540\n",
      "epoch:   13 | step:  1699 | train Sqc: 0.5236 | loss: 1.2541\n",
      "epoch:   13 | step:  1799 | train Sqc: 0.5237 | loss: 1.2541\n",
      "epoch:   13 | step:  1899 | train Sqc: 0.5238 | loss: 1.2542\n",
      "epoch:   13 | step:  1999 | train Sqc: 0.5249 | loss: 1.2543\n",
      "epoch:   13 | step:  2099 | train Sqc: 0.5247 | loss: 1.2543\n",
      "epoch:   13 | step:  2199 | train Sqc: 0.5249 | loss: 1.2543\n",
      "epoch:   13 | step:  2299 | train Sqc: 0.5254 | loss: 1.2545\n",
      "epoch:   13 | step:  2399 | train Sqc: 0.5261 | loss: 1.2546\n",
      "epoch:   13 | step:  2499 | train Sqc: 0.5263 | loss: 1.2547\n",
      "epoch:   13 | step:  2599 | train Sqc: 0.5254 | loss: 1.2546\n",
      "epoch:   13 | step:  2699 | train Sqc: 0.5256 | loss: 1.2547\n",
      "epoch:   13 | step:  2799 | train Sqc: 0.5261 | loss: 1.2547\n",
      "epoch:   13 | step:  2899 | train Sqc: 0.5260 | loss: 1.2547\n",
      "epoch:   13 | step:  2999 | train Sqc: 0.5261 | loss: 1.2547\n",
      "epoch:   13 | step:  3099 | train Sqc: 0.5252 | loss: 1.2546\n",
      "epoch:   13 | step:  3199 | train Sqc: 0.5243 | loss: 1.2545\n",
      "epoch:   13 | step:  3299 | train Sqc: 0.5241 | loss: 1.2545\n",
      "epoch:   13 | step:  3399 | train Sqc: 0.5241 | loss: 1.2545\n",
      "epoch:   13 | step:  3499 | train Sqc: 0.5241 | loss: 1.2545\n",
      "epoch:   13 | step:  3599 | train Sqc: 0.5241 | loss: 1.2545\n",
      "epoch:   13 | step:  3699 | train Sqc: 0.5243 | loss: 1.2545\n",
      "epoch:   13 | step:  3799 | train Sqc: 0.5235 | loss: 1.2544\n",
      "epoch:   13 | step:  3899 | train Sqc: 0.5233 | loss: 1.2544\n",
      "epoch:   13 | step:  3999 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   13 | step:  4099 | train Sqc: 0.5236 | loss: 1.2544\n",
      "epoch:   13 | step:  4199 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   13 | step:  4299 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   13 | step:  4399 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   13 | step:  4499 | train Sqc: 0.5238 | loss: 1.2545\n",
      "epoch:   13 | step:  4599 | train Sqc: 0.5236 | loss: 1.2545\n",
      "epoch:   13 | step:  4699 | train Sqc: 0.5232 | loss: 1.2544\n",
      "epoch:   13 | step:  4799 | train Sqc: 0.5241 | loss: 1.2545\n",
      "epoch:   13 | step:  4899 | train Sqc: 0.5240 | loss: 1.2545\n",
      "epoch:   13 | step:  4999 | train Sqc: 0.5239 | loss: 1.2545\n",
      "epoch:   13 | step:  5099 | train Sqc: 0.5243 | loss: 1.2546\n",
      "epoch:   13 | step:  5199 | train Sqc: 0.5239 | loss: 1.2545\n",
      "epoch:   13 | step:  5299 | train Sqc: 0.5239 | loss: 1.2545\n",
      "epoch:   13 | step:  5399 | train Sqc: 0.5238 | loss: 1.2546\n",
      "epoch:   13 | step:  5499 | train Sqc: 0.5241 | loss: 1.2546\n",
      "epoch:   13 | step:  5599 | train Sqc: 0.5238 | loss: 1.2546\n",
      "epoch:   13 | step:  5699 | train Sqc: 0.5238 | loss: 1.2546\n",
      "epoch:   13 | step:  5799 | train Sqc: 0.5242 | loss: 1.2546\n",
      "epoch:   13 | step:  5899 | train Sqc: 0.5243 | loss: 1.2546\n",
      "epoch:   13 | step:  5999 | train Sqc: 0.5241 | loss: 1.2546\n",
      "epoch:   13 | step:  6099 | train Sqc: 0.5244 | loss: 1.2546\n",
      "epoch:   13 | step:  6199 | train Sqc: 0.5241 | loss: 1.2546\n",
      "epoch:   13 | step:  6299 | train Sqc: 0.5242 | loss: 1.2546\n",
      "epoch:   13 | step:  6399 | train Sqc: 0.5243 | loss: 1.2546\n",
      "epoch:   13 | step:  6499 | train Sqc: 0.5246 | loss: 1.2547\n",
      "epoch:   13 | step:  6599 | train Sqc: 0.5243 | loss: 1.2546\n",
      "epoch:   13 | step:  6699 | train Sqc: 0.5241 | loss: 1.2546\n",
      "epoch:   13 | step:  6799 | train Sqc: 0.5243 | loss: 1.2546\n",
      "epoch:   13 | step:  6899 | train Sqc: 0.5239 | loss: 1.2546\n",
      "epoch:   13 | step:  6999 | train Sqc: 0.5232 | loss: 1.2545\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   13 | step:   99 | N:  4 | test Sqc: 0.3667 | test Neg: 0.4155\n",
      "epoch:   13 | step:  199 | N:  4 | test Sqc: 0.3736 | test Neg: 0.4143\n",
      "epoch:   13 | step:   99 | N:  6 | test Sqc: 0.4296 | test Neg: 0.3996\n",
      "epoch:   13 | step:  199 | N:  6 | test Sqc: 0.4101 | test Neg: 0.4048\n",
      "epoch:   13 | step:   99 | N:  8 | test Sqc: 0.4894 | test Neg: 0.3820\n",
      "epoch:   13 | step:  199 | N:  8 | test Sqc: 0.4849 | test Neg: 0.3823\n",
      "epoch:   13 | step:   99 | N:  10 | test Sqc: 0.4757 | test Neg: 0.3846\n",
      "epoch:   13 | step:  199 | N:  10 | test Sqc: 0.4887 | test Neg: 0.3797\n",
      "epoch:   13 | step:   99 | N:  12 | test Sqc: 0.5600 | test Neg: 0.3582\n",
      "epoch:   13 | step:  199 | N:  12 | test Sqc: 0.5592 | test Neg: 0.3571\n",
      "epoch:   13 | step:   99 | N:  14 | test Sqc: 0.6525 | test Neg: 0.3249\n",
      "epoch:   13 | step:  199 | N:  14 | test Sqc: 0.6611 | test Neg: 0.3227\n",
      "epoch:   13 | step:   99 | N:  16 | test Sqc: 0.7075 | test Neg: 0.3018\n",
      "epoch:   13 | step:  199 | N:  16 | test Sqc: 0.7109 | test Neg: 0.3019\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   14 | step:   99 | train Sqc: 0.5309 | loss: 1.2558\n",
      "epoch:   14 | step:  199 | train Sqc: 0.5369 | loss: 1.2565\n",
      "epoch:   14 | step:  299 | train Sqc: 0.5275 | loss: 1.2552\n",
      "epoch:   14 | step:  399 | train Sqc: 0.5249 | loss: 1.2548\n",
      "epoch:   14 | step:  499 | train Sqc: 0.5309 | loss: 1.2551\n",
      "epoch:   14 | step:  599 | train Sqc: 0.5301 | loss: 1.2549\n",
      "epoch:   14 | step:  699 | train Sqc: 0.5287 | loss: 1.2547\n",
      "epoch:   14 | step:  799 | train Sqc: 0.5276 | loss: 1.2547\n",
      "epoch:   14 | step:  899 | train Sqc: 0.5287 | loss: 1.2548\n",
      "epoch:   14 | step:  999 | train Sqc: 0.5270 | loss: 1.2546\n",
      "epoch:   14 | step:  1099 | train Sqc: 0.5274 | loss: 1.2546\n",
      "epoch:   14 | step:  1199 | train Sqc: 0.5277 | loss: 1.2546\n",
      "epoch:   14 | step:  1299 | train Sqc: 0.5259 | loss: 1.2544\n",
      "epoch:   14 | step:  1399 | train Sqc: 0.5251 | loss: 1.2543\n",
      "epoch:   14 | step:  1499 | train Sqc: 0.5247 | loss: 1.2543\n",
      "epoch:   14 | step:  1599 | train Sqc: 0.5232 | loss: 1.2541\n",
      "epoch:   14 | step:  1699 | train Sqc: 0.5233 | loss: 1.2541\n",
      "epoch:   14 | step:  1799 | train Sqc: 0.5232 | loss: 1.2541\n",
      "epoch:   14 | step:  1899 | train Sqc: 0.5232 | loss: 1.2542\n",
      "epoch:   14 | step:  1999 | train Sqc: 0.5242 | loss: 1.2542\n",
      "epoch:   14 | step:  2099 | train Sqc: 0.5240 | loss: 1.2543\n",
      "epoch:   14 | step:  2199 | train Sqc: 0.5246 | loss: 1.2544\n",
      "epoch:   14 | step:  2299 | train Sqc: 0.5253 | loss: 1.2545\n",
      "epoch:   14 | step:  2399 | train Sqc: 0.5259 | loss: 1.2547\n",
      "epoch:   14 | step:  2499 | train Sqc: 0.5260 | loss: 1.2547\n",
      "epoch:   14 | step:  2599 | train Sqc: 0.5252 | loss: 1.2547\n",
      "epoch:   14 | step:  2699 | train Sqc: 0.5253 | loss: 1.2547\n",
      "epoch:   14 | step:  2799 | train Sqc: 0.5258 | loss: 1.2547\n",
      "epoch:   14 | step:  2899 | train Sqc: 0.5257 | loss: 1.2547\n",
      "epoch:   14 | step:  2999 | train Sqc: 0.5258 | loss: 1.2547\n",
      "epoch:   14 | step:  3099 | train Sqc: 0.5249 | loss: 1.2547\n",
      "epoch:   14 | step:  3199 | train Sqc: 0.5238 | loss: 1.2545\n",
      "epoch:   14 | step:  3299 | train Sqc: 0.5237 | loss: 1.2545\n",
      "epoch:   14 | step:  3399 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   14 | step:  3499 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   14 | step:  3599 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   14 | step:  3699 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   14 | step:  3799 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   14 | step:  3899 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   14 | step:  3999 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   14 | step:  4099 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   14 | step:  4199 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   14 | step:  4299 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   14 | step:  4399 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   14 | step:  4499 | train Sqc: 0.5232 | loss: 1.2545\n",
      "epoch:   14 | step:  4599 | train Sqc: 0.5230 | loss: 1.2544\n",
      "epoch:   14 | step:  4699 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   14 | step:  4799 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   14 | step:  4899 | train Sqc: 0.5233 | loss: 1.2545\n",
      "epoch:   14 | step:  4999 | train Sqc: 0.5232 | loss: 1.2545\n",
      "epoch:   14 | step:  5099 | train Sqc: 0.5236 | loss: 1.2545\n",
      "epoch:   14 | step:  5199 | train Sqc: 0.5232 | loss: 1.2545\n",
      "epoch:   14 | step:  5299 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   14 | step:  5399 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   14 | step:  5499 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   14 | step:  5599 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   14 | step:  5699 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   14 | step:  5799 | train Sqc: 0.5235 | loss: 1.2546\n",
      "epoch:   14 | step:  5899 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   14 | step:  5999 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   14 | step:  6099 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   14 | step:  6199 | train Sqc: 0.5234 | loss: 1.2546\n",
      "epoch:   14 | step:  6299 | train Sqc: 0.5235 | loss: 1.2545\n",
      "epoch:   14 | step:  6399 | train Sqc: 0.5236 | loss: 1.2546\n",
      "epoch:   14 | step:  6499 | train Sqc: 0.5239 | loss: 1.2546\n",
      "epoch:   14 | step:  6599 | train Sqc: 0.5236 | loss: 1.2546\n",
      "epoch:   14 | step:  6699 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   14 | step:  6799 | train Sqc: 0.5235 | loss: 1.2546\n",
      "epoch:   14 | step:  6899 | train Sqc: 0.5232 | loss: 1.2545\n",
      "epoch:   14 | step:  6999 | train Sqc: 0.5226 | loss: 1.2545\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   14 | step:   99 | N:  4 | test Sqc: 0.3666 | test Neg: 0.4156\n",
      "epoch:   14 | step:  199 | N:  4 | test Sqc: 0.3731 | test Neg: 0.4143\n",
      "epoch:   14 | step:   99 | N:  6 | test Sqc: 0.4296 | test Neg: 0.3995\n",
      "epoch:   14 | step:  199 | N:  6 | test Sqc: 0.4103 | test Neg: 0.4048\n",
      "epoch:   14 | step:   99 | N:  8 | test Sqc: 0.4931 | test Neg: 0.3818\n",
      "epoch:   14 | step:  199 | N:  8 | test Sqc: 0.4873 | test Neg: 0.3823\n",
      "epoch:   14 | step:   99 | N:  10 | test Sqc: 0.4774 | test Neg: 0.3847\n",
      "epoch:   14 | step:  199 | N:  10 | test Sqc: 0.4891 | test Neg: 0.3798\n",
      "epoch:   14 | step:   99 | N:  12 | test Sqc: 0.5600 | test Neg: 0.3583\n",
      "epoch:   14 | step:  199 | N:  12 | test Sqc: 0.5584 | test Neg: 0.3572\n",
      "epoch:   14 | step:   99 | N:  14 | test Sqc: 0.6518 | test Neg: 0.3250\n",
      "epoch:   14 | step:  199 | N:  14 | test Sqc: 0.6592 | test Neg: 0.3229\n",
      "epoch:   14 | step:   99 | N:  16 | test Sqc: 0.7070 | test Neg: 0.3020\n",
      "epoch:   14 | step:  199 | N:  16 | test Sqc: 0.7084 | test Neg: 0.3019\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   15 | step:   99 | train Sqc: 0.5297 | loss: 1.2559\n",
      "epoch:   15 | step:  199 | train Sqc: 0.5362 | loss: 1.2566\n",
      "epoch:   15 | step:  299 | train Sqc: 0.5269 | loss: 1.2552\n",
      "epoch:   15 | step:  399 | train Sqc: 0.5248 | loss: 1.2549\n",
      "epoch:   15 | step:  499 | train Sqc: 0.5310 | loss: 1.2552\n",
      "epoch:   15 | step:  599 | train Sqc: 0.5298 | loss: 1.2548\n",
      "epoch:   15 | step:  699 | train Sqc: 0.5282 | loss: 1.2546\n",
      "epoch:   15 | step:  799 | train Sqc: 0.5269 | loss: 1.2547\n",
      "epoch:   15 | step:  899 | train Sqc: 0.5281 | loss: 1.2547\n",
      "epoch:   15 | step:  999 | train Sqc: 0.5265 | loss: 1.2545\n",
      "epoch:   15 | step:  1099 | train Sqc: 0.5267 | loss: 1.2545\n",
      "epoch:   15 | step:  1199 | train Sqc: 0.5269 | loss: 1.2545\n",
      "epoch:   15 | step:  1299 | train Sqc: 0.5249 | loss: 1.2542\n",
      "epoch:   15 | step:  1399 | train Sqc: 0.5242 | loss: 1.2541\n",
      "epoch:   15 | step:  1499 | train Sqc: 0.5242 | loss: 1.2542\n",
      "epoch:   15 | step:  1599 | train Sqc: 0.5229 | loss: 1.2540\n",
      "epoch:   15 | step:  1699 | train Sqc: 0.5230 | loss: 1.2540\n",
      "epoch:   15 | step:  1799 | train Sqc: 0.5229 | loss: 1.2540\n",
      "epoch:   15 | step:  1899 | train Sqc: 0.5230 | loss: 1.2541\n",
      "epoch:   15 | step:  1999 | train Sqc: 0.5240 | loss: 1.2542\n",
      "epoch:   15 | step:  2099 | train Sqc: 0.5236 | loss: 1.2542\n",
      "epoch:   15 | step:  2199 | train Sqc: 0.5237 | loss: 1.2542\n",
      "epoch:   15 | step:  2299 | train Sqc: 0.5242 | loss: 1.2544\n",
      "epoch:   15 | step:  2399 | train Sqc: 0.5249 | loss: 1.2545\n",
      "epoch:   15 | step:  2499 | train Sqc: 0.5252 | loss: 1.2546\n",
      "epoch:   15 | step:  2599 | train Sqc: 0.5243 | loss: 1.2545\n",
      "epoch:   15 | step:  2699 | train Sqc: 0.5245 | loss: 1.2546\n",
      "epoch:   15 | step:  2799 | train Sqc: 0.5251 | loss: 1.2546\n",
      "epoch:   15 | step:  2899 | train Sqc: 0.5250 | loss: 1.2546\n",
      "epoch:   15 | step:  2999 | train Sqc: 0.5251 | loss: 1.2546\n",
      "epoch:   15 | step:  3099 | train Sqc: 0.5240 | loss: 1.2545\n",
      "epoch:   15 | step:  3199 | train Sqc: 0.5230 | loss: 1.2544\n",
      "epoch:   15 | step:  3299 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   15 | step:  3399 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   15 | step:  3499 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   15 | step:  3599 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   15 | step:  3699 | train Sqc: 0.5229 | loss: 1.2544\n",
      "epoch:   15 | step:  3799 | train Sqc: 0.5222 | loss: 1.2543\n",
      "epoch:   15 | step:  3899 | train Sqc: 0.5220 | loss: 1.2543\n",
      "epoch:   15 | step:  3999 | train Sqc: 0.5222 | loss: 1.2543\n",
      "epoch:   15 | step:  4099 | train Sqc: 0.5223 | loss: 1.2543\n",
      "epoch:   15 | step:  4199 | train Sqc: 0.5222 | loss: 1.2544\n",
      "epoch:   15 | step:  4299 | train Sqc: 0.5221 | loss: 1.2543\n",
      "epoch:   15 | step:  4399 | train Sqc: 0.5222 | loss: 1.2543\n",
      "epoch:   15 | step:  4499 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   15 | step:  4599 | train Sqc: 0.5223 | loss: 1.2543\n",
      "epoch:   15 | step:  4699 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   15 | step:  4799 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   15 | step:  4899 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   15 | step:  4999 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   15 | step:  5099 | train Sqc: 0.5230 | loss: 1.2544\n",
      "epoch:   15 | step:  5199 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   15 | step:  5299 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   15 | step:  5399 | train Sqc: 0.5224 | loss: 1.2544\n",
      "epoch:   15 | step:  5499 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   15 | step:  5599 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   15 | step:  5699 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   15 | step:  5799 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   15 | step:  5899 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   15 | step:  5999 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   15 | step:  6099 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   15 | step:  6199 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   15 | step:  6299 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   15 | step:  6399 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   15 | step:  6499 | train Sqc: 0.5233 | loss: 1.2545\n",
      "epoch:   15 | step:  6599 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   15 | step:  6699 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   15 | step:  6799 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   15 | step:  6899 | train Sqc: 0.5226 | loss: 1.2544\n",
      "epoch:   15 | step:  6999 | train Sqc: 0.5221 | loss: 1.2544\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   15 | step:   99 | N:  4 | test Sqc: 0.3657 | test Neg: 0.4155\n",
      "epoch:   15 | step:  199 | N:  4 | test Sqc: 0.3729 | test Neg: 0.4143\n",
      "epoch:   15 | step:   99 | N:  6 | test Sqc: 0.4307 | test Neg: 0.3997\n",
      "epoch:   15 | step:  199 | N:  6 | test Sqc: 0.4095 | test Neg: 0.4049\n",
      "epoch:   15 | step:   99 | N:  8 | test Sqc: 0.4906 | test Neg: 0.3819\n",
      "epoch:   15 | step:  199 | N:  8 | test Sqc: 0.4856 | test Neg: 0.3823\n",
      "epoch:   15 | step:   99 | N:  10 | test Sqc: 0.4792 | test Neg: 0.3812\n",
      "epoch:   15 | step:  199 | N:  10 | test Sqc: 0.4918 | test Neg: 0.3764\n",
      "epoch:   15 | step:   99 | N:  12 | test Sqc: 0.5631 | test Neg: 0.3574\n",
      "epoch:   15 | step:  199 | N:  12 | test Sqc: 0.5598 | test Neg: 0.3564\n",
      "epoch:   15 | step:   99 | N:  14 | test Sqc: 0.6529 | test Neg: 0.3247\n",
      "epoch:   15 | step:  199 | N:  14 | test Sqc: 0.6621 | test Neg: 0.3221\n",
      "epoch:   15 | step:   99 | N:  16 | test Sqc: 0.7067 | test Neg: 0.3031\n",
      "epoch:   15 | step:  199 | N:  16 | test Sqc: 0.7073 | test Neg: 0.3027\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   16 | step:   99 | train Sqc: 0.5286 | loss: 1.2559\n",
      "epoch:   16 | step:  199 | train Sqc: 0.5349 | loss: 1.2565\n",
      "epoch:   16 | step:  299 | train Sqc: 0.5267 | loss: 1.2552\n",
      "epoch:   16 | step:  399 | train Sqc: 0.5241 | loss: 1.2548\n",
      "epoch:   16 | step:  499 | train Sqc: 0.5300 | loss: 1.2551\n",
      "epoch:   16 | step:  599 | train Sqc: 0.5285 | loss: 1.2547\n",
      "epoch:   16 | step:  699 | train Sqc: 0.5268 | loss: 1.2545\n",
      "epoch:   16 | step:  799 | train Sqc: 0.5256 | loss: 1.2545\n",
      "epoch:   16 | step:  899 | train Sqc: 0.5267 | loss: 1.2546\n",
      "epoch:   16 | step:  999 | train Sqc: 0.5250 | loss: 1.2543\n",
      "epoch:   16 | step:  1099 | train Sqc: 0.5253 | loss: 1.2543\n",
      "epoch:   16 | step:  1199 | train Sqc: 0.5257 | loss: 1.2543\n",
      "epoch:   16 | step:  1299 | train Sqc: 0.5236 | loss: 1.2541\n",
      "epoch:   16 | step:  1399 | train Sqc: 0.5231 | loss: 1.2540\n",
      "epoch:   16 | step:  1499 | train Sqc: 0.5230 | loss: 1.2540\n",
      "epoch:   16 | step:  1599 | train Sqc: 0.5216 | loss: 1.2538\n",
      "epoch:   16 | step:  1699 | train Sqc: 0.5215 | loss: 1.2538\n",
      "epoch:   16 | step:  1799 | train Sqc: 0.5214 | loss: 1.2538\n",
      "epoch:   16 | step:  1899 | train Sqc: 0.5214 | loss: 1.2539\n",
      "epoch:   16 | step:  1999 | train Sqc: 0.5226 | loss: 1.2540\n",
      "epoch:   16 | step:  2099 | train Sqc: 0.5224 | loss: 1.2540\n",
      "epoch:   16 | step:  2199 | train Sqc: 0.5227 | loss: 1.2541\n",
      "epoch:   16 | step:  2299 | train Sqc: 0.5232 | loss: 1.2542\n",
      "epoch:   16 | step:  2399 | train Sqc: 0.5239 | loss: 1.2544\n",
      "epoch:   16 | step:  2499 | train Sqc: 0.5242 | loss: 1.2545\n",
      "epoch:   16 | step:  2599 | train Sqc: 0.5234 | loss: 1.2544\n",
      "epoch:   16 | step:  2699 | train Sqc: 0.5237 | loss: 1.2544\n",
      "epoch:   16 | step:  2799 | train Sqc: 0.5242 | loss: 1.2545\n",
      "epoch:   16 | step:  2899 | train Sqc: 0.5242 | loss: 1.2545\n",
      "epoch:   16 | step:  2999 | train Sqc: 0.5243 | loss: 1.2545\n",
      "epoch:   16 | step:  3099 | train Sqc: 0.5235 | loss: 1.2544\n",
      "epoch:   16 | step:  3199 | train Sqc: 0.5224 | loss: 1.2543\n",
      "epoch:   16 | step:  3299 | train Sqc: 0.5225 | loss: 1.2543\n",
      "epoch:   16 | step:  3399 | train Sqc: 0.5223 | loss: 1.2544\n",
      "epoch:   16 | step:  3499 | train Sqc: 0.5222 | loss: 1.2543\n",
      "epoch:   16 | step:  3599 | train Sqc: 0.5223 | loss: 1.2543\n",
      "epoch:   16 | step:  3699 | train Sqc: 0.5225 | loss: 1.2543\n",
      "epoch:   16 | step:  3799 | train Sqc: 0.5230 | loss: 1.2544\n",
      "epoch:   16 | step:  3899 | train Sqc: 0.5228 | loss: 1.2544\n",
      "epoch:   16 | step:  3999 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   16 | step:  4099 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   16 | step:  4199 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  4299 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  4399 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   16 | step:  4499 | train Sqc: 0.5233 | loss: 1.2545\n",
      "epoch:   16 | step:  4599 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  4699 | train Sqc: 0.5224 | loss: 1.2544\n",
      "epoch:   16 | step:  4799 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   16 | step:  4899 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   16 | step:  4999 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  5099 | train Sqc: 0.5234 | loss: 1.2545\n",
      "epoch:   16 | step:  5199 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   16 | step:  5299 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  5399 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  5499 | train Sqc: 0.5232 | loss: 1.2546\n",
      "epoch:   16 | step:  5599 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   16 | step:  5699 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   16 | step:  5799 | train Sqc: 0.5233 | loss: 1.2546\n",
      "epoch:   16 | step:  5899 | train Sqc: 0.5234 | loss: 1.2546\n",
      "epoch:   16 | step:  5999 | train Sqc: 0.5232 | loss: 1.2545\n",
      "epoch:   16 | step:  6099 | train Sqc: 0.5234 | loss: 1.2546\n",
      "epoch:   16 | step:  6199 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   16 | step:  6299 | train Sqc: 0.5233 | loss: 1.2545\n",
      "epoch:   16 | step:  6399 | train Sqc: 0.5233 | loss: 1.2546\n",
      "epoch:   16 | step:  6499 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   16 | step:  6599 | train Sqc: 0.5234 | loss: 1.2546\n",
      "epoch:   16 | step:  6699 | train Sqc: 0.5233 | loss: 1.2545\n",
      "epoch:   16 | step:  6799 | train Sqc: 0.5234 | loss: 1.2546\n",
      "epoch:   16 | step:  6899 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   16 | step:  6999 | train Sqc: 0.5225 | loss: 1.2545\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   16 | step:   99 | N:  4 | test Sqc: 0.3661 | test Neg: 0.4157\n",
      "epoch:   16 | step:  199 | N:  4 | test Sqc: 0.3724 | test Neg: 0.4144\n",
      "epoch:   16 | step:   99 | N:  6 | test Sqc: 0.4296 | test Neg: 0.3996\n",
      "epoch:   16 | step:  199 | N:  6 | test Sqc: 0.4074 | test Neg: 0.4049\n",
      "epoch:   16 | step:   99 | N:  8 | test Sqc: 0.4877 | test Neg: 0.3819\n",
      "epoch:   16 | step:  199 | N:  8 | test Sqc: 0.4848 | test Neg: 0.3822\n",
      "epoch:   16 | step:   99 | N:  10 | test Sqc: 0.4777 | test Neg: 0.3847\n",
      "epoch:   16 | step:  199 | N:  10 | test Sqc: 0.4898 | test Neg: 0.3797\n",
      "epoch:   16 | step:   99 | N:  12 | test Sqc: 0.5720 | test Neg: 0.3559\n",
      "epoch:   16 | step:  199 | N:  12 | test Sqc: 0.5674 | test Neg: 0.3551\n",
      "epoch:   16 | step:   99 | N:  14 | test Sqc: 0.6575 | test Neg: 0.3235\n",
      "epoch:   16 | step:  199 | N:  14 | test Sqc: 0.6671 | test Neg: 0.3216\n",
      "epoch:   16 | step:   99 | N:  16 | test Sqc: 0.7127 | test Neg: 0.3003\n",
      "epoch:   16 | step:  199 | N:  16 | test Sqc: 0.7118 | test Neg: 0.3005\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   17 | step:   99 | train Sqc: 0.5300 | loss: 1.2559\n",
      "epoch:   17 | step:  199 | train Sqc: 0.5373 | loss: 1.2566\n",
      "epoch:   17 | step:  299 | train Sqc: 0.5281 | loss: 1.2553\n",
      "epoch:   17 | step:  399 | train Sqc: 0.5252 | loss: 1.2549\n",
      "epoch:   17 | step:  499 | train Sqc: 0.5308 | loss: 1.2552\n",
      "epoch:   17 | step:  599 | train Sqc: 0.5292 | loss: 1.2548\n",
      "epoch:   17 | step:  699 | train Sqc: 0.5271 | loss: 1.2545\n",
      "epoch:   17 | step:  799 | train Sqc: 0.5261 | loss: 1.2546\n",
      "epoch:   17 | step:  899 | train Sqc: 0.5274 | loss: 1.2546\n",
      "epoch:   17 | step:  999 | train Sqc: 0.5258 | loss: 1.2544\n",
      "epoch:   17 | step:  1099 | train Sqc: 0.5258 | loss: 1.2544\n",
      "epoch:   17 | step:  1199 | train Sqc: 0.5262 | loss: 1.2544\n",
      "epoch:   17 | step:  1299 | train Sqc: 0.5240 | loss: 1.2541\n",
      "epoch:   17 | step:  1399 | train Sqc: 0.5233 | loss: 1.2540\n",
      "epoch:   17 | step:  1499 | train Sqc: 0.5230 | loss: 1.2540\n",
      "epoch:   17 | step:  1599 | train Sqc: 0.5216 | loss: 1.2538\n",
      "epoch:   17 | step:  1699 | train Sqc: 0.5216 | loss: 1.2538\n",
      "epoch:   17 | step:  1799 | train Sqc: 0.5216 | loss: 1.2538\n",
      "epoch:   17 | step:  1899 | train Sqc: 0.5216 | loss: 1.2539\n",
      "epoch:   17 | step:  1999 | train Sqc: 0.5226 | loss: 1.2540\n",
      "epoch:   17 | step:  2099 | train Sqc: 0.5223 | loss: 1.2540\n",
      "epoch:   17 | step:  2199 | train Sqc: 0.5225 | loss: 1.2541\n",
      "epoch:   17 | step:  2299 | train Sqc: 0.5231 | loss: 1.2542\n",
      "epoch:   17 | step:  2399 | train Sqc: 0.5238 | loss: 1.2544\n",
      "epoch:   17 | step:  2499 | train Sqc: 0.5244 | loss: 1.2545\n",
      "epoch:   17 | step:  2599 | train Sqc: 0.5236 | loss: 1.2544\n",
      "epoch:   17 | step:  2699 | train Sqc: 0.5239 | loss: 1.2545\n",
      "epoch:   17 | step:  2799 | train Sqc: 0.5245 | loss: 1.2545\n",
      "epoch:   17 | step:  2899 | train Sqc: 0.5244 | loss: 1.2545\n",
      "epoch:   17 | step:  2999 | train Sqc: 0.5244 | loss: 1.2545\n",
      "epoch:   17 | step:  3099 | train Sqc: 0.5233 | loss: 1.2544\n",
      "epoch:   17 | step:  3199 | train Sqc: 0.5223 | loss: 1.2543\n",
      "epoch:   17 | step:  3299 | train Sqc: 0.5221 | loss: 1.2543\n",
      "epoch:   17 | step:  3399 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  3499 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  3599 | train Sqc: 0.5219 | loss: 1.2542\n",
      "epoch:   17 | step:  3699 | train Sqc: 0.5221 | loss: 1.2543\n",
      "epoch:   17 | step:  3799 | train Sqc: 0.5214 | loss: 1.2541\n",
      "epoch:   17 | step:  3899 | train Sqc: 0.5212 | loss: 1.2541\n",
      "epoch:   17 | step:  3999 | train Sqc: 0.5214 | loss: 1.2542\n",
      "epoch:   17 | step:  4099 | train Sqc: 0.5215 | loss: 1.2542\n",
      "epoch:   17 | step:  4199 | train Sqc: 0.5213 | loss: 1.2542\n",
      "epoch:   17 | step:  4299 | train Sqc: 0.5214 | loss: 1.2542\n",
      "epoch:   17 | step:  4399 | train Sqc: 0.5216 | loss: 1.2542\n",
      "epoch:   17 | step:  4499 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  4599 | train Sqc: 0.5216 | loss: 1.2542\n",
      "epoch:   17 | step:  4699 | train Sqc: 0.5211 | loss: 1.2542\n",
      "epoch:   17 | step:  4799 | train Sqc: 0.5221 | loss: 1.2543\n",
      "epoch:   17 | step:  4899 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  4999 | train Sqc: 0.5218 | loss: 1.2543\n",
      "epoch:   17 | step:  5099 | train Sqc: 0.5223 | loss: 1.2543\n",
      "epoch:   17 | step:  5199 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  5299 | train Sqc: 0.5218 | loss: 1.2543\n",
      "epoch:   17 | step:  5399 | train Sqc: 0.5218 | loss: 1.2543\n",
      "epoch:   17 | step:  5499 | train Sqc: 0.5221 | loss: 1.2544\n",
      "epoch:   17 | step:  5599 | train Sqc: 0.5218 | loss: 1.2543\n",
      "epoch:   17 | step:  5699 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   17 | step:  5799 | train Sqc: 0.5223 | loss: 1.2544\n",
      "epoch:   17 | step:  5899 | train Sqc: 0.5224 | loss: 1.2544\n",
      "epoch:   17 | step:  5999 | train Sqc: 0.5222 | loss: 1.2544\n",
      "epoch:   17 | step:  6099 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   17 | step:  6199 | train Sqc: 0.5222 | loss: 1.2544\n",
      "epoch:   17 | step:  6299 | train Sqc: 0.5223 | loss: 1.2544\n",
      "epoch:   17 | step:  6399 | train Sqc: 0.5224 | loss: 1.2544\n",
      "epoch:   17 | step:  6499 | train Sqc: 0.5227 | loss: 1.2544\n",
      "epoch:   17 | step:  6599 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   17 | step:  6699 | train Sqc: 0.5224 | loss: 1.2544\n",
      "epoch:   17 | step:  6799 | train Sqc: 0.5225 | loss: 1.2544\n",
      "epoch:   17 | step:  6899 | train Sqc: 0.5221 | loss: 1.2544\n",
      "epoch:   17 | step:  6999 | train Sqc: 0.5216 | loss: 1.2544\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   17 | step:   99 | N:  4 | test Sqc: 0.3672 | test Neg: 0.4156\n",
      "epoch:   17 | step:  199 | N:  4 | test Sqc: 0.3728 | test Neg: 0.4143\n",
      "epoch:   17 | step:   99 | N:  6 | test Sqc: 0.4279 | test Neg: 0.3997\n",
      "epoch:   17 | step:  199 | N:  6 | test Sqc: 0.4072 | test Neg: 0.4049\n",
      "epoch:   17 | step:   99 | N:  8 | test Sqc: 0.4911 | test Neg: 0.3818\n",
      "epoch:   17 | step:  199 | N:  8 | test Sqc: 0.4859 | test Neg: 0.3822\n",
      "epoch:   17 | step:   99 | N:  10 | test Sqc: 0.4770 | test Neg: 0.3847\n",
      "epoch:   17 | step:  199 | N:  10 | test Sqc: 0.4904 | test Neg: 0.3797\n",
      "epoch:   17 | step:   99 | N:  12 | test Sqc: 0.5641 | test Neg: 0.3580\n",
      "epoch:   17 | step:  199 | N:  12 | test Sqc: 0.5602 | test Neg: 0.3568\n",
      "epoch:   17 | step:   99 | N:  14 | test Sqc: 0.6602 | test Neg: 0.3228\n",
      "epoch:   17 | step:  199 | N:  14 | test Sqc: 0.6655 | test Neg: 0.3208\n",
      "epoch:   17 | step:   99 | N:  16 | test Sqc: 0.7082 | test Neg: 0.2999\n",
      "epoch:   17 | step:  199 | N:  16 | test Sqc: 0.7123 | test Neg: 0.2999\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   18 | step:   99 | train Sqc: 0.5302 | loss: 1.2561\n",
      "epoch:   18 | step:  199 | train Sqc: 0.5370 | loss: 1.2567\n",
      "epoch:   18 | step:  299 | train Sqc: 0.5270 | loss: 1.2553\n",
      "epoch:   18 | step:  399 | train Sqc: 0.5247 | loss: 1.2549\n",
      "epoch:   18 | step:  499 | train Sqc: 0.5299 | loss: 1.2551\n",
      "epoch:   18 | step:  599 | train Sqc: 0.5283 | loss: 1.2547\n",
      "epoch:   18 | step:  699 | train Sqc: 0.5263 | loss: 1.2545\n",
      "epoch:   18 | step:  799 | train Sqc: 0.5254 | loss: 1.2545\n",
      "epoch:   18 | step:  899 | train Sqc: 0.5271 | loss: 1.2546\n",
      "epoch:   18 | step:  999 | train Sqc: 0.5260 | loss: 1.2544\n",
      "epoch:   18 | step:  1099 | train Sqc: 0.5262 | loss: 1.2544\n",
      "epoch:   18 | step:  1199 | train Sqc: 0.5264 | loss: 1.2544\n",
      "epoch:   18 | step:  1299 | train Sqc: 0.5243 | loss: 1.2542\n",
      "epoch:   18 | step:  1399 | train Sqc: 0.5236 | loss: 1.2540\n",
      "epoch:   18 | step:  1499 | train Sqc: 0.5233 | loss: 1.2540\n",
      "epoch:   18 | step:  1599 | train Sqc: 0.5217 | loss: 1.2539\n",
      "epoch:   18 | step:  1699 | train Sqc: 0.5216 | loss: 1.2538\n",
      "epoch:   18 | step:  1799 | train Sqc: 0.5215 | loss: 1.2538\n",
      "epoch:   18 | step:  1899 | train Sqc: 0.5215 | loss: 1.2539\n",
      "epoch:   18 | step:  1999 | train Sqc: 0.5225 | loss: 1.2540\n",
      "epoch:   18 | step:  2099 | train Sqc: 0.5222 | loss: 1.2540\n",
      "epoch:   18 | step:  2199 | train Sqc: 0.5224 | loss: 1.2541\n",
      "epoch:   18 | step:  2299 | train Sqc: 0.5229 | loss: 1.2542\n",
      "epoch:   18 | step:  2399 | train Sqc: 0.5236 | loss: 1.2543\n",
      "epoch:   18 | step:  2499 | train Sqc: 0.5239 | loss: 1.2544\n",
      "epoch:   18 | step:  2599 | train Sqc: 0.5230 | loss: 1.2543\n",
      "epoch:   18 | step:  2699 | train Sqc: 0.5231 | loss: 1.2544\n",
      "epoch:   18 | step:  2799 | train Sqc: 0.5236 | loss: 1.2544\n",
      "epoch:   18 | step:  2899 | train Sqc: 0.5235 | loss: 1.2544\n",
      "epoch:   18 | step:  2999 | train Sqc: 0.5235 | loss: 1.2544\n",
      "epoch:   18 | step:  3099 | train Sqc: 0.5226 | loss: 1.2543\n",
      "epoch:   18 | step:  3199 | train Sqc: 0.5214 | loss: 1.2542\n",
      "epoch:   18 | step:  3299 | train Sqc: 0.5213 | loss: 1.2542\n",
      "epoch:   18 | step:  3399 | train Sqc: 0.5212 | loss: 1.2542\n",
      "epoch:   18 | step:  3499 | train Sqc: 0.5211 | loss: 1.2542\n",
      "epoch:   18 | step:  3599 | train Sqc: 0.5211 | loss: 1.2542\n",
      "epoch:   18 | step:  3699 | train Sqc: 0.5213 | loss: 1.2542\n",
      "epoch:   18 | step:  3799 | train Sqc: 0.5206 | loss: 1.2540\n",
      "epoch:   18 | step:  3899 | train Sqc: 0.5204 | loss: 1.2540\n",
      "epoch:   18 | step:  3999 | train Sqc: 0.5206 | loss: 1.2541\n",
      "epoch:   18 | step:  4099 | train Sqc: 0.5207 | loss: 1.2541\n",
      "epoch:   18 | step:  4199 | train Sqc: 0.5205 | loss: 1.2541\n",
      "epoch:   18 | step:  4299 | train Sqc: 0.5206 | loss: 1.2541\n",
      "epoch:   18 | step:  4399 | train Sqc: 0.5207 | loss: 1.2542\n",
      "epoch:   18 | step:  4499 | train Sqc: 0.5210 | loss: 1.2542\n",
      "epoch:   18 | step:  4599 | train Sqc: 0.5207 | loss: 1.2541\n",
      "epoch:   18 | step:  4699 | train Sqc: 0.5202 | loss: 1.2541\n",
      "epoch:   18 | step:  4799 | train Sqc: 0.5212 | loss: 1.2542\n",
      "epoch:   18 | step:  4899 | train Sqc: 0.5210 | loss: 1.2542\n",
      "epoch:   18 | step:  4999 | train Sqc: 0.5209 | loss: 1.2542\n",
      "epoch:   18 | step:  5099 | train Sqc: 0.5214 | loss: 1.2542\n",
      "epoch:   18 | step:  5199 | train Sqc: 0.5210 | loss: 1.2542\n",
      "epoch:   18 | step:  5299 | train Sqc: 0.5209 | loss: 1.2542\n",
      "epoch:   18 | step:  5399 | train Sqc: 0.5209 | loss: 1.2542\n",
      "epoch:   18 | step:  5499 | train Sqc: 0.5212 | loss: 1.2543\n",
      "epoch:   18 | step:  5599 | train Sqc: 0.5210 | loss: 1.2542\n",
      "epoch:   18 | step:  5699 | train Sqc: 0.5210 | loss: 1.2542\n",
      "epoch:   18 | step:  5799 | train Sqc: 0.5214 | loss: 1.2543\n",
      "epoch:   18 | step:  5899 | train Sqc: 0.5215 | loss: 1.2543\n",
      "epoch:   18 | step:  5999 | train Sqc: 0.5214 | loss: 1.2543\n",
      "epoch:   18 | step:  6099 | train Sqc: 0.5217 | loss: 1.2543\n",
      "epoch:   18 | step:  6199 | train Sqc: 0.5214 | loss: 1.2543\n",
      "epoch:   18 | step:  6299 | train Sqc: 0.5215 | loss: 1.2543\n",
      "epoch:   18 | step:  6399 | train Sqc: 0.5216 | loss: 1.2543\n",
      "epoch:   18 | step:  6499 | train Sqc: 0.5219 | loss: 1.2543\n",
      "epoch:   18 | step:  6599 | train Sqc: 0.5217 | loss: 1.2543\n",
      "epoch:   18 | step:  6699 | train Sqc: 0.5215 | loss: 1.2543\n",
      "epoch:   18 | step:  6799 | train Sqc: 0.5215 | loss: 1.2543\n",
      "epoch:   18 | step:  6899 | train Sqc: 0.5211 | loss: 1.2543\n",
      "epoch:   18 | step:  6999 | train Sqc: 0.5206 | loss: 1.2542\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   18 | step:   99 | N:  4 | test Sqc: 0.3670 | test Neg: 0.4156\n",
      "epoch:   18 | step:  199 | N:  4 | test Sqc: 0.3724 | test Neg: 0.4143\n",
      "epoch:   18 | step:   99 | N:  6 | test Sqc: 0.4306 | test Neg: 0.3996\n",
      "epoch:   18 | step:  199 | N:  6 | test Sqc: 0.4093 | test Neg: 0.4049\n",
      "epoch:   18 | step:   99 | N:  8 | test Sqc: 0.4886 | test Neg: 0.3820\n",
      "epoch:   18 | step:  199 | N:  8 | test Sqc: 0.4837 | test Neg: 0.3824\n",
      "epoch:   18 | step:   99 | N:  10 | test Sqc: 0.4782 | test Neg: 0.3846\n",
      "epoch:   18 | step:  199 | N:  10 | test Sqc: 0.4909 | test Neg: 0.3798\n",
      "epoch:   18 | step:   99 | N:  12 | test Sqc: 0.5598 | test Neg: 0.3581\n",
      "epoch:   18 | step:  199 | N:  12 | test Sqc: 0.5584 | test Neg: 0.3571\n",
      "epoch:   18 | step:   99 | N:  14 | test Sqc: 0.6532 | test Neg: 0.3252\n",
      "epoch:   18 | step:  199 | N:  14 | test Sqc: 0.6604 | test Neg: 0.3232\n",
      "epoch:   18 | step:   99 | N:  16 | test Sqc: 0.7062 | test Neg: 0.3033\n",
      "epoch:   18 | step:  199 | N:  16 | test Sqc: 0.7088 | test Neg: 0.3024\n",
      "==================================================   Train   ==================================================\n",
      "epoch:   19 | step:   99 | train Sqc: 0.5254 | loss: 1.2556\n",
      "epoch:   19 | step:  199 | train Sqc: 0.5348 | loss: 1.2565\n",
      "epoch:   19 | step:  299 | train Sqc: 0.5263 | loss: 1.2551\n",
      "epoch:   19 | step:  399 | train Sqc: 0.5240 | loss: 1.2547\n",
      "epoch:   19 | step:  499 | train Sqc: 0.5299 | loss: 1.2550\n",
      "epoch:   19 | step:  599 | train Sqc: 0.5283 | loss: 1.2546\n",
      "epoch:   19 | step:  699 | train Sqc: 0.5265 | loss: 1.2544\n",
      "epoch:   19 | step:  799 | train Sqc: 0.5253 | loss: 1.2545\n",
      "epoch:   19 | step:  899 | train Sqc: 0.5270 | loss: 1.2546\n",
      "epoch:   19 | step:  999 | train Sqc: 0.5253 | loss: 1.2543\n",
      "epoch:   19 | step:  1099 | train Sqc: 0.5252 | loss: 1.2543\n",
      "epoch:   19 | step:  1199 | train Sqc: 0.5256 | loss: 1.2543\n",
      "epoch:   19 | step:  1299 | train Sqc: 0.5235 | loss: 1.2540\n",
      "epoch:   19 | step:  1399 | train Sqc: 0.5229 | loss: 1.2539\n",
      "epoch:   19 | step:  1499 | train Sqc: 0.5225 | loss: 1.2539\n",
      "epoch:   19 | step:  1599 | train Sqc: 0.5212 | loss: 1.2538\n",
      "epoch:   19 | step:  1699 | train Sqc: 0.5211 | loss: 1.2538\n",
      "epoch:   19 | step:  1799 | train Sqc: 0.5210 | loss: 1.2538\n",
      "epoch:   19 | step:  1899 | train Sqc: 0.5211 | loss: 1.2539\n",
      "epoch:   19 | step:  1999 | train Sqc: 0.5226 | loss: 1.2540\n",
      "epoch:   19 | step:  2099 | train Sqc: 0.5254 | loss: 1.2546\n",
      "epoch:   19 | step:  2199 | train Sqc: 0.5263 | loss: 1.2548\n",
      "epoch:   19 | step:  2299 | train Sqc: 0.5268 | loss: 1.2549\n",
      "epoch:   19 | step:  2399 | train Sqc: 0.5273 | loss: 1.2550\n",
      "epoch:   19 | step:  2499 | train Sqc: 0.5274 | loss: 1.2550\n",
      "epoch:   19 | step:  2599 | train Sqc: 0.5264 | loss: 1.2550\n",
      "epoch:   19 | step:  2699 | train Sqc: 0.5265 | loss: 1.2550\n",
      "epoch:   19 | step:  2799 | train Sqc: 0.5270 | loss: 1.2550\n",
      "epoch:   19 | step:  2899 | train Sqc: 0.5267 | loss: 1.2550\n",
      "epoch:   19 | step:  2999 | train Sqc: 0.5267 | loss: 1.2549\n",
      "epoch:   19 | step:  3099 | train Sqc: 0.5256 | loss: 1.2548\n",
      "epoch:   19 | step:  3199 | train Sqc: 0.5243 | loss: 1.2547\n",
      "epoch:   19 | step:  3299 | train Sqc: 0.5241 | loss: 1.2547\n",
      "epoch:   19 | step:  3399 | train Sqc: 0.5238 | loss: 1.2547\n",
      "epoch:   19 | step:  3499 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   19 | step:  3599 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   19 | step:  3699 | train Sqc: 0.5237 | loss: 1.2546\n",
      "epoch:   19 | step:  3799 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  3899 | train Sqc: 0.5227 | loss: 1.2545\n",
      "epoch:   19 | step:  3999 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  4099 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  4199 | train Sqc: 0.5226 | loss: 1.2545\n",
      "epoch:   19 | step:  4299 | train Sqc: 0.5226 | loss: 1.2545\n",
      "epoch:   19 | step:  4399 | train Sqc: 0.5226 | loss: 1.2545\n",
      "epoch:   19 | step:  4499 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  4599 | train Sqc: 0.5225 | loss: 1.2545\n",
      "epoch:   19 | step:  4699 | train Sqc: 0.5220 | loss: 1.2544\n",
      "epoch:   19 | step:  4799 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   19 | step:  4899 | train Sqc: 0.5227 | loss: 1.2545\n",
      "epoch:   19 | step:  4999 | train Sqc: 0.5226 | loss: 1.2545\n",
      "epoch:   19 | step:  5099 | train Sqc: 0.5231 | loss: 1.2545\n",
      "epoch:   19 | step:  5199 | train Sqc: 0.5226 | loss: 1.2545\n",
      "epoch:   19 | step:  5299 | train Sqc: 0.5225 | loss: 1.2545\n",
      "epoch:   19 | step:  5399 | train Sqc: 0.5224 | loss: 1.2545\n",
      "epoch:   19 | step:  5499 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  5599 | train Sqc: 0.5224 | loss: 1.2545\n",
      "epoch:   19 | step:  5699 | train Sqc: 0.5225 | loss: 1.2545\n",
      "epoch:   19 | step:  5799 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  5899 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  5999 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  6099 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   19 | step:  6199 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  6299 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  6399 | train Sqc: 0.5229 | loss: 1.2545\n",
      "epoch:   19 | step:  6499 | train Sqc: 0.5232 | loss: 1.2546\n",
      "epoch:   19 | step:  6599 | train Sqc: 0.5230 | loss: 1.2545\n",
      "epoch:   19 | step:  6699 | train Sqc: 0.5227 | loss: 1.2545\n",
      "epoch:   19 | step:  6799 | train Sqc: 0.5228 | loss: 1.2545\n",
      "epoch:   19 | step:  6899 | train Sqc: 0.5223 | loss: 1.2545\n",
      "epoch:   19 | step:  6999 | train Sqc: 0.5217 | loss: 1.2544\n",
      "==================================================   Test   ==================================================\n",
      "epoch:   19 | step:   99 | N:  4 | test Sqc: 0.3676 | test Neg: 0.4156\n",
      "epoch:   19 | step:  199 | N:  4 | test Sqc: 0.3731 | test Neg: 0.4143\n",
      "epoch:   19 | step:   99 | N:  6 | test Sqc: 0.4303 | test Neg: 0.3993\n",
      "epoch:   19 | step:  199 | N:  6 | test Sqc: 0.4083 | test Neg: 0.4047\n",
      "epoch:   19 | step:   99 | N:  8 | test Sqc: 0.4884 | test Neg: 0.3818\n",
      "epoch:   19 | step:  199 | N:  8 | test Sqc: 0.4846 | test Neg: 0.3822\n",
      "epoch:   19 | step:   99 | N:  10 | test Sqc: 0.4775 | test Neg: 0.3846\n",
      "epoch:   19 | step:  199 | N:  10 | test Sqc: 0.4895 | test Neg: 0.3796\n",
      "epoch:   19 | step:   99 | N:  12 | test Sqc: 0.5594 | test Neg: 0.3583\n",
      "epoch:   19 | step:  199 | N:  12 | test Sqc: 0.5575 | test Neg: 0.3571\n",
      "epoch:   19 | step:   99 | N:  14 | test Sqc: 0.6568 | test Neg: 0.3245\n",
      "epoch:   19 | step:  199 | N:  14 | test Sqc: 0.6636 | test Neg: 0.3226\n",
      "epoch:   19 | step:   99 | N:  16 | test Sqc: 0.7021 | test Neg: 0.3045\n",
      "epoch:   19 | step:  199 | N:  16 | test Sqc: 0.7034 | test Neg: 0.3039\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    # Train\n",
    "    print('='*50+'   Train   '+'='*50)\n",
    "    mdl.train()\n",
    "    for i in range(prepseq_train.shape[0]):\n",
    "        rhoC = mdl(prepseq_train[i])\n",
    "        l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "        optimizer.zero_grad()\n",
    "        probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "        loss = -probs.log().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l['loss'].append(loss.item())\n",
    "        if (i+1)%100 == 0:\n",
    "            trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "            loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "            print('epoch:  %3d | step:  %3d | train Sqc: %.4f | loss: %.4f' %(epoch, i, trainS, loss))\n",
    "    # Test\n",
    "    if test:\n",
    "        with torch.no_grad():\n",
    "            print('='*50+'   Test   '+'='*50)\n",
    "            mdl.eval()\n",
    "            for n in range(prepseq_test.shape[0]):\n",
    "                for i in range(prepseq_test.shape[1]):\n",
    "                    N = n*2+4\n",
    "                    rhoC = mdl(prepseq_test[n,i])\n",
    "                    l['test Sqc'].append([N,bSqc(rhoS_test[n,i], rhoC).mean().item()])\n",
    "                    l['test Neg'].append([N,Neg(rhoS_test[n,i], rhoC).mean().item()])\n",
    "                    l['test Sa'].append([N,Neg(rhoS_test[n,i], rhoC).mean().item()])\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean(0)[-1].item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean(0)[-1].item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "    torch.save(l, f'{file}/record/gpt_pa.pt')\n",
    "    torch.save(mdl.state_dict(), f'{file}/models/gpt_pa.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1421c9-26b0-4331-9280-6489d25e1de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
