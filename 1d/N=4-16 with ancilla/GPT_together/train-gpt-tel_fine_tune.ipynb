{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg, Sa\n",
    "from Llama2 import LlamaPredictor\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49976"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "train, test = True, True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 5/6\n",
    "batch = 500\n",
    "\n",
    "# mdl = LlamaPredictor(L_max=35,\n",
    "#                      n_embd=12, \n",
    "#                      n_layer=6, \n",
    "#                      n_head=6, \n",
    "#                      vocab_size=4, \n",
    "#                      dropout_prob=0.0).to(device)\n",
    "mdl = LlamaPredictor(L_max=35,\n",
    "                     n_embd=24, \n",
    "                     n_layer=12, \n",
    "                     n_head=6, \n",
    "                     vocab_size=4, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "#mdl.load_state_dict(torch.load(f'{file}/models/gpt_na.pt'))\n",
    "    \n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | train Sqc: 0.3487 | loss: 1.2278\n",
      "epoch:    0 | step:  199 | N:  4 | train Sqc: 0.3658 | loss: 1.2295\n",
      "epoch:    0 | step:  299 | N:  4 | train Sqc: 0.3742 | loss: 1.2300\n",
      "epoch:    0 | step:  399 | N:  4 | train Sqc: 0.3816 | loss: 1.2309\n",
      "epoch:    0 | step:  499 | N:  4 | train Sqc: 0.3802 | loss: 1.2307\n",
      "epoch:    0 | step:  599 | N:  4 | train Sqc: 0.3822 | loss: 1.2308\n",
      "epoch:    0 | step:  699 | N:  4 | train Sqc: 0.3834 | loss: 1.2307\n",
      "epoch:    0 | step:  799 | N:  4 | train Sqc: 0.3834 | loss: 1.2307\n",
      "epoch:    0 | step:  899 | N:  4 | train Sqc: 0.3831 | loss: 1.2305\n",
      "epoch:    0 | step:  999 | N:  4 | train Sqc: 0.3835 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3651 | test Neg: 0.4155\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3692 | test Neg: 0.4142\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | train Sqc: 0.3474 | loss: 1.2277\n",
      "epoch:    1 | step:  199 | N:  4 | train Sqc: 0.3659 | loss: 1.2294\n",
      "epoch:    1 | step:  299 | N:  4 | train Sqc: 0.3738 | loss: 1.2300\n",
      "epoch:    1 | step:  399 | N:  4 | train Sqc: 0.3808 | loss: 1.2308\n",
      "epoch:    1 | step:  499 | N:  4 | train Sqc: 0.3794 | loss: 1.2306\n",
      "epoch:    1 | step:  599 | N:  4 | train Sqc: 0.3813 | loss: 1.2307\n",
      "epoch:    1 | step:  699 | N:  4 | train Sqc: 0.3827 | loss: 1.2307\n",
      "epoch:    1 | step:  799 | N:  4 | train Sqc: 0.3826 | loss: 1.2306\n",
      "epoch:    1 | step:  899 | N:  4 | train Sqc: 0.3824 | loss: 1.2305\n",
      "epoch:    1 | step:  999 | N:  4 | train Sqc: 0.3829 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3657 | test Neg: 0.4156\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3698 | test Neg: 0.4142\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | train Sqc: 0.3473 | loss: 1.2276\n",
      "epoch:    2 | step:  199 | N:  4 | train Sqc: 0.3660 | loss: 1.2294\n",
      "epoch:    2 | step:  299 | N:  4 | train Sqc: 0.3736 | loss: 1.2299\n",
      "epoch:    2 | step:  399 | N:  4 | train Sqc: 0.3805 | loss: 1.2308\n",
      "epoch:    2 | step:  499 | N:  4 | train Sqc: 0.3792 | loss: 1.2306\n",
      "epoch:    2 | step:  599 | N:  4 | train Sqc: 0.3810 | loss: 1.2307\n",
      "epoch:    2 | step:  699 | N:  4 | train Sqc: 0.3824 | loss: 1.2307\n",
      "epoch:    2 | step:  799 | N:  4 | train Sqc: 0.3824 | loss: 1.2306\n",
      "epoch:    2 | step:  899 | N:  4 | train Sqc: 0.3821 | loss: 1.2304\n",
      "epoch:    2 | step:  999 | N:  4 | train Sqc: 0.3826 | loss: 1.2305\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3659 | test Neg: 0.4156\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3700 | test Neg: 0.4142\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | train Sqc: 0.4289 | loss: 1.2356\n",
      "epoch:    0 | step:  199 | N:  6 | train Sqc: 0.4102 | loss: 1.2336\n",
      "epoch:    0 | step:  299 | N:  6 | train Sqc: 0.4042 | loss: 1.2334\n",
      "epoch:    0 | step:  399 | N:  6 | train Sqc: 0.3963 | loss: 1.2326\n",
      "epoch:    0 | step:  499 | N:  6 | train Sqc: 0.3934 | loss: 1.2321\n",
      "epoch:    0 | step:  599 | N:  6 | train Sqc: 0.3930 | loss: 1.2325\n",
      "epoch:    0 | step:  699 | N:  6 | train Sqc: 0.3983 | loss: 1.2331\n",
      "epoch:    0 | step:  799 | N:  6 | train Sqc: 0.3948 | loss: 1.2329\n",
      "epoch:    0 | step:  899 | N:  6 | train Sqc: 0.3956 | loss: 1.2329\n",
      "epoch:    0 | step:  999 | N:  6 | train Sqc: 0.3966 | loss: 1.2331\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4247 | test Neg: 0.3999\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4038 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | train Sqc: 0.4259 | loss: 1.2354\n",
      "epoch:    1 | step:  199 | N:  6 | train Sqc: 0.4087 | loss: 1.2335\n",
      "epoch:    1 | step:  299 | N:  6 | train Sqc: 0.4038 | loss: 1.2333\n",
      "epoch:    1 | step:  399 | N:  6 | train Sqc: 0.3955 | loss: 1.2325\n",
      "epoch:    1 | step:  499 | N:  6 | train Sqc: 0.3925 | loss: 1.2320\n",
      "epoch:    1 | step:  599 | N:  6 | train Sqc: 0.3919 | loss: 1.2324\n",
      "epoch:    1 | step:  699 | N:  6 | train Sqc: 0.3969 | loss: 1.2330\n",
      "epoch:    1 | step:  799 | N:  6 | train Sqc: 0.3934 | loss: 1.2328\n",
      "epoch:    1 | step:  899 | N:  6 | train Sqc: 0.3944 | loss: 1.2328\n",
      "epoch:    1 | step:  999 | N:  6 | train Sqc: 0.3955 | loss: 1.2330\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | test Sqc: 0.4246 | test Neg: 0.3999\n",
      "epoch:    1 | step:  199 | N:  6 | test Sqc: 0.4037 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | train Sqc: 0.4259 | loss: 1.2354\n",
      "epoch:    2 | step:  199 | N:  6 | train Sqc: 0.4085 | loss: 1.2334\n",
      "epoch:    2 | step:  299 | N:  6 | train Sqc: 0.4030 | loss: 1.2333\n",
      "epoch:    2 | step:  399 | N:  6 | train Sqc: 0.3951 | loss: 1.2325\n",
      "epoch:    2 | step:  499 | N:  6 | train Sqc: 0.3919 | loss: 1.2320\n",
      "epoch:    2 | step:  599 | N:  6 | train Sqc: 0.3912 | loss: 1.2323\n",
      "epoch:    2 | step:  699 | N:  6 | train Sqc: 0.3962 | loss: 1.2330\n",
      "epoch:    2 | step:  799 | N:  6 | train Sqc: 0.3927 | loss: 1.2327\n",
      "epoch:    2 | step:  899 | N:  6 | train Sqc: 0.3938 | loss: 1.2327\n",
      "epoch:    2 | step:  999 | N:  6 | train Sqc: 0.3949 | loss: 1.2330\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | test Sqc: 0.4241 | test Neg: 0.3999\n",
      "epoch:    2 | step:  199 | N:  6 | test Sqc: 0.4037 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | train Sqc: 0.5133 | loss: 1.2544\n",
      "epoch:    0 | step:  199 | N:  8 | train Sqc: 0.5095 | loss: 1.2525\n",
      "epoch:    0 | step:  299 | N:  8 | train Sqc: 0.5007 | loss: 1.2511\n",
      "epoch:    0 | step:  399 | N:  8 | train Sqc: 0.4974 | loss: 1.2504\n",
      "epoch:    0 | step:  499 | N:  8 | train Sqc: 0.4959 | loss: 1.2501\n",
      "epoch:    0 | step:  599 | N:  8 | train Sqc: 0.5013 | loss: 1.2504\n",
      "epoch:    0 | step:  699 | N:  8 | train Sqc: 0.4987 | loss: 1.2500\n",
      "epoch:    0 | step:  799 | N:  8 | train Sqc: 0.4970 | loss: 1.2499\n",
      "epoch:    0 | step:  899 | N:  8 | train Sqc: 0.4926 | loss: 1.2494\n",
      "epoch:    0 | step:  999 | N:  8 | train Sqc: 0.4910 | loss: 1.2491\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.4903 | test Neg: 0.3819\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.4854 | test Neg: 0.3823\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | train Sqc: 0.5145 | loss: 1.2544\n",
      "epoch:    1 | step:  199 | N:  8 | train Sqc: 0.5083 | loss: 1.2523\n",
      "epoch:    1 | step:  299 | N:  8 | train Sqc: 0.4992 | loss: 1.2510\n",
      "epoch:    1 | step:  399 | N:  8 | train Sqc: 0.4958 | loss: 1.2502\n",
      "epoch:    1 | step:  499 | N:  8 | train Sqc: 0.4943 | loss: 1.2499\n",
      "epoch:    1 | step:  599 | N:  8 | train Sqc: 0.4996 | loss: 1.2503\n",
      "epoch:    1 | step:  699 | N:  8 | train Sqc: 0.4970 | loss: 1.2499\n",
      "epoch:    1 | step:  799 | N:  8 | train Sqc: 0.4963 | loss: 1.2499\n",
      "epoch:    1 | step:  899 | N:  8 | train Sqc: 0.4946 | loss: 1.2498\n",
      "epoch:    1 | step:  999 | N:  8 | train Sqc: 0.4929 | loss: 1.2495\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | test Sqc: 0.4931 | test Neg: 0.3817\n",
      "epoch:    1 | step:  199 | N:  8 | test Sqc: 0.4870 | test Neg: 0.3822\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | train Sqc: 0.5138 | loss: 1.2544\n",
      "epoch:    2 | step:  199 | N:  8 | train Sqc: 0.5086 | loss: 1.2523\n",
      "epoch:    2 | step:  299 | N:  8 | train Sqc: 0.4990 | loss: 1.2509\n",
      "epoch:    2 | step:  399 | N:  8 | train Sqc: 0.4955 | loss: 1.2502\n",
      "epoch:    2 | step:  499 | N:  8 | train Sqc: 0.4940 | loss: 1.2499\n",
      "epoch:    2 | step:  599 | N:  8 | train Sqc: 0.4991 | loss: 1.2502\n",
      "epoch:    2 | step:  699 | N:  8 | train Sqc: 0.4965 | loss: 1.2498\n",
      "epoch:    2 | step:  799 | N:  8 | train Sqc: 0.4950 | loss: 1.2496\n",
      "epoch:    2 | step:  899 | N:  8 | train Sqc: 0.4905 | loss: 1.2492\n",
      "epoch:    2 | step:  999 | N:  8 | train Sqc: 0.4890 | loss: 1.2489\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | test Sqc: 0.4926 | test Neg: 0.3818\n",
      "epoch:    2 | step:  199 | N:  8 | test Sqc: 0.4868 | test Neg: 0.3822\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | train Sqc: 0.4954 | loss: 1.2492\n",
      "epoch:    0 | step:  199 | N:  10 | train Sqc: 0.4954 | loss: 1.2496\n",
      "epoch:    0 | step:  299 | N:  10 | train Sqc: 0.4923 | loss: 1.2490\n",
      "epoch:    0 | step:  399 | N:  10 | train Sqc: 0.4980 | loss: 1.2497\n",
      "epoch:    0 | step:  499 | N:  10 | train Sqc: 0.4891 | loss: 1.2491\n",
      "epoch:    0 | step:  599 | N:  10 | train Sqc: 0.4896 | loss: 1.2490\n",
      "epoch:    0 | step:  699 | N:  10 | train Sqc: 0.4923 | loss: 1.2494\n",
      "epoch:    0 | step:  799 | N:  10 | train Sqc: 0.4908 | loss: 1.2492\n",
      "epoch:    0 | step:  899 | N:  10 | train Sqc: 0.4902 | loss: 1.2492\n",
      "epoch:    0 | step:  999 | N:  10 | train Sqc: 0.4875 | loss: 1.2488\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | test Sqc: 0.4781 | test Neg: 0.3816\n",
      "epoch:    0 | step:  199 | N:  10 | test Sqc: 0.4925 | test Neg: 0.3765\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  10 | train Sqc: 0.4931 | loss: 1.2490\n",
      "epoch:    1 | step:  199 | N:  10 | train Sqc: 0.4920 | loss: 1.2493\n",
      "epoch:    1 | step:  299 | N:  10 | train Sqc: 0.4892 | loss: 1.2487\n",
      "epoch:    1 | step:  399 | N:  10 | train Sqc: 0.4950 | loss: 1.2494\n",
      "epoch:    1 | step:  499 | N:  10 | train Sqc: 0.4860 | loss: 1.2488\n",
      "epoch:    1 | step:  599 | N:  10 | train Sqc: 0.4866 | loss: 1.2488\n",
      "epoch:    1 | step:  699 | N:  10 | train Sqc: 0.4891 | loss: 1.2491\n",
      "epoch:    1 | step:  799 | N:  10 | train Sqc: 0.4876 | loss: 1.2489\n",
      "epoch:    1 | step:  899 | N:  10 | train Sqc: 0.4868 | loss: 1.2489\n",
      "epoch:    1 | step:  999 | N:  10 | train Sqc: 0.4842 | loss: 1.2485\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  10 | test Sqc: 0.4771 | test Neg: 0.3814\n",
      "epoch:    1 | step:  199 | N:  10 | test Sqc: 0.4929 | test Neg: 0.3763\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  10 | train Sqc: 0.4966 | loss: 1.2497\n",
      "epoch:    2 | step:  199 | N:  10 | train Sqc: 0.4943 | loss: 1.2497\n",
      "epoch:    2 | step:  299 | N:  10 | train Sqc: 0.4902 | loss: 1.2489\n",
      "epoch:    2 | step:  399 | N:  10 | train Sqc: 0.4957 | loss: 1.2495\n",
      "epoch:    2 | step:  499 | N:  10 | train Sqc: 0.4862 | loss: 1.2489\n",
      "epoch:    2 | step:  599 | N:  10 | train Sqc: 0.4866 | loss: 1.2488\n",
      "epoch:    2 | step:  699 | N:  10 | train Sqc: 0.4888 | loss: 1.2492\n",
      "epoch:    2 | step:  799 | N:  10 | train Sqc: 0.4873 | loss: 1.2489\n",
      "epoch:    2 | step:  899 | N:  10 | train Sqc: 0.4863 | loss: 1.2489\n",
      "epoch:    2 | step:  999 | N:  10 | train Sqc: 0.4837 | loss: 1.2485\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  10 | test Sqc: 0.4763 | test Neg: 0.3821\n",
      "epoch:    2 | step:  199 | N:  10 | test Sqc: 0.4929 | test Neg: 0.3771\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  12 | train Sqc: 0.5435 | loss: 1.2589\n",
      "epoch:    0 | step:  199 | N:  12 | train Sqc: 0.5379 | loss: 1.2577\n",
      "epoch:    0 | step:  299 | N:  12 | train Sqc: 0.5378 | loss: 1.2577\n",
      "epoch:    0 | step:  399 | N:  12 | train Sqc: 0.5413 | loss: 1.2582\n",
      "epoch:    0 | step:  499 | N:  12 | train Sqc: 0.5445 | loss: 1.2585\n",
      "epoch:    0 | step:  599 | N:  12 | train Sqc: 0.5427 | loss: 1.2583\n",
      "epoch:    0 | step:  699 | N:  12 | train Sqc: 0.5428 | loss: 1.2583\n",
      "epoch:    0 | step:  799 | N:  12 | train Sqc: 0.5398 | loss: 1.2578\n",
      "epoch:    0 | step:  899 | N:  12 | train Sqc: 0.5412 | loss: 1.2581\n",
      "epoch:    0 | step:  999 | N:  12 | train Sqc: 0.5417 | loss: 1.2582\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  12 | test Sqc: 0.5578 | test Neg: 0.3592\n",
      "epoch:    0 | step:  199 | N:  12 | test Sqc: 0.5537 | test Neg: 0.3582\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  12 | train Sqc: 0.5400 | loss: 1.2583\n",
      "epoch:    1 | step:  199 | N:  12 | train Sqc: 0.5336 | loss: 1.2569\n",
      "epoch:    1 | step:  299 | N:  12 | train Sqc: 0.5342 | loss: 1.2571\n",
      "epoch:    1 | step:  399 | N:  12 | train Sqc: 0.5376 | loss: 1.2577\n",
      "epoch:    1 | step:  499 | N:  12 | train Sqc: 0.5409 | loss: 1.2579\n",
      "epoch:    1 | step:  599 | N:  12 | train Sqc: 0.5392 | loss: 1.2577\n",
      "epoch:    1 | step:  699 | N:  12 | train Sqc: 0.5394 | loss: 1.2577\n",
      "epoch:    1 | step:  799 | N:  12 | train Sqc: 0.5364 | loss: 1.2573\n",
      "epoch:    1 | step:  899 | N:  12 | train Sqc: 0.5378 | loss: 1.2576\n",
      "epoch:    1 | step:  999 | N:  12 | train Sqc: 0.5386 | loss: 1.2577\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  12 | test Sqc: 0.5590 | test Neg: 0.3592\n",
      "epoch:    1 | step:  199 | N:  12 | test Sqc: 0.5538 | test Neg: 0.3581\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  12 | train Sqc: 0.5395 | loss: 1.2582\n",
      "epoch:    2 | step:  199 | N:  12 | train Sqc: 0.5327 | loss: 1.2568\n",
      "epoch:    2 | step:  299 | N:  12 | train Sqc: 0.5330 | loss: 1.2570\n",
      "epoch:    2 | step:  399 | N:  12 | train Sqc: 0.5364 | loss: 1.2575\n",
      "epoch:    2 | step:  499 | N:  12 | train Sqc: 0.5395 | loss: 1.2577\n",
      "epoch:    2 | step:  599 | N:  12 | train Sqc: 0.5375 | loss: 1.2576\n",
      "epoch:    2 | step:  699 | N:  12 | train Sqc: 0.5375 | loss: 1.2575\n",
      "epoch:    2 | step:  799 | N:  12 | train Sqc: 0.5343 | loss: 1.2571\n",
      "epoch:    2 | step:  899 | N:  12 | train Sqc: 0.5356 | loss: 1.2574\n",
      "epoch:    2 | step:  999 | N:  12 | train Sqc: 0.5363 | loss: 1.2575\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  12 | test Sqc: 0.5679 | test Neg: 0.3566\n",
      "epoch:    2 | step:  199 | N:  12 | test Sqc: 0.5617 | test Neg: 0.3559\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  14 | train Sqc: 0.6699 | loss: 1.2779\n",
      "epoch:    0 | step:  199 | N:  14 | train Sqc: 0.6487 | loss: 1.2755\n",
      "epoch:    0 | step:  299 | N:  14 | train Sqc: 0.6516 | loss: 1.2760\n",
      "epoch:    0 | step:  399 | N:  14 | train Sqc: 0.6543 | loss: 1.2764\n",
      "epoch:    0 | step:  499 | N:  14 | train Sqc: 0.6558 | loss: 1.2769\n",
      "epoch:    0 | step:  599 | N:  14 | train Sqc: 0.6541 | loss: 1.2765\n",
      "epoch:    0 | step:  699 | N:  14 | train Sqc: 0.6515 | loss: 1.2761\n",
      "epoch:    0 | step:  799 | N:  14 | train Sqc: 0.6515 | loss: 1.2762\n",
      "epoch:    0 | step:  899 | N:  14 | train Sqc: 0.6514 | loss: 1.2762\n",
      "epoch:    0 | step:  999 | N:  14 | train Sqc: 0.6482 | loss: 1.2757\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  14 | test Sqc: 0.6471 | test Neg: 0.3257\n",
      "epoch:    0 | step:  199 | N:  14 | test Sqc: 0.6542 | test Neg: 0.3236\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  14 | train Sqc: 0.6649 | loss: 1.2771\n",
      "epoch:    1 | step:  199 | N:  14 | train Sqc: 0.6454 | loss: 1.2749\n",
      "epoch:    1 | step:  299 | N:  14 | train Sqc: 0.6475 | loss: 1.2755\n",
      "epoch:    1 | step:  399 | N:  14 | train Sqc: 0.6501 | loss: 1.2759\n",
      "epoch:    1 | step:  499 | N:  14 | train Sqc: 0.6517 | loss: 1.2764\n",
      "epoch:    1 | step:  599 | N:  14 | train Sqc: 0.6503 | loss: 1.2761\n",
      "epoch:    1 | step:  699 | N:  14 | train Sqc: 0.6482 | loss: 1.2758\n",
      "epoch:    1 | step:  799 | N:  14 | train Sqc: 0.6481 | loss: 1.2758\n",
      "epoch:    1 | step:  899 | N:  14 | train Sqc: 0.6479 | loss: 1.2758\n",
      "epoch:    1 | step:  999 | N:  14 | train Sqc: 0.6448 | loss: 1.2753\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  14 | test Sqc: 0.6496 | test Neg: 0.3254\n",
      "epoch:    1 | step:  199 | N:  14 | test Sqc: 0.6565 | test Neg: 0.3233\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  14 | train Sqc: 0.6614 | loss: 1.2766\n",
      "epoch:    2 | step:  199 | N:  14 | train Sqc: 0.6423 | loss: 1.2746\n",
      "epoch:    2 | step:  299 | N:  14 | train Sqc: 0.6441 | loss: 1.2751\n",
      "epoch:    2 | step:  399 | N:  14 | train Sqc: 0.6465 | loss: 1.2755\n",
      "epoch:    2 | step:  499 | N:  14 | train Sqc: 0.6479 | loss: 1.2759\n",
      "epoch:    2 | step:  599 | N:  14 | train Sqc: 0.6464 | loss: 1.2756\n",
      "epoch:    2 | step:  699 | N:  14 | train Sqc: 0.6438 | loss: 1.2753\n",
      "epoch:    2 | step:  799 | N:  14 | train Sqc: 0.6437 | loss: 1.2753\n",
      "epoch:    2 | step:  899 | N:  14 | train Sqc: 0.6439 | loss: 1.2753\n",
      "epoch:    2 | step:  999 | N:  14 | train Sqc: 0.6409 | loss: 1.2749\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  14 | test Sqc: 0.6507 | test Neg: 0.3250\n",
      "epoch:    2 | step:  199 | N:  14 | test Sqc: 0.6589 | test Neg: 0.3227\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  16 | train Sqc: 0.6911 | loss: 1.2840\n",
      "epoch:    0 | step:  199 | N:  16 | train Sqc: 0.6889 | loss: 1.2831\n",
      "epoch:    0 | step:  299 | N:  16 | train Sqc: 0.6909 | loss: 1.2833\n",
      "epoch:    0 | step:  399 | N:  16 | train Sqc: 0.6888 | loss: 1.2828\n",
      "epoch:    0 | step:  499 | N:  16 | train Sqc: 0.6858 | loss: 1.2826\n",
      "epoch:    0 | step:  599 | N:  16 | train Sqc: 0.6852 | loss: 1.2826\n",
      "epoch:    0 | step:  699 | N:  16 | train Sqc: 0.6834 | loss: 1.2823\n",
      "epoch:    0 | step:  799 | N:  16 | train Sqc: 0.6836 | loss: 1.2821\n",
      "epoch:    0 | step:  899 | N:  16 | train Sqc: 0.6794 | loss: 1.2817\n",
      "epoch:    0 | step:  999 | N:  16 | train Sqc: 0.6791 | loss: 1.2818\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  16 | test Sqc: 0.7009 | test Neg: 0.3054\n",
      "epoch:    0 | step:  199 | N:  16 | test Sqc: 0.7040 | test Neg: 0.3047\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  16 | train Sqc: 0.6863 | loss: 1.2833\n",
      "epoch:    1 | step:  199 | N:  16 | train Sqc: 0.6852 | loss: 1.2826\n",
      "epoch:    1 | step:  299 | N:  16 | train Sqc: 0.6865 | loss: 1.2828\n",
      "epoch:    1 | step:  399 | N:  16 | train Sqc: 0.6836 | loss: 1.2822\n",
      "epoch:    1 | step:  499 | N:  16 | train Sqc: 0.6803 | loss: 1.2819\n",
      "epoch:    1 | step:  599 | N:  16 | train Sqc: 0.6798 | loss: 1.2820\n",
      "epoch:    1 | step:  699 | N:  16 | train Sqc: 0.6783 | loss: 1.2817\n",
      "epoch:    1 | step:  799 | N:  16 | train Sqc: 0.6783 | loss: 1.2815\n",
      "epoch:    1 | step:  899 | N:  16 | train Sqc: 0.6742 | loss: 1.2812\n",
      "epoch:    1 | step:  999 | N:  16 | train Sqc: 0.6738 | loss: 1.2812\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  16 | test Sqc: 0.7029 | test Neg: 0.3060\n",
      "epoch:    1 | step:  199 | N:  16 | test Sqc: 0.7061 | test Neg: 0.3050\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  16 | train Sqc: 0.6800 | loss: 1.2825\n",
      "epoch:    2 | step:  199 | N:  16 | train Sqc: 0.6793 | loss: 1.2817\n",
      "epoch:    2 | step:  299 | N:  16 | train Sqc: 0.6810 | loss: 1.2820\n",
      "epoch:    2 | step:  399 | N:  16 | train Sqc: 0.6786 | loss: 1.2815\n",
      "epoch:    2 | step:  499 | N:  16 | train Sqc: 0.6755 | loss: 1.2813\n",
      "epoch:    2 | step:  599 | N:  16 | train Sqc: 0.6746 | loss: 1.2813\n",
      "epoch:    2 | step:  699 | N:  16 | train Sqc: 0.6731 | loss: 1.2810\n",
      "epoch:    2 | step:  799 | N:  16 | train Sqc: 0.6732 | loss: 1.2809\n",
      "epoch:    2 | step:  899 | N:  16 | train Sqc: 0.6694 | loss: 1.2805\n",
      "epoch:    2 | step:  999 | N:  16 | train Sqc: 0.6692 | loss: 1.2806\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  16 | test Sqc: 0.7066 | test Neg: 0.3045\n",
      "epoch:    2 | step:  199 | N:  16 | test Sqc: 0.7096 | test Neg: 0.3041\n"
     ]
    }
   ],
   "source": [
    "for N in range(4,18,2):\n",
    "    # if N > 4:\n",
    "    #     mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N-2}_pa.pt'))\n",
    "    #mdl.load_state_dict(torch.load(f'{file}/models/gpt_N={N}_pa.pt'))\n",
    "    mdl.load_state_dict(torch.load(f'{file}/models/gpt_pa.pt'))\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/post_selected/data_{N}pa.pickle', shuffle=True)\n",
    "    \n",
    "    prepseq, shadow_state, rhoS = prepseq[:600000], shadow_state[:600000], rhoS[:600000]\n",
    "    \n",
    "    prepseq = torch.cat([prepseq+2, torch.zeros(prepseq.shape[0], 14-prepseq.shape[1], dtype=prepseq.dtype), torch.ones(prepseq.shape[0], 1, dtype=prepseq.dtype)], 1)\n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "        \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, 15)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, 15)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'test Sa':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        # Train\n",
    "        if train:\n",
    "            print('='*50+'   Train   '+'='*50)\n",
    "            mdl.train()\n",
    "            for i in range(prepseq_train.shape[0]):\n",
    "                rhoC = mdl(prepseq_train[i])\n",
    "                l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "                optimizer.zero_grad()\n",
    "                probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "                loss = -probs.log().mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                l['loss'].append(loss.item())\n",
    "                if (i+1)%100 == 0:\n",
    "                    trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                    loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                    print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Sa'].append(Sa(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/gpt_N={N}_pa.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/gpt_N={N}_pa.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db531145-d589-4085-a3c8-3834e14b27a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
