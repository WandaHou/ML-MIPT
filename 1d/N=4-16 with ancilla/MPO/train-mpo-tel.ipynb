{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0825a41-2749-4cd7-aca5-23e995cd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import blogm, bSqc, Neg, Sa\n",
    "from mpo import tel_mpo\n",
    "import torch\n",
    "from math import prod\n",
    "from functools import reduce\n",
    "import pandas\n",
    "from utils import dtype, device, pauli, basis, torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86ee1a7-a7b8-4b18-9671-c08a589b4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "train, test = True, True\n",
    "file = f'seed{seed}'\n",
    "train_ratio = 5/6\n",
    "batch = 500\n",
    "\n",
    "mdl = tel_mpo(16, bond=10)\n",
    "total=0 # find size of the model\n",
    "for p in mdl.parameters():\n",
    "    total+=prod(p.shape)\n",
    "total#, True_fid(mdl, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fbbf84-c0ed-4d4a-a66c-06a80339b5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | train Sqc: 1.3189 | loss: 1.3808\n",
      "epoch:    0 | step:  199 | N:  4 | train Sqc: 1.2028 | loss: 1.3647\n",
      "epoch:    0 | step:  299 | N:  4 | train Sqc: 1.0759 | loss: 1.3461\n",
      "epoch:    0 | step:  399 | N:  4 | train Sqc: 0.9574 | loss: 1.3272\n",
      "epoch:    0 | step:  499 | N:  4 | train Sqc: 0.8580 | loss: 1.3108\n",
      "epoch:    0 | step:  599 | N:  4 | train Sqc: 0.7858 | loss: 1.2984\n",
      "epoch:    0 | step:  699 | N:  4 | train Sqc: 0.7313 | loss: 1.2891\n",
      "epoch:    0 | step:  799 | N:  4 | train Sqc: 0.6887 | loss: 1.2818\n",
      "epoch:    0 | step:  899 | N:  4 | train Sqc: 0.6551 | loss: 1.2761\n",
      "epoch:    0 | step:  999 | N:  4 | train Sqc: 0.6289 | loss: 1.2716\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  4 | test Sqc: 0.3713 | test Neg: 0.4154\n",
      "epoch:    0 | step:  199 | N:  4 | test Sqc: 0.3784 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | train Sqc: 0.3545 | loss: 1.2280\n",
      "epoch:    1 | step:  199 | N:  4 | train Sqc: 0.3696 | loss: 1.2296\n",
      "epoch:    1 | step:  299 | N:  4 | train Sqc: 0.3766 | loss: 1.2301\n",
      "epoch:    1 | step:  399 | N:  4 | train Sqc: 0.3834 | loss: 1.2310\n",
      "epoch:    1 | step:  499 | N:  4 | train Sqc: 0.3817 | loss: 1.2308\n",
      "epoch:    1 | step:  599 | N:  4 | train Sqc: 0.3836 | loss: 1.2309\n",
      "epoch:    1 | step:  699 | N:  4 | train Sqc: 0.3847 | loss: 1.2309\n",
      "epoch:    1 | step:  799 | N:  4 | train Sqc: 0.3847 | loss: 1.2308\n",
      "epoch:    1 | step:  899 | N:  4 | train Sqc: 0.3844 | loss: 1.2306\n",
      "epoch:    1 | step:  999 | N:  4 | train Sqc: 0.3849 | loss: 1.2307\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  4 | test Sqc: 0.3675 | test Neg: 0.4154\n",
      "epoch:    1 | step:  199 | N:  4 | test Sqc: 0.3757 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | train Sqc: 0.3513 | loss: 1.2278\n",
      "epoch:    2 | step:  199 | N:  4 | train Sqc: 0.3672 | loss: 1.2295\n",
      "epoch:    2 | step:  299 | N:  4 | train Sqc: 0.3745 | loss: 1.2300\n",
      "epoch:    2 | step:  399 | N:  4 | train Sqc: 0.3814 | loss: 1.2309\n",
      "epoch:    2 | step:  499 | N:  4 | train Sqc: 0.3799 | loss: 1.2307\n",
      "epoch:    2 | step:  599 | N:  4 | train Sqc: 0.3818 | loss: 1.2308\n",
      "epoch:    2 | step:  699 | N:  4 | train Sqc: 0.3830 | loss: 1.2308\n",
      "epoch:    2 | step:  799 | N:  4 | train Sqc: 0.3831 | loss: 1.2307\n",
      "epoch:    2 | step:  899 | N:  4 | train Sqc: 0.3829 | loss: 1.2306\n",
      "epoch:    2 | step:  999 | N:  4 | train Sqc: 0.3836 | loss: 1.2307\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  4 | test Sqc: 0.3676 | test Neg: 0.4154\n",
      "epoch:    2 | step:  199 | N:  4 | test Sqc: 0.3755 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | train Sqc: 0.3504 | loss: 1.2278\n",
      "epoch:    3 | step:  199 | N:  4 | train Sqc: 0.3666 | loss: 1.2295\n",
      "epoch:    3 | step:  299 | N:  4 | train Sqc: 0.3741 | loss: 1.2300\n",
      "epoch:    3 | step:  399 | N:  4 | train Sqc: 0.3809 | loss: 1.2309\n",
      "epoch:    3 | step:  499 | N:  4 | train Sqc: 0.3794 | loss: 1.2307\n",
      "epoch:    3 | step:  599 | N:  4 | train Sqc: 0.3813 | loss: 1.2308\n",
      "epoch:    3 | step:  699 | N:  4 | train Sqc: 0.3826 | loss: 1.2307\n",
      "epoch:    3 | step:  799 | N:  4 | train Sqc: 0.3826 | loss: 1.2307\n",
      "epoch:    3 | step:  899 | N:  4 | train Sqc: 0.3825 | loss: 1.2305\n",
      "epoch:    3 | step:  999 | N:  4 | train Sqc: 0.3831 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  4 | test Sqc: 0.3677 | test Neg: 0.4154\n",
      "epoch:    3 | step:  199 | N:  4 | test Sqc: 0.3752 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | train Sqc: 0.3498 | loss: 1.2278\n",
      "epoch:    4 | step:  199 | N:  4 | train Sqc: 0.3664 | loss: 1.2294\n",
      "epoch:    4 | step:  299 | N:  4 | train Sqc: 0.3739 | loss: 1.2300\n",
      "epoch:    4 | step:  399 | N:  4 | train Sqc: 0.3807 | loss: 1.2309\n",
      "epoch:    4 | step:  499 | N:  4 | train Sqc: 0.3792 | loss: 1.2307\n",
      "epoch:    4 | step:  599 | N:  4 | train Sqc: 0.3811 | loss: 1.2308\n",
      "epoch:    4 | step:  699 | N:  4 | train Sqc: 0.3823 | loss: 1.2307\n",
      "epoch:    4 | step:  799 | N:  4 | train Sqc: 0.3823 | loss: 1.2307\n",
      "epoch:    4 | step:  899 | N:  4 | train Sqc: 0.3822 | loss: 1.2305\n",
      "epoch:    4 | step:  999 | N:  4 | train Sqc: 0.3828 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  4 | test Sqc: 0.3678 | test Neg: 0.4154\n",
      "epoch:    4 | step:  199 | N:  4 | test Sqc: 0.3750 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | train Sqc: 0.3494 | loss: 1.2278\n",
      "epoch:    5 | step:  199 | N:  4 | train Sqc: 0.3663 | loss: 1.2294\n",
      "epoch:    5 | step:  299 | N:  4 | train Sqc: 0.3738 | loss: 1.2300\n",
      "epoch:    5 | step:  399 | N:  4 | train Sqc: 0.3805 | loss: 1.2309\n",
      "epoch:    5 | step:  499 | N:  4 | train Sqc: 0.3791 | loss: 1.2307\n",
      "epoch:    5 | step:  599 | N:  4 | train Sqc: 0.3809 | loss: 1.2308\n",
      "epoch:    5 | step:  699 | N:  4 | train Sqc: 0.3821 | loss: 1.2307\n",
      "epoch:    5 | step:  799 | N:  4 | train Sqc: 0.3822 | loss: 1.2307\n",
      "epoch:    5 | step:  899 | N:  4 | train Sqc: 0.3820 | loss: 1.2305\n",
      "epoch:    5 | step:  999 | N:  4 | train Sqc: 0.3826 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  4 | test Sqc: 0.3679 | test Neg: 0.4154\n",
      "epoch:    5 | step:  199 | N:  4 | test Sqc: 0.3748 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | train Sqc: 0.3492 | loss: 1.2278\n",
      "epoch:    6 | step:  199 | N:  4 | train Sqc: 0.3662 | loss: 1.2294\n",
      "epoch:    6 | step:  299 | N:  4 | train Sqc: 0.3737 | loss: 1.2300\n",
      "epoch:    6 | step:  399 | N:  4 | train Sqc: 0.3804 | loss: 1.2309\n",
      "epoch:    6 | step:  499 | N:  4 | train Sqc: 0.3790 | loss: 1.2307\n",
      "epoch:    6 | step:  599 | N:  4 | train Sqc: 0.3808 | loss: 1.2308\n",
      "epoch:    6 | step:  699 | N:  4 | train Sqc: 0.3820 | loss: 1.2307\n",
      "epoch:    6 | step:  799 | N:  4 | train Sqc: 0.3821 | loss: 1.2307\n",
      "epoch:    6 | step:  899 | N:  4 | train Sqc: 0.3819 | loss: 1.2305\n",
      "epoch:    6 | step:  999 | N:  4 | train Sqc: 0.3824 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  4 | test Sqc: 0.3679 | test Neg: 0.4155\n",
      "epoch:    6 | step:  199 | N:  4 | test Sqc: 0.3746 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | train Sqc: 0.3490 | loss: 1.2278\n",
      "epoch:    7 | step:  199 | N:  4 | train Sqc: 0.3662 | loss: 1.2294\n",
      "epoch:    7 | step:  299 | N:  4 | train Sqc: 0.3737 | loss: 1.2300\n",
      "epoch:    7 | step:  399 | N:  4 | train Sqc: 0.3803 | loss: 1.2309\n",
      "epoch:    7 | step:  499 | N:  4 | train Sqc: 0.3789 | loss: 1.2307\n",
      "epoch:    7 | step:  599 | N:  4 | train Sqc: 0.3807 | loss: 1.2308\n",
      "epoch:    7 | step:  699 | N:  4 | train Sqc: 0.3820 | loss: 1.2307\n",
      "epoch:    7 | step:  799 | N:  4 | train Sqc: 0.3820 | loss: 1.2307\n",
      "epoch:    7 | step:  899 | N:  4 | train Sqc: 0.3818 | loss: 1.2305\n",
      "epoch:    7 | step:  999 | N:  4 | train Sqc: 0.3823 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  4 | test Sqc: 0.3680 | test Neg: 0.4155\n",
      "epoch:    7 | step:  199 | N:  4 | test Sqc: 0.3745 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | train Sqc: 0.3488 | loss: 1.2278\n",
      "epoch:    8 | step:  199 | N:  4 | train Sqc: 0.3662 | loss: 1.2294\n",
      "epoch:    8 | step:  299 | N:  4 | train Sqc: 0.3736 | loss: 1.2300\n",
      "epoch:    8 | step:  399 | N:  4 | train Sqc: 0.3803 | loss: 1.2309\n",
      "epoch:    8 | step:  499 | N:  4 | train Sqc: 0.3788 | loss: 1.2307\n",
      "epoch:    8 | step:  599 | N:  4 | train Sqc: 0.3806 | loss: 1.2308\n",
      "epoch:    8 | step:  699 | N:  4 | train Sqc: 0.3819 | loss: 1.2307\n",
      "epoch:    8 | step:  799 | N:  4 | train Sqc: 0.3819 | loss: 1.2307\n",
      "epoch:    8 | step:  899 | N:  4 | train Sqc: 0.3817 | loss: 1.2305\n",
      "epoch:    8 | step:  999 | N:  4 | train Sqc: 0.3822 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  4 | test Sqc: 0.3680 | test Neg: 0.4155\n",
      "epoch:    8 | step:  199 | N:  4 | test Sqc: 0.3744 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | train Sqc: 0.3487 | loss: 1.2278\n",
      "epoch:    9 | step:  199 | N:  4 | train Sqc: 0.3662 | loss: 1.2294\n",
      "epoch:    9 | step:  299 | N:  4 | train Sqc: 0.3736 | loss: 1.2300\n",
      "epoch:    9 | step:  399 | N:  4 | train Sqc: 0.3802 | loss: 1.2309\n",
      "epoch:    9 | step:  499 | N:  4 | train Sqc: 0.3788 | loss: 1.2307\n",
      "epoch:    9 | step:  599 | N:  4 | train Sqc: 0.3806 | loss: 1.2308\n",
      "epoch:    9 | step:  699 | N:  4 | train Sqc: 0.3819 | loss: 1.2307\n",
      "epoch:    9 | step:  799 | N:  4 | train Sqc: 0.3819 | loss: 1.2307\n",
      "epoch:    9 | step:  899 | N:  4 | train Sqc: 0.3816 | loss: 1.2305\n",
      "epoch:    9 | step:  999 | N:  4 | train Sqc: 0.3822 | loss: 1.2306\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  4 | test Sqc: 0.3680 | test Neg: 0.4155\n",
      "epoch:    9 | step:  199 | N:  4 | test Sqc: 0.3743 | test Neg: 0.4141\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | train Sqc: 1.8060 | loss: 1.5208\n",
      "epoch:    0 | step:  199 | N:  6 | train Sqc: 1.5342 | loss: 1.4523\n",
      "epoch:    0 | step:  299 | N:  6 | train Sqc: 1.3505 | loss: 1.4142\n",
      "epoch:    0 | step:  399 | N:  6 | train Sqc: 1.2028 | loss: 1.3860\n",
      "epoch:    0 | step:  499 | N:  6 | train Sqc: 1.0826 | loss: 1.3632\n",
      "epoch:    0 | step:  599 | N:  6 | train Sqc: 0.9855 | loss: 1.3448\n",
      "epoch:    0 | step:  699 | N:  6 | train Sqc: 0.9128 | loss: 1.3307\n",
      "epoch:    0 | step:  799 | N:  6 | train Sqc: 0.8498 | loss: 1.3189\n",
      "epoch:    0 | step:  899 | N:  6 | train Sqc: 0.8023 | loss: 1.3097\n",
      "epoch:    0 | step:  999 | N:  6 | train Sqc: 0.7641 | loss: 1.3024\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  6 | test Sqc: 0.4411 | test Neg: 0.3997\n",
      "epoch:    0 | step:  199 | N:  6 | test Sqc: 0.4167 | test Neg: 0.4048\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | train Sqc: 0.4375 | loss: 1.2369\n",
      "epoch:    1 | step:  199 | N:  6 | train Sqc: 0.4212 | loss: 1.2347\n",
      "epoch:    1 | step:  299 | N:  6 | train Sqc: 0.4151 | loss: 1.2344\n",
      "epoch:    1 | step:  399 | N:  6 | train Sqc: 0.4072 | loss: 1.2336\n",
      "epoch:    1 | step:  499 | N:  6 | train Sqc: 0.4054 | loss: 1.2331\n",
      "epoch:    1 | step:  599 | N:  6 | train Sqc: 0.4045 | loss: 1.2334\n",
      "epoch:    1 | step:  699 | N:  6 | train Sqc: 0.4083 | loss: 1.2340\n",
      "epoch:    1 | step:  799 | N:  6 | train Sqc: 0.4042 | loss: 1.2337\n",
      "epoch:    1 | step:  899 | N:  6 | train Sqc: 0.4049 | loss: 1.2337\n",
      "epoch:    1 | step:  999 | N:  6 | train Sqc: 0.4054 | loss: 1.2339\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  6 | test Sqc: 0.4345 | test Neg: 0.4000\n",
      "epoch:    1 | step:  199 | N:  6 | test Sqc: 0.4086 | test Neg: 0.4050\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | train Sqc: 0.4321 | loss: 1.2361\n",
      "epoch:    2 | step:  199 | N:  6 | train Sqc: 0.4152 | loss: 1.2340\n",
      "epoch:    2 | step:  299 | N:  6 | train Sqc: 0.4096 | loss: 1.2338\n",
      "epoch:    2 | step:  399 | N:  6 | train Sqc: 0.4018 | loss: 1.2330\n",
      "epoch:    2 | step:  499 | N:  6 | train Sqc: 0.4001 | loss: 1.2326\n",
      "epoch:    2 | step:  599 | N:  6 | train Sqc: 0.3994 | loss: 1.2330\n",
      "epoch:    2 | step:  699 | N:  6 | train Sqc: 0.4034 | loss: 1.2336\n",
      "epoch:    2 | step:  799 | N:  6 | train Sqc: 0.3993 | loss: 1.2333\n",
      "epoch:    2 | step:  899 | N:  6 | train Sqc: 0.4004 | loss: 1.2333\n",
      "epoch:    2 | step:  999 | N:  6 | train Sqc: 0.4011 | loss: 1.2336\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  6 | test Sqc: 0.4324 | test Neg: 0.4000\n",
      "epoch:    2 | step:  199 | N:  6 | test Sqc: 0.4068 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  6 | train Sqc: 0.4299 | loss: 1.2359\n",
      "epoch:    3 | step:  199 | N:  6 | train Sqc: 0.4127 | loss: 1.2338\n",
      "epoch:    3 | step:  299 | N:  6 | train Sqc: 0.4074 | loss: 1.2337\n",
      "epoch:    3 | step:  399 | N:  6 | train Sqc: 0.3995 | loss: 1.2329\n",
      "epoch:    3 | step:  499 | N:  6 | train Sqc: 0.3976 | loss: 1.2325\n",
      "epoch:    3 | step:  599 | N:  6 | train Sqc: 0.3970 | loss: 1.2328\n",
      "epoch:    3 | step:  699 | N:  6 | train Sqc: 0.4011 | loss: 1.2335\n",
      "epoch:    3 | step:  799 | N:  6 | train Sqc: 0.3970 | loss: 1.2332\n",
      "epoch:    3 | step:  899 | N:  6 | train Sqc: 0.3982 | loss: 1.2332\n",
      "epoch:    3 | step:  999 | N:  6 | train Sqc: 0.3990 | loss: 1.2335\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  6 | test Sqc: 0.4312 | test Neg: 0.4001\n",
      "epoch:    3 | step:  199 | N:  6 | test Sqc: 0.4061 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  6 | train Sqc: 0.4285 | loss: 1.2358\n",
      "epoch:    4 | step:  199 | N:  6 | train Sqc: 0.4110 | loss: 1.2338\n",
      "epoch:    4 | step:  299 | N:  6 | train Sqc: 0.4060 | loss: 1.2336\n",
      "epoch:    4 | step:  399 | N:  6 | train Sqc: 0.3980 | loss: 1.2329\n",
      "epoch:    4 | step:  499 | N:  6 | train Sqc: 0.3959 | loss: 1.2324\n",
      "epoch:    4 | step:  599 | N:  6 | train Sqc: 0.3953 | loss: 1.2328\n",
      "epoch:    4 | step:  699 | N:  6 | train Sqc: 0.3996 | loss: 1.2334\n",
      "epoch:    4 | step:  799 | N:  6 | train Sqc: 0.3955 | loss: 1.2331\n",
      "epoch:    4 | step:  899 | N:  6 | train Sqc: 0.3968 | loss: 1.2332\n",
      "epoch:    4 | step:  999 | N:  6 | train Sqc: 0.3976 | loss: 1.2334\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  6 | test Sqc: 0.4304 | test Neg: 0.4001\n",
      "epoch:    4 | step:  199 | N:  6 | test Sqc: 0.4058 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  6 | train Sqc: 0.4276 | loss: 1.2358\n",
      "epoch:    5 | step:  199 | N:  6 | train Sqc: 0.4099 | loss: 1.2337\n",
      "epoch:    5 | step:  299 | N:  6 | train Sqc: 0.4049 | loss: 1.2336\n",
      "epoch:    5 | step:  399 | N:  6 | train Sqc: 0.3969 | loss: 1.2328\n",
      "epoch:    5 | step:  499 | N:  6 | train Sqc: 0.3946 | loss: 1.2324\n",
      "epoch:    5 | step:  599 | N:  6 | train Sqc: 0.3942 | loss: 1.2327\n",
      "epoch:    5 | step:  699 | N:  6 | train Sqc: 0.3985 | loss: 1.2334\n",
      "epoch:    5 | step:  799 | N:  6 | train Sqc: 0.3944 | loss: 1.2331\n",
      "epoch:    5 | step:  899 | N:  6 | train Sqc: 0.3957 | loss: 1.2331\n",
      "epoch:    5 | step:  999 | N:  6 | train Sqc: 0.3967 | loss: 1.2334\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  6 | test Sqc: 0.4298 | test Neg: 0.4001\n",
      "epoch:    5 | step:  199 | N:  6 | test Sqc: 0.4056 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  6 | train Sqc: 0.4269 | loss: 1.2358\n",
      "epoch:    6 | step:  199 | N:  6 | train Sqc: 0.4090 | loss: 1.2337\n",
      "epoch:    6 | step:  299 | N:  6 | train Sqc: 0.4042 | loss: 1.2335\n",
      "epoch:    6 | step:  399 | N:  6 | train Sqc: 0.3961 | loss: 1.2328\n",
      "epoch:    6 | step:  499 | N:  6 | train Sqc: 0.3937 | loss: 1.2323\n",
      "epoch:    6 | step:  599 | N:  6 | train Sqc: 0.3933 | loss: 1.2327\n",
      "epoch:    6 | step:  699 | N:  6 | train Sqc: 0.3977 | loss: 1.2333\n",
      "epoch:    6 | step:  799 | N:  6 | train Sqc: 0.3936 | loss: 1.2331\n",
      "epoch:    6 | step:  899 | N:  6 | train Sqc: 0.3949 | loss: 1.2331\n",
      "epoch:    6 | step:  999 | N:  6 | train Sqc: 0.3959 | loss: 1.2333\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  6 | test Sqc: 0.4294 | test Neg: 0.4001\n",
      "epoch:    6 | step:  199 | N:  6 | test Sqc: 0.4055 | test Neg: 0.4051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  6 | train Sqc: 0.4264 | loss: 1.2357\n",
      "epoch:    7 | step:  199 | N:  6 | train Sqc: 0.4083 | loss: 1.2337\n",
      "epoch:    7 | step:  299 | N:  6 | train Sqc: 0.4036 | loss: 1.2335\n",
      "epoch:    7 | step:  399 | N:  6 | train Sqc: 0.3955 | loss: 1.2328\n",
      "epoch:    7 | step:  499 | N:  6 | train Sqc: 0.3930 | loss: 1.2323\n",
      "epoch:    7 | step:  599 | N:  6 | train Sqc: 0.3926 | loss: 1.2327\n",
      "epoch:    7 | step:  699 | N:  6 | train Sqc: 0.3970 | loss: 1.2333\n",
      "epoch:    7 | step:  799 | N:  6 | train Sqc: 0.3930 | loss: 1.2331\n",
      "epoch:    7 | step:  899 | N:  6 | train Sqc: 0.3943 | loss: 1.2331\n",
      "epoch:    7 | step:  999 | N:  6 | train Sqc: 0.3953 | loss: 1.2333\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  6 | test Sqc: 0.4290 | test Neg: 0.4001\n",
      "epoch:    7 | step:  199 | N:  6 | test Sqc: 0.4055 | test Neg: 0.4052\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  6 | train Sqc: 0.4260 | loss: 1.2357\n",
      "epoch:    8 | step:  199 | N:  6 | train Sqc: 0.4078 | loss: 1.2336\n",
      "epoch:    8 | step:  299 | N:  6 | train Sqc: 0.4031 | loss: 1.2335\n",
      "epoch:    8 | step:  399 | N:  6 | train Sqc: 0.3950 | loss: 1.2328\n",
      "epoch:    8 | step:  499 | N:  6 | train Sqc: 0.3924 | loss: 1.2323\n",
      "epoch:    8 | step:  599 | N:  6 | train Sqc: 0.3920 | loss: 1.2327\n",
      "epoch:    8 | step:  699 | N:  6 | train Sqc: 0.3965 | loss: 1.2333\n",
      "epoch:    8 | step:  799 | N:  6 | train Sqc: 0.3925 | loss: 1.2330\n",
      "epoch:    8 | step:  899 | N:  6 | train Sqc: 0.3938 | loss: 1.2331\n",
      "epoch:    8 | step:  999 | N:  6 | train Sqc: 0.3949 | loss: 1.2333\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  6 | test Sqc: 0.4288 | test Neg: 0.4001\n",
      "epoch:    8 | step:  199 | N:  6 | test Sqc: 0.4055 | test Neg: 0.4052\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  6 | train Sqc: 0.4256 | loss: 1.2357\n",
      "epoch:    9 | step:  199 | N:  6 | train Sqc: 0.4073 | loss: 1.2336\n",
      "epoch:    9 | step:  299 | N:  6 | train Sqc: 0.4027 | loss: 1.2335\n",
      "epoch:    9 | step:  399 | N:  6 | train Sqc: 0.3946 | loss: 1.2327\n",
      "epoch:    9 | step:  499 | N:  6 | train Sqc: 0.3919 | loss: 1.2323\n",
      "epoch:    9 | step:  599 | N:  6 | train Sqc: 0.3916 | loss: 1.2326\n",
      "epoch:    9 | step:  699 | N:  6 | train Sqc: 0.3961 | loss: 1.2333\n",
      "epoch:    9 | step:  799 | N:  6 | train Sqc: 0.3920 | loss: 1.2330\n",
      "epoch:    9 | step:  899 | N:  6 | train Sqc: 0.3934 | loss: 1.2330\n",
      "epoch:    9 | step:  999 | N:  6 | train Sqc: 0.3945 | loss: 1.2333\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  6 | test Sqc: 0.4286 | test Neg: 0.4001\n",
      "epoch:    9 | step:  199 | N:  6 | test Sqc: 0.4054 | test Neg: 0.4052\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | train Sqc: 1.6120 | loss: 1.4745\n",
      "epoch:    0 | step:  199 | N:  8 | train Sqc: 1.3504 | loss: 1.4146\n",
      "epoch:    0 | step:  299 | N:  8 | train Sqc: 1.1741 | loss: 1.3802\n",
      "epoch:    0 | step:  399 | N:  8 | train Sqc: 1.0421 | loss: 1.3550\n",
      "epoch:    0 | step:  499 | N:  8 | train Sqc: 0.9490 | loss: 1.3367\n",
      "epoch:    0 | step:  599 | N:  8 | train Sqc: 0.8865 | loss: 1.3237\n",
      "epoch:    0 | step:  699 | N:  8 | train Sqc: 0.8342 | loss: 1.3135\n",
      "epoch:    0 | step:  799 | N:  8 | train Sqc: 0.7931 | loss: 1.3059\n",
      "epoch:    0 | step:  899 | N:  8 | train Sqc: 0.7589 | loss: 1.2995\n",
      "epoch:    0 | step:  999 | N:  8 | train Sqc: 0.7314 | loss: 1.2945\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  8 | test Sqc: 0.5082 | test Neg: 0.3806\n",
      "epoch:    0 | step:  199 | N:  8 | test Sqc: 0.5070 | test Neg: 0.3811\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | train Sqc: 0.5376 | loss: 1.2571\n",
      "epoch:    1 | step:  199 | N:  8 | train Sqc: 0.5267 | loss: 1.2546\n",
      "epoch:    1 | step:  299 | N:  8 | train Sqc: 0.5168 | loss: 1.2532\n",
      "epoch:    1 | step:  399 | N:  8 | train Sqc: 0.5143 | loss: 1.2524\n",
      "epoch:    1 | step:  499 | N:  8 | train Sqc: 0.5129 | loss: 1.2520\n",
      "epoch:    1 | step:  599 | N:  8 | train Sqc: 0.5186 | loss: 1.2523\n",
      "epoch:    1 | step:  699 | N:  8 | train Sqc: 0.5157 | loss: 1.2520\n",
      "epoch:    1 | step:  799 | N:  8 | train Sqc: 0.5126 | loss: 1.2517\n",
      "epoch:    1 | step:  899 | N:  8 | train Sqc: 0.5084 | loss: 1.2513\n",
      "epoch:    1 | step:  999 | N:  8 | train Sqc: 0.5054 | loss: 1.2510\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  8 | test Sqc: 0.5007 | test Neg: 0.3812\n",
      "epoch:    1 | step:  199 | N:  8 | test Sqc: 0.5000 | test Neg: 0.3818\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | train Sqc: 0.5315 | loss: 1.2564\n",
      "epoch:    2 | step:  199 | N:  8 | train Sqc: 0.5202 | loss: 1.2540\n",
      "epoch:    2 | step:  299 | N:  8 | train Sqc: 0.5110 | loss: 1.2526\n",
      "epoch:    2 | step:  399 | N:  8 | train Sqc: 0.5087 | loss: 1.2519\n",
      "epoch:    2 | step:  499 | N:  8 | train Sqc: 0.5075 | loss: 1.2515\n",
      "epoch:    2 | step:  599 | N:  8 | train Sqc: 0.5134 | loss: 1.2518\n",
      "epoch:    2 | step:  699 | N:  8 | train Sqc: 0.5105 | loss: 1.2515\n",
      "epoch:    2 | step:  799 | N:  8 | train Sqc: 0.5078 | loss: 1.2513\n",
      "epoch:    2 | step:  899 | N:  8 | train Sqc: 0.5038 | loss: 1.2508\n",
      "epoch:    2 | step:  999 | N:  8 | train Sqc: 0.5011 | loss: 1.2505\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  8 | test Sqc: 0.4974 | test Neg: 0.3814\n",
      "epoch:    2 | step:  199 | N:  8 | test Sqc: 0.4969 | test Neg: 0.3819\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  8 | train Sqc: 0.5294 | loss: 1.2562\n",
      "epoch:    3 | step:  199 | N:  8 | train Sqc: 0.5181 | loss: 1.2538\n",
      "epoch:    3 | step:  299 | N:  8 | train Sqc: 0.5090 | loss: 1.2525\n",
      "epoch:    3 | step:  399 | N:  8 | train Sqc: 0.5066 | loss: 1.2517\n",
      "epoch:    3 | step:  499 | N:  8 | train Sqc: 0.5054 | loss: 1.2513\n",
      "epoch:    3 | step:  599 | N:  8 | train Sqc: 0.5113 | loss: 1.2517\n",
      "epoch:    3 | step:  699 | N:  8 | train Sqc: 0.5084 | loss: 1.2513\n",
      "epoch:    3 | step:  799 | N:  8 | train Sqc: 0.5059 | loss: 1.2511\n",
      "epoch:    3 | step:  899 | N:  8 | train Sqc: 0.5019 | loss: 1.2507\n",
      "epoch:    3 | step:  999 | N:  8 | train Sqc: 0.4993 | loss: 1.2504\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  8 | test Sqc: 0.4957 | test Neg: 0.3814\n",
      "epoch:    3 | step:  199 | N:  8 | test Sqc: 0.4952 | test Neg: 0.3820\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  8 | train Sqc: 0.5283 | loss: 1.2561\n",
      "epoch:    4 | step:  199 | N:  8 | train Sqc: 0.5171 | loss: 1.2537\n",
      "epoch:    4 | step:  299 | N:  8 | train Sqc: 0.5080 | loss: 1.2524\n",
      "epoch:    4 | step:  399 | N:  8 | train Sqc: 0.5055 | loss: 1.2516\n",
      "epoch:    4 | step:  499 | N:  8 | train Sqc: 0.5043 | loss: 1.2512\n",
      "epoch:    4 | step:  599 | N:  8 | train Sqc: 0.5101 | loss: 1.2516\n",
      "epoch:    4 | step:  699 | N:  8 | train Sqc: 0.5072 | loss: 1.2512\n",
      "epoch:    4 | step:  799 | N:  8 | train Sqc: 0.5048 | loss: 1.2510\n",
      "epoch:    4 | step:  899 | N:  8 | train Sqc: 0.5008 | loss: 1.2506\n",
      "epoch:    4 | step:  999 | N:  8 | train Sqc: 0.4983 | loss: 1.2503\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  8 | test Sqc: 0.4946 | test Neg: 0.3815\n",
      "epoch:    4 | step:  199 | N:  8 | test Sqc: 0.4942 | test Neg: 0.3820\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  8 | train Sqc: 0.5276 | loss: 1.2560\n",
      "epoch:    5 | step:  199 | N:  8 | train Sqc: 0.5165 | loss: 1.2537\n",
      "epoch:    5 | step:  299 | N:  8 | train Sqc: 0.5073 | loss: 1.2523\n",
      "epoch:    5 | step:  399 | N:  8 | train Sqc: 0.5047 | loss: 1.2515\n",
      "epoch:    5 | step:  499 | N:  8 | train Sqc: 0.5036 | loss: 1.2511\n",
      "epoch:    5 | step:  599 | N:  8 | train Sqc: 0.5093 | loss: 1.2515\n",
      "epoch:    5 | step:  699 | N:  8 | train Sqc: 0.5064 | loss: 1.2511\n",
      "epoch:    5 | step:  799 | N:  8 | train Sqc: 0.5041 | loss: 1.2509\n",
      "epoch:    5 | step:  899 | N:  8 | train Sqc: 0.5000 | loss: 1.2505\n",
      "epoch:    5 | step:  999 | N:  8 | train Sqc: 0.4976 | loss: 1.2502\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  8 | test Sqc: 0.4940 | test Neg: 0.3815\n",
      "epoch:    5 | step:  199 | N:  8 | test Sqc: 0.4935 | test Neg: 0.3820\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  8 | train Sqc: 0.5269 | loss: 1.2559\n",
      "epoch:    6 | step:  199 | N:  8 | train Sqc: 0.5161 | loss: 1.2536\n",
      "epoch:    6 | step:  299 | N:  8 | train Sqc: 0.5068 | loss: 1.2522\n",
      "epoch:    6 | step:  399 | N:  8 | train Sqc: 0.5042 | loss: 1.2515\n",
      "epoch:    6 | step:  499 | N:  8 | train Sqc: 0.5030 | loss: 1.2511\n",
      "epoch:    6 | step:  599 | N:  8 | train Sqc: 0.5087 | loss: 1.2514\n",
      "epoch:    6 | step:  699 | N:  8 | train Sqc: 0.5058 | loss: 1.2511\n",
      "epoch:    6 | step:  799 | N:  8 | train Sqc: 0.5035 | loss: 1.2509\n",
      "epoch:    6 | step:  899 | N:  8 | train Sqc: 0.4994 | loss: 1.2504\n",
      "epoch:    6 | step:  999 | N:  8 | train Sqc: 0.4971 | loss: 1.2502\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  8 | test Sqc: 0.4935 | test Neg: 0.3815\n",
      "epoch:    6 | step:  199 | N:  8 | test Sqc: 0.4930 | test Neg: 0.3821\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  8 | train Sqc: 0.5264 | loss: 1.2559\n",
      "epoch:    7 | step:  199 | N:  8 | train Sqc: 0.5157 | loss: 1.2536\n",
      "epoch:    7 | step:  299 | N:  8 | train Sqc: 0.5064 | loss: 1.2522\n",
      "epoch:    7 | step:  399 | N:  8 | train Sqc: 0.5037 | loss: 1.2514\n",
      "epoch:    7 | step:  499 | N:  8 | train Sqc: 0.5025 | loss: 1.2511\n",
      "epoch:    7 | step:  599 | N:  8 | train Sqc: 0.5082 | loss: 1.2514\n",
      "epoch:    7 | step:  699 | N:  8 | train Sqc: 0.5053 | loss: 1.2510\n",
      "epoch:    7 | step:  799 | N:  8 | train Sqc: 0.5030 | loss: 1.2508\n",
      "epoch:    7 | step:  899 | N:  8 | train Sqc: 0.4990 | loss: 1.2504\n",
      "epoch:    7 | step:  999 | N:  8 | train Sqc: 0.4966 | loss: 1.2501\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  8 | test Sqc: 0.4932 | test Neg: 0.3815\n",
      "epoch:    7 | step:  199 | N:  8 | test Sqc: 0.4926 | test Neg: 0.3821\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  8 | train Sqc: 0.5259 | loss: 1.2558\n",
      "epoch:    8 | step:  199 | N:  8 | train Sqc: 0.5153 | loss: 1.2535\n",
      "epoch:    8 | step:  299 | N:  8 | train Sqc: 0.5061 | loss: 1.2522\n",
      "epoch:    8 | step:  399 | N:  8 | train Sqc: 0.5032 | loss: 1.2514\n",
      "epoch:    8 | step:  499 | N:  8 | train Sqc: 0.5021 | loss: 1.2510\n",
      "epoch:    8 | step:  599 | N:  8 | train Sqc: 0.5077 | loss: 1.2513\n",
      "epoch:    8 | step:  699 | N:  8 | train Sqc: 0.5049 | loss: 1.2510\n",
      "epoch:    8 | step:  799 | N:  8 | train Sqc: 0.5026 | loss: 1.2508\n",
      "epoch:    8 | step:  899 | N:  8 | train Sqc: 0.4985 | loss: 1.2504\n",
      "epoch:    8 | step:  999 | N:  8 | train Sqc: 0.4963 | loss: 1.2501\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  8 | test Sqc: 0.4929 | test Neg: 0.3815\n",
      "epoch:    8 | step:  199 | N:  8 | test Sqc: 0.4923 | test Neg: 0.3821\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  8 | train Sqc: 0.5255 | loss: 1.2558\n",
      "epoch:    9 | step:  199 | N:  8 | train Sqc: 0.5150 | loss: 1.2535\n",
      "epoch:    9 | step:  299 | N:  8 | train Sqc: 0.5058 | loss: 1.2521\n",
      "epoch:    9 | step:  399 | N:  8 | train Sqc: 0.5029 | loss: 1.2513\n",
      "epoch:    9 | step:  499 | N:  8 | train Sqc: 0.5017 | loss: 1.2510\n",
      "epoch:    9 | step:  599 | N:  8 | train Sqc: 0.5074 | loss: 1.2513\n",
      "epoch:    9 | step:  699 | N:  8 | train Sqc: 0.5045 | loss: 1.2510\n",
      "epoch:    9 | step:  799 | N:  8 | train Sqc: 0.5023 | loss: 1.2508\n",
      "epoch:    9 | step:  899 | N:  8 | train Sqc: 0.4982 | loss: 1.2503\n",
      "epoch:    9 | step:  999 | N:  8 | train Sqc: 0.4959 | loss: 1.2501\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  8 | test Sqc: 0.4927 | test Neg: 0.3815\n",
      "epoch:    9 | step:  199 | N:  8 | test Sqc: 0.4920 | test Neg: 0.3821\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | train Sqc: 1.7573 | loss: 1.4781\n",
      "epoch:    0 | step:  199 | N:  10 | train Sqc: 1.4535 | loss: 1.4194\n",
      "epoch:    0 | step:  299 | N:  10 | train Sqc: 1.2596 | loss: 1.3855\n",
      "epoch:    0 | step:  399 | N:  10 | train Sqc: 1.1198 | loss: 1.3611\n",
      "epoch:    0 | step:  499 | N:  10 | train Sqc: 1.0102 | loss: 1.3418\n",
      "epoch:    0 | step:  599 | N:  10 | train Sqc: 0.9339 | loss: 1.3279\n",
      "epoch:    0 | step:  699 | N:  10 | train Sqc: 0.8802 | loss: 1.3179\n",
      "epoch:    0 | step:  799 | N:  10 | train Sqc: 0.8336 | loss: 1.3097\n",
      "epoch:    0 | step:  899 | N:  10 | train Sqc: 0.7969 | loss: 1.3032\n",
      "epoch:    0 | step:  999 | N:  10 | train Sqc: 0.7662 | loss: 1.2978\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  10 | test Sqc: 0.4940 | test Neg: 0.3834\n",
      "epoch:    0 | step:  199 | N:  10 | test Sqc: 0.5093 | test Neg: 0.3787\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  10 | train Sqc: 0.5162 | loss: 1.2515\n",
      "epoch:    1 | step:  199 | N:  10 | train Sqc: 0.5199 | loss: 1.2525\n",
      "epoch:    1 | step:  299 | N:  10 | train Sqc: 0.5157 | loss: 1.2516\n",
      "epoch:    1 | step:  399 | N:  10 | train Sqc: 0.5182 | loss: 1.2522\n",
      "epoch:    1 | step:  499 | N:  10 | train Sqc: 0.5099 | loss: 1.2515\n",
      "epoch:    1 | step:  599 | N:  10 | train Sqc: 0.5094 | loss: 1.2515\n",
      "epoch:    1 | step:  699 | N:  10 | train Sqc: 0.5126 | loss: 1.2519\n",
      "epoch:    1 | step:  799 | N:  10 | train Sqc: 0.5101 | loss: 1.2516\n",
      "epoch:    1 | step:  899 | N:  10 | train Sqc: 0.5084 | loss: 1.2515\n",
      "epoch:    1 | step:  999 | N:  10 | train Sqc: 0.5055 | loss: 1.2511\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  10 | test Sqc: 0.4842 | test Neg: 0.3842\n",
      "epoch:    1 | step:  199 | N:  10 | test Sqc: 0.4994 | test Neg: 0.3794\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  10 | train Sqc: 0.5102 | loss: 1.2509\n",
      "epoch:    2 | step:  199 | N:  10 | train Sqc: 0.5132 | loss: 1.2518\n",
      "epoch:    2 | step:  299 | N:  10 | train Sqc: 0.5090 | loss: 1.2510\n",
      "epoch:    2 | step:  399 | N:  10 | train Sqc: 0.5126 | loss: 1.2516\n",
      "epoch:    2 | step:  499 | N:  10 | train Sqc: 0.5044 | loss: 1.2510\n",
      "epoch:    2 | step:  599 | N:  10 | train Sqc: 0.5039 | loss: 1.2509\n",
      "epoch:    2 | step:  699 | N:  10 | train Sqc: 0.5072 | loss: 1.2514\n",
      "epoch:    2 | step:  799 | N:  10 | train Sqc: 0.5049 | loss: 1.2511\n",
      "epoch:    2 | step:  899 | N:  10 | train Sqc: 0.5035 | loss: 1.2511\n",
      "epoch:    2 | step:  999 | N:  10 | train Sqc: 0.5009 | loss: 1.2507\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  10 | test Sqc: 0.4812 | test Neg: 0.3844\n",
      "epoch:    2 | step:  199 | N:  10 | test Sqc: 0.4968 | test Neg: 0.3796\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  10 | train Sqc: 0.5076 | loss: 1.2507\n",
      "epoch:    3 | step:  199 | N:  10 | train Sqc: 0.5103 | loss: 1.2516\n",
      "epoch:    3 | step:  299 | N:  10 | train Sqc: 0.5062 | loss: 1.2508\n",
      "epoch:    3 | step:  399 | N:  10 | train Sqc: 0.5103 | loss: 1.2514\n",
      "epoch:    3 | step:  499 | N:  10 | train Sqc: 0.5021 | loss: 1.2508\n",
      "epoch:    3 | step:  599 | N:  10 | train Sqc: 0.5016 | loss: 1.2507\n",
      "epoch:    3 | step:  699 | N:  10 | train Sqc: 0.5049 | loss: 1.2512\n",
      "epoch:    3 | step:  799 | N:  10 | train Sqc: 0.5027 | loss: 1.2510\n",
      "epoch:    3 | step:  899 | N:  10 | train Sqc: 0.5015 | loss: 1.2509\n",
      "epoch:    3 | step:  999 | N:  10 | train Sqc: 0.4990 | loss: 1.2505\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  10 | test Sqc: 0.4796 | test Neg: 0.3845\n",
      "epoch:    3 | step:  199 | N:  10 | test Sqc: 0.4956 | test Neg: 0.3796\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  10 | train Sqc: 0.5061 | loss: 1.2506\n",
      "epoch:    4 | step:  199 | N:  10 | train Sqc: 0.5085 | loss: 1.2515\n",
      "epoch:    4 | step:  299 | N:  10 | train Sqc: 0.5046 | loss: 1.2506\n",
      "epoch:    4 | step:  399 | N:  10 | train Sqc: 0.5090 | loss: 1.2513\n",
      "epoch:    4 | step:  499 | N:  10 | train Sqc: 0.5007 | loss: 1.2507\n",
      "epoch:    4 | step:  599 | N:  10 | train Sqc: 0.5003 | loss: 1.2506\n",
      "epoch:    4 | step:  699 | N:  10 | train Sqc: 0.5035 | loss: 1.2511\n",
      "epoch:    4 | step:  799 | N:  10 | train Sqc: 0.5015 | loss: 1.2509\n",
      "epoch:    4 | step:  899 | N:  10 | train Sqc: 0.5003 | loss: 1.2508\n",
      "epoch:    4 | step:  999 | N:  10 | train Sqc: 0.4978 | loss: 1.2504\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  10 | test Sqc: 0.4786 | test Neg: 0.3845\n",
      "epoch:    4 | step:  199 | N:  10 | test Sqc: 0.4950 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  10 | train Sqc: 0.5051 | loss: 1.2505\n",
      "epoch:    5 | step:  199 | N:  10 | train Sqc: 0.5073 | loss: 1.2514\n",
      "epoch:    5 | step:  299 | N:  10 | train Sqc: 0.5035 | loss: 1.2506\n",
      "epoch:    5 | step:  399 | N:  10 | train Sqc: 0.5080 | loss: 1.2512\n",
      "epoch:    5 | step:  499 | N:  10 | train Sqc: 0.4998 | loss: 1.2506\n",
      "epoch:    5 | step:  599 | N:  10 | train Sqc: 0.4994 | loss: 1.2506\n",
      "epoch:    5 | step:  699 | N:  10 | train Sqc: 0.5026 | loss: 1.2510\n",
      "epoch:    5 | step:  799 | N:  10 | train Sqc: 0.5006 | loss: 1.2508\n",
      "epoch:    5 | step:  899 | N:  10 | train Sqc: 0.4995 | loss: 1.2507\n",
      "epoch:    5 | step:  999 | N:  10 | train Sqc: 0.4971 | loss: 1.2503\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  10 | test Sqc: 0.4779 | test Neg: 0.3845\n",
      "epoch:    5 | step:  199 | N:  10 | test Sqc: 0.4945 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  10 | train Sqc: 0.5044 | loss: 1.2504\n",
      "epoch:    6 | step:  199 | N:  10 | train Sqc: 0.5063 | loss: 1.2513\n",
      "epoch:    6 | step:  299 | N:  10 | train Sqc: 0.5027 | loss: 1.2505\n",
      "epoch:    6 | step:  399 | N:  10 | train Sqc: 0.5074 | loss: 1.2512\n",
      "epoch:    6 | step:  499 | N:  10 | train Sqc: 0.4992 | loss: 1.2506\n",
      "epoch:    6 | step:  599 | N:  10 | train Sqc: 0.4988 | loss: 1.2505\n",
      "epoch:    6 | step:  699 | N:  10 | train Sqc: 0.5019 | loss: 1.2510\n",
      "epoch:    6 | step:  799 | N:  10 | train Sqc: 0.5000 | loss: 1.2507\n",
      "epoch:    6 | step:  899 | N:  10 | train Sqc: 0.4989 | loss: 1.2507\n",
      "epoch:    6 | step:  999 | N:  10 | train Sqc: 0.4965 | loss: 1.2503\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  10 | test Sqc: 0.4774 | test Neg: 0.3846\n",
      "epoch:    6 | step:  199 | N:  10 | test Sqc: 0.4942 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  10 | train Sqc: 0.5038 | loss: 1.2504\n",
      "epoch:    7 | step:  199 | N:  10 | train Sqc: 0.5056 | loss: 1.2513\n",
      "epoch:    7 | step:  299 | N:  10 | train Sqc: 0.5021 | loss: 1.2505\n",
      "epoch:    7 | step:  399 | N:  10 | train Sqc: 0.5069 | loss: 1.2511\n",
      "epoch:    7 | step:  499 | N:  10 | train Sqc: 0.4987 | loss: 1.2505\n",
      "epoch:    7 | step:  599 | N:  10 | train Sqc: 0.4982 | loss: 1.2505\n",
      "epoch:    7 | step:  699 | N:  10 | train Sqc: 0.5014 | loss: 1.2509\n",
      "epoch:    7 | step:  799 | N:  10 | train Sqc: 0.4995 | loss: 1.2507\n",
      "epoch:    7 | step:  899 | N:  10 | train Sqc: 0.4984 | loss: 1.2506\n",
      "epoch:    7 | step:  999 | N:  10 | train Sqc: 0.4961 | loss: 1.2502\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  10 | test Sqc: 0.4770 | test Neg: 0.3846\n",
      "epoch:    7 | step:  199 | N:  10 | test Sqc: 0.4940 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  10 | train Sqc: 0.5033 | loss: 1.2503\n",
      "epoch:    8 | step:  199 | N:  10 | train Sqc: 0.5050 | loss: 1.2512\n",
      "epoch:    8 | step:  299 | N:  10 | train Sqc: 0.5016 | loss: 1.2504\n",
      "epoch:    8 | step:  399 | N:  10 | train Sqc: 0.5065 | loss: 1.2511\n",
      "epoch:    8 | step:  499 | N:  10 | train Sqc: 0.4982 | loss: 1.2505\n",
      "epoch:    8 | step:  599 | N:  10 | train Sqc: 0.4978 | loss: 1.2504\n",
      "epoch:    8 | step:  699 | N:  10 | train Sqc: 0.5010 | loss: 1.2509\n",
      "epoch:    8 | step:  799 | N:  10 | train Sqc: 0.4991 | loss: 1.2507\n",
      "epoch:    8 | step:  899 | N:  10 | train Sqc: 0.4980 | loss: 1.2506\n",
      "epoch:    8 | step:  999 | N:  10 | train Sqc: 0.4957 | loss: 1.2502\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  10 | test Sqc: 0.4767 | test Neg: 0.3846\n",
      "epoch:    8 | step:  199 | N:  10 | test Sqc: 0.4938 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  10 | train Sqc: 0.5030 | loss: 1.2503\n",
      "epoch:    9 | step:  199 | N:  10 | train Sqc: 0.5046 | loss: 1.2512\n",
      "epoch:    9 | step:  299 | N:  10 | train Sqc: 0.5012 | loss: 1.2504\n",
      "epoch:    9 | step:  399 | N:  10 | train Sqc: 0.5062 | loss: 1.2511\n",
      "epoch:    9 | step:  499 | N:  10 | train Sqc: 0.4979 | loss: 1.2505\n",
      "epoch:    9 | step:  599 | N:  10 | train Sqc: 0.4975 | loss: 1.2504\n",
      "epoch:    9 | step:  699 | N:  10 | train Sqc: 0.5006 | loss: 1.2509\n",
      "epoch:    9 | step:  799 | N:  10 | train Sqc: 0.4987 | loss: 1.2506\n",
      "epoch:    9 | step:  899 | N:  10 | train Sqc: 0.4977 | loss: 1.2506\n",
      "epoch:    9 | step:  999 | N:  10 | train Sqc: 0.4954 | loss: 1.2502\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  10 | test Sqc: 0.4764 | test Neg: 0.3846\n",
      "epoch:    9 | step:  199 | N:  10 | test Sqc: 0.4936 | test Neg: 0.3797\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  12 | train Sqc: 1.6474 | loss: 1.4690\n",
      "epoch:    0 | step:  199 | N:  12 | train Sqc: 1.4207 | loss: 1.4208\n",
      "epoch:    0 | step:  299 | N:  12 | train Sqc: 1.2675 | loss: 1.3919\n",
      "epoch:    0 | step:  399 | N:  12 | train Sqc: 1.1461 | loss: 1.3698\n",
      "epoch:    0 | step:  499 | N:  12 | train Sqc: 1.0516 | loss: 1.3521\n",
      "epoch:    0 | step:  599 | N:  12 | train Sqc: 0.9775 | loss: 1.3383\n",
      "epoch:    0 | step:  699 | N:  12 | train Sqc: 0.9194 | loss: 1.3277\n",
      "epoch:    0 | step:  799 | N:  12 | train Sqc: 0.8732 | loss: 1.3192\n",
      "epoch:    0 | step:  899 | N:  12 | train Sqc: 0.8412 | loss: 1.3131\n",
      "epoch:    0 | step:  999 | N:  12 | train Sqc: 0.8136 | loss: 1.3080\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  12 | test Sqc: 0.5777 | test Neg: 0.3579\n",
      "epoch:    0 | step:  199 | N:  12 | test Sqc: 0.5728 | test Neg: 0.3569\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  12 | train Sqc: 0.5713 | loss: 1.2623\n",
      "epoch:    1 | step:  199 | N:  12 | train Sqc: 0.5641 | loss: 1.2608\n",
      "epoch:    1 | step:  299 | N:  12 | train Sqc: 0.5615 | loss: 1.2606\n",
      "epoch:    1 | step:  399 | N:  12 | train Sqc: 0.5646 | loss: 1.2613\n",
      "epoch:    1 | step:  499 | N:  12 | train Sqc: 0.5669 | loss: 1.2615\n",
      "epoch:    1 | step:  599 | N:  12 | train Sqc: 0.5663 | loss: 1.2613\n",
      "epoch:    1 | step:  699 | N:  12 | train Sqc: 0.5645 | loss: 1.2612\n",
      "epoch:    1 | step:  799 | N:  12 | train Sqc: 0.5610 | loss: 1.2607\n",
      "epoch:    1 | step:  899 | N:  12 | train Sqc: 0.5623 | loss: 1.2610\n",
      "epoch:    1 | step:  999 | N:  12 | train Sqc: 0.5620 | loss: 1.2610\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  12 | test Sqc: 0.5693 | test Neg: 0.3587\n",
      "epoch:    1 | step:  199 | N:  12 | test Sqc: 0.5649 | test Neg: 0.3577\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  12 | train Sqc: 0.5628 | loss: 1.2613\n",
      "epoch:    2 | step:  199 | N:  12 | train Sqc: 0.5561 | loss: 1.2598\n",
      "epoch:    2 | step:  299 | N:  12 | train Sqc: 0.5537 | loss: 1.2598\n",
      "epoch:    2 | step:  399 | N:  12 | train Sqc: 0.5573 | loss: 1.2605\n",
      "epoch:    2 | step:  499 | N:  12 | train Sqc: 0.5602 | loss: 1.2607\n",
      "epoch:    2 | step:  599 | N:  12 | train Sqc: 0.5597 | loss: 1.2606\n",
      "epoch:    2 | step:  699 | N:  12 | train Sqc: 0.5586 | loss: 1.2605\n",
      "epoch:    2 | step:  799 | N:  12 | train Sqc: 0.5554 | loss: 1.2601\n",
      "epoch:    2 | step:  899 | N:  12 | train Sqc: 0.5568 | loss: 1.2604\n",
      "epoch:    2 | step:  999 | N:  12 | train Sqc: 0.5568 | loss: 1.2604\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  12 | test Sqc: 0.5661 | test Neg: 0.3589\n",
      "epoch:    2 | step:  199 | N:  12 | test Sqc: 0.5628 | test Neg: 0.3579\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  12 | train Sqc: 0.5587 | loss: 1.2610\n",
      "epoch:    3 | step:  199 | N:  12 | train Sqc: 0.5526 | loss: 1.2595\n",
      "epoch:    3 | step:  299 | N:  12 | train Sqc: 0.5504 | loss: 1.2595\n",
      "epoch:    3 | step:  399 | N:  12 | train Sqc: 0.5542 | loss: 1.2602\n",
      "epoch:    3 | step:  499 | N:  12 | train Sqc: 0.5573 | loss: 1.2605\n",
      "epoch:    3 | step:  599 | N:  12 | train Sqc: 0.5568 | loss: 1.2603\n",
      "epoch:    3 | step:  699 | N:  12 | train Sqc: 0.5560 | loss: 1.2603\n",
      "epoch:    3 | step:  799 | N:  12 | train Sqc: 0.5529 | loss: 1.2599\n",
      "epoch:    3 | step:  899 | N:  12 | train Sqc: 0.5543 | loss: 1.2602\n",
      "epoch:    3 | step:  999 | N:  12 | train Sqc: 0.5544 | loss: 1.2602\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  12 | test Sqc: 0.5645 | test Neg: 0.3591\n",
      "epoch:    3 | step:  199 | N:  12 | test Sqc: 0.5621 | test Neg: 0.3580\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  12 | train Sqc: 0.5563 | loss: 1.2608\n",
      "epoch:    4 | step:  199 | N:  12 | train Sqc: 0.5507 | loss: 1.2594\n",
      "epoch:    4 | step:  299 | N:  12 | train Sqc: 0.5487 | loss: 1.2594\n",
      "epoch:    4 | step:  399 | N:  12 | train Sqc: 0.5525 | loss: 1.2601\n",
      "epoch:    4 | step:  499 | N:  12 | train Sqc: 0.5557 | loss: 1.2603\n",
      "epoch:    4 | step:  599 | N:  12 | train Sqc: 0.5552 | loss: 1.2602\n",
      "epoch:    4 | step:  699 | N:  12 | train Sqc: 0.5546 | loss: 1.2602\n",
      "epoch:    4 | step:  799 | N:  12 | train Sqc: 0.5515 | loss: 1.2597\n",
      "epoch:    4 | step:  899 | N:  12 | train Sqc: 0.5529 | loss: 1.2600\n",
      "epoch:    4 | step:  999 | N:  12 | train Sqc: 0.5532 | loss: 1.2601\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  12 | test Sqc: 0.5637 | test Neg: 0.3591\n",
      "epoch:    4 | step:  199 | N:  12 | test Sqc: 0.5618 | test Neg: 0.3581\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  12 | train Sqc: 0.5548 | loss: 1.2607\n",
      "epoch:    5 | step:  199 | N:  12 | train Sqc: 0.5495 | loss: 1.2593\n",
      "epoch:    5 | step:  299 | N:  12 | train Sqc: 0.5476 | loss: 1.2593\n",
      "epoch:    5 | step:  399 | N:  12 | train Sqc: 0.5514 | loss: 1.2600\n",
      "epoch:    5 | step:  499 | N:  12 | train Sqc: 0.5547 | loss: 1.2602\n",
      "epoch:    5 | step:  599 | N:  12 | train Sqc: 0.5541 | loss: 1.2601\n",
      "epoch:    5 | step:  699 | N:  12 | train Sqc: 0.5536 | loss: 1.2601\n",
      "epoch:    5 | step:  799 | N:  12 | train Sqc: 0.5506 | loss: 1.2596\n",
      "epoch:    5 | step:  899 | N:  12 | train Sqc: 0.5520 | loss: 1.2600\n",
      "epoch:    5 | step:  999 | N:  12 | train Sqc: 0.5523 | loss: 1.2600\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  12 | test Sqc: 0.5632 | test Neg: 0.3592\n",
      "epoch:    5 | step:  199 | N:  12 | test Sqc: 0.5616 | test Neg: 0.3581\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  12 | train Sqc: 0.5538 | loss: 1.2606\n",
      "epoch:    6 | step:  199 | N:  12 | train Sqc: 0.5487 | loss: 1.2592\n",
      "epoch:    6 | step:  299 | N:  12 | train Sqc: 0.5469 | loss: 1.2592\n",
      "epoch:    6 | step:  399 | N:  12 | train Sqc: 0.5507 | loss: 1.2599\n",
      "epoch:    6 | step:  499 | N:  12 | train Sqc: 0.5540 | loss: 1.2602\n",
      "epoch:    6 | step:  599 | N:  12 | train Sqc: 0.5534 | loss: 1.2600\n",
      "epoch:    6 | step:  699 | N:  12 | train Sqc: 0.5530 | loss: 1.2600\n",
      "epoch:    6 | step:  799 | N:  12 | train Sqc: 0.5499 | loss: 1.2596\n",
      "epoch:    6 | step:  899 | N:  12 | train Sqc: 0.5514 | loss: 1.2599\n",
      "epoch:    6 | step:  999 | N:  12 | train Sqc: 0.5518 | loss: 1.2599\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  12 | test Sqc: 0.5629 | test Neg: 0.3592\n",
      "epoch:    6 | step:  199 | N:  12 | test Sqc: 0.5615 | test Neg: 0.3582\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  12 | train Sqc: 0.5531 | loss: 1.2605\n",
      "epoch:    7 | step:  199 | N:  12 | train Sqc: 0.5481 | loss: 1.2592\n",
      "epoch:    7 | step:  299 | N:  12 | train Sqc: 0.5464 | loss: 1.2592\n",
      "epoch:    7 | step:  399 | N:  12 | train Sqc: 0.5501 | loss: 1.2598\n",
      "epoch:    7 | step:  499 | N:  12 | train Sqc: 0.5535 | loss: 1.2601\n",
      "epoch:    7 | step:  599 | N:  12 | train Sqc: 0.5528 | loss: 1.2600\n",
      "epoch:    7 | step:  699 | N:  12 | train Sqc: 0.5524 | loss: 1.2599\n",
      "epoch:    7 | step:  799 | N:  12 | train Sqc: 0.5494 | loss: 1.2595\n",
      "epoch:    7 | step:  899 | N:  12 | train Sqc: 0.5509 | loss: 1.2598\n",
      "epoch:    7 | step:  999 | N:  12 | train Sqc: 0.5513 | loss: 1.2599\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  12 | test Sqc: 0.5626 | test Neg: 0.3592\n",
      "epoch:    7 | step:  199 | N:  12 | test Sqc: 0.5615 | test Neg: 0.3582\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  12 | train Sqc: 0.5525 | loss: 1.2605\n",
      "epoch:    8 | step:  199 | N:  12 | train Sqc: 0.5476 | loss: 1.2591\n",
      "epoch:    8 | step:  299 | N:  12 | train Sqc: 0.5459 | loss: 1.2591\n",
      "epoch:    8 | step:  399 | N:  12 | train Sqc: 0.5496 | loss: 1.2598\n",
      "epoch:    8 | step:  499 | N:  12 | train Sqc: 0.5530 | loss: 1.2601\n",
      "epoch:    8 | step:  599 | N:  12 | train Sqc: 0.5524 | loss: 1.2599\n",
      "epoch:    8 | step:  699 | N:  12 | train Sqc: 0.5520 | loss: 1.2599\n",
      "epoch:    8 | step:  799 | N:  12 | train Sqc: 0.5490 | loss: 1.2595\n",
      "epoch:    8 | step:  899 | N:  12 | train Sqc: 0.5505 | loss: 1.2598\n",
      "epoch:    8 | step:  999 | N:  12 | train Sqc: 0.5509 | loss: 1.2598\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  12 | test Sqc: 0.5625 | test Neg: 0.3592\n",
      "epoch:    8 | step:  199 | N:  12 | test Sqc: 0.5614 | test Neg: 0.3582\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  12 | train Sqc: 0.5521 | loss: 1.2605\n",
      "epoch:    9 | step:  199 | N:  12 | train Sqc: 0.5473 | loss: 1.2591\n",
      "epoch:    9 | step:  299 | N:  12 | train Sqc: 0.5456 | loss: 1.2591\n",
      "epoch:    9 | step:  399 | N:  12 | train Sqc: 0.5492 | loss: 1.2598\n",
      "epoch:    9 | step:  499 | N:  12 | train Sqc: 0.5527 | loss: 1.2600\n",
      "epoch:    9 | step:  599 | N:  12 | train Sqc: 0.5520 | loss: 1.2599\n",
      "epoch:    9 | step:  699 | N:  12 | train Sqc: 0.5517 | loss: 1.2598\n",
      "epoch:    9 | step:  799 | N:  12 | train Sqc: 0.5486 | loss: 1.2594\n",
      "epoch:    9 | step:  899 | N:  12 | train Sqc: 0.5502 | loss: 1.2598\n",
      "epoch:    9 | step:  999 | N:  12 | train Sqc: 0.5506 | loss: 1.2598\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  12 | test Sqc: 0.5623 | test Neg: 0.3592\n",
      "epoch:    9 | step:  199 | N:  12 | test Sqc: 0.5614 | test Neg: 0.3582\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  14 | train Sqc: 1.6513 | loss: 1.4672\n",
      "epoch:    0 | step:  199 | N:  14 | train Sqc: 1.4243 | loss: 1.4190\n",
      "epoch:    0 | step:  299 | N:  14 | train Sqc: 1.2845 | loss: 1.3925\n",
      "epoch:    0 | step:  399 | N:  14 | train Sqc: 1.1806 | loss: 1.3733\n",
      "epoch:    0 | step:  499 | N:  14 | train Sqc: 1.0994 | loss: 1.3583\n",
      "epoch:    0 | step:  599 | N:  14 | train Sqc: 1.0353 | loss: 1.3463\n",
      "epoch:    0 | step:  699 | N:  14 | train Sqc: 0.9838 | loss: 1.3368\n",
      "epoch:    0 | step:  799 | N:  14 | train Sqc: 0.9461 | loss: 1.3299\n",
      "epoch:    0 | step:  899 | N:  14 | train Sqc: 0.9167 | loss: 1.3243\n",
      "epoch:    0 | step:  999 | N:  14 | train Sqc: 0.8901 | loss: 1.3194\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  14 | test Sqc: 0.6641 | test Neg: 0.3252\n",
      "epoch:    0 | step:  199 | N:  14 | test Sqc: 0.6714 | test Neg: 0.3226\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  14 | train Sqc: 0.6931 | loss: 1.2814\n",
      "epoch:    1 | step:  199 | N:  14 | train Sqc: 0.6683 | loss: 1.2788\n",
      "epoch:    1 | step:  299 | N:  14 | train Sqc: 0.6712 | loss: 1.2793\n",
      "epoch:    1 | step:  399 | N:  14 | train Sqc: 0.6747 | loss: 1.2796\n",
      "epoch:    1 | step:  499 | N:  14 | train Sqc: 0.6756 | loss: 1.2801\n",
      "epoch:    1 | step:  599 | N:  14 | train Sqc: 0.6739 | loss: 1.2798\n",
      "epoch:    1 | step:  699 | N:  14 | train Sqc: 0.6707 | loss: 1.2793\n",
      "epoch:    1 | step:  799 | N:  14 | train Sqc: 0.6706 | loss: 1.2793\n",
      "epoch:    1 | step:  899 | N:  14 | train Sqc: 0.6702 | loss: 1.2792\n",
      "epoch:    1 | step:  999 | N:  14 | train Sqc: 0.6670 | loss: 1.2787\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  14 | test Sqc: 0.6577 | test Neg: 0.3258\n",
      "epoch:    1 | step:  199 | N:  14 | test Sqc: 0.6651 | test Neg: 0.3232\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  14 | train Sqc: 0.6849 | loss: 1.2805\n",
      "epoch:    2 | step:  199 | N:  14 | train Sqc: 0.6616 | loss: 1.2780\n",
      "epoch:    2 | step:  299 | N:  14 | train Sqc: 0.6652 | loss: 1.2785\n",
      "epoch:    2 | step:  399 | N:  14 | train Sqc: 0.6686 | loss: 1.2789\n",
      "epoch:    2 | step:  499 | N:  14 | train Sqc: 0.6697 | loss: 1.2795\n",
      "epoch:    2 | step:  599 | N:  14 | train Sqc: 0.6680 | loss: 1.2792\n",
      "epoch:    2 | step:  699 | N:  14 | train Sqc: 0.6652 | loss: 1.2788\n",
      "epoch:    2 | step:  799 | N:  14 | train Sqc: 0.6653 | loss: 1.2788\n",
      "epoch:    2 | step:  899 | N:  14 | train Sqc: 0.6650 | loss: 1.2787\n",
      "epoch:    2 | step:  999 | N:  14 | train Sqc: 0.6620 | loss: 1.2782\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  14 | test Sqc: 0.6560 | test Neg: 0.3259\n",
      "epoch:    2 | step:  199 | N:  14 | test Sqc: 0.6634 | test Neg: 0.3233\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  14 | train Sqc: 0.6824 | loss: 1.2802\n",
      "epoch:    3 | step:  199 | N:  14 | train Sqc: 0.6598 | loss: 1.2777\n",
      "epoch:    3 | step:  299 | N:  14 | train Sqc: 0.6636 | loss: 1.2783\n",
      "epoch:    3 | step:  399 | N:  14 | train Sqc: 0.6665 | loss: 1.2787\n",
      "epoch:    3 | step:  499 | N:  14 | train Sqc: 0.6676 | loss: 1.2792\n",
      "epoch:    3 | step:  599 | N:  14 | train Sqc: 0.6659 | loss: 1.2790\n",
      "epoch:    3 | step:  699 | N:  14 | train Sqc: 0.6631 | loss: 1.2786\n",
      "epoch:    3 | step:  799 | N:  14 | train Sqc: 0.6633 | loss: 1.2786\n",
      "epoch:    3 | step:  899 | N:  14 | train Sqc: 0.6630 | loss: 1.2785\n",
      "epoch:    3 | step:  999 | N:  14 | train Sqc: 0.6600 | loss: 1.2780\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  14 | test Sqc: 0.6551 | test Neg: 0.3260\n",
      "epoch:    3 | step:  199 | N:  14 | test Sqc: 0.6626 | test Neg: 0.3234\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  14 | train Sqc: 0.6812 | loss: 1.2801\n",
      "epoch:    4 | step:  199 | N:  14 | train Sqc: 0.6590 | loss: 1.2776\n",
      "epoch:    4 | step:  299 | N:  14 | train Sqc: 0.6628 | loss: 1.2782\n",
      "epoch:    4 | step:  399 | N:  14 | train Sqc: 0.6654 | loss: 1.2786\n",
      "epoch:    4 | step:  499 | N:  14 | train Sqc: 0.6664 | loss: 1.2791\n",
      "epoch:    4 | step:  599 | N:  14 | train Sqc: 0.6648 | loss: 1.2788\n",
      "epoch:    4 | step:  699 | N:  14 | train Sqc: 0.6620 | loss: 1.2784\n",
      "epoch:    4 | step:  799 | N:  14 | train Sqc: 0.6622 | loss: 1.2785\n",
      "epoch:    4 | step:  899 | N:  14 | train Sqc: 0.6618 | loss: 1.2784\n",
      "epoch:    4 | step:  999 | N:  14 | train Sqc: 0.6588 | loss: 1.2779\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  14 | test Sqc: 0.6544 | test Neg: 0.3261\n",
      "epoch:    4 | step:  199 | N:  14 | test Sqc: 0.6620 | test Neg: 0.3234\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  14 | train Sqc: 0.6805 | loss: 1.2800\n",
      "epoch:    5 | step:  199 | N:  14 | train Sqc: 0.6586 | loss: 1.2775\n",
      "epoch:    5 | step:  299 | N:  14 | train Sqc: 0.6623 | loss: 1.2781\n",
      "epoch:    5 | step:  399 | N:  14 | train Sqc: 0.6647 | loss: 1.2785\n",
      "epoch:    5 | step:  499 | N:  14 | train Sqc: 0.6656 | loss: 1.2790\n",
      "epoch:    5 | step:  599 | N:  14 | train Sqc: 0.6640 | loss: 1.2788\n",
      "epoch:    5 | step:  699 | N:  14 | train Sqc: 0.6613 | loss: 1.2784\n",
      "epoch:    5 | step:  799 | N:  14 | train Sqc: 0.6614 | loss: 1.2784\n",
      "epoch:    5 | step:  899 | N:  14 | train Sqc: 0.6610 | loss: 1.2783\n",
      "epoch:    5 | step:  999 | N:  14 | train Sqc: 0.6581 | loss: 1.2778\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  14 | test Sqc: 0.6540 | test Neg: 0.3261\n",
      "epoch:    5 | step:  199 | N:  14 | test Sqc: 0.6616 | test Neg: 0.3235\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  14 | train Sqc: 0.6799 | loss: 1.2799\n",
      "epoch:    6 | step:  199 | N:  14 | train Sqc: 0.6584 | loss: 1.2774\n",
      "epoch:    6 | step:  299 | N:  14 | train Sqc: 0.6620 | loss: 1.2780\n",
      "epoch:    6 | step:  399 | N:  14 | train Sqc: 0.6641 | loss: 1.2784\n",
      "epoch:    6 | step:  499 | N:  14 | train Sqc: 0.6651 | loss: 1.2790\n",
      "epoch:    6 | step:  599 | N:  14 | train Sqc: 0.6635 | loss: 1.2787\n",
      "epoch:    6 | step:  699 | N:  14 | train Sqc: 0.6608 | loss: 1.2783\n",
      "epoch:    6 | step:  799 | N:  14 | train Sqc: 0.6608 | loss: 1.2783\n",
      "epoch:    6 | step:  899 | N:  14 | train Sqc: 0.6605 | loss: 1.2782\n",
      "epoch:    6 | step:  999 | N:  14 | train Sqc: 0.6575 | loss: 1.2778\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  14 | test Sqc: 0.6537 | test Neg: 0.3261\n",
      "epoch:    6 | step:  199 | N:  14 | test Sqc: 0.6614 | test Neg: 0.3235\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  14 | train Sqc: 0.6795 | loss: 1.2798\n",
      "epoch:    7 | step:  199 | N:  14 | train Sqc: 0.6582 | loss: 1.2774\n",
      "epoch:    7 | step:  299 | N:  14 | train Sqc: 0.6617 | loss: 1.2780\n",
      "epoch:    7 | step:  399 | N:  14 | train Sqc: 0.6637 | loss: 1.2783\n",
      "epoch:    7 | step:  499 | N:  14 | train Sqc: 0.6646 | loss: 1.2789\n",
      "epoch:    7 | step:  599 | N:  14 | train Sqc: 0.6631 | loss: 1.2787\n",
      "epoch:    7 | step:  699 | N:  14 | train Sqc: 0.6605 | loss: 1.2782\n",
      "epoch:    7 | step:  799 | N:  14 | train Sqc: 0.6604 | loss: 1.2783\n",
      "epoch:    7 | step:  899 | N:  14 | train Sqc: 0.6601 | loss: 1.2782\n",
      "epoch:    7 | step:  999 | N:  14 | train Sqc: 0.6571 | loss: 1.2777\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  14 | test Sqc: 0.6534 | test Neg: 0.3262\n",
      "epoch:    7 | step:  199 | N:  14 | test Sqc: 0.6612 | test Neg: 0.3235\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  14 | train Sqc: 0.6791 | loss: 1.2798\n",
      "epoch:    8 | step:  199 | N:  14 | train Sqc: 0.6580 | loss: 1.2774\n",
      "epoch:    8 | step:  299 | N:  14 | train Sqc: 0.6614 | loss: 1.2779\n",
      "epoch:    8 | step:  399 | N:  14 | train Sqc: 0.6634 | loss: 1.2783\n",
      "epoch:    8 | step:  499 | N:  14 | train Sqc: 0.6643 | loss: 1.2789\n",
      "epoch:    8 | step:  599 | N:  14 | train Sqc: 0.6627 | loss: 1.2786\n",
      "epoch:    8 | step:  699 | N:  14 | train Sqc: 0.6601 | loss: 1.2782\n",
      "epoch:    8 | step:  799 | N:  14 | train Sqc: 0.6601 | loss: 1.2782\n",
      "epoch:    8 | step:  899 | N:  14 | train Sqc: 0.6597 | loss: 1.2781\n",
      "epoch:    8 | step:  999 | N:  14 | train Sqc: 0.6567 | loss: 1.2777\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  14 | test Sqc: 0.6532 | test Neg: 0.3262\n",
      "epoch:    8 | step:  199 | N:  14 | test Sqc: 0.6610 | test Neg: 0.3235\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  14 | train Sqc: 0.6788 | loss: 1.2797\n",
      "epoch:    9 | step:  199 | N:  14 | train Sqc: 0.6579 | loss: 1.2773\n",
      "epoch:    9 | step:  299 | N:  14 | train Sqc: 0.6612 | loss: 1.2779\n",
      "epoch:    9 | step:  399 | N:  14 | train Sqc: 0.6631 | loss: 1.2783\n",
      "epoch:    9 | step:  499 | N:  14 | train Sqc: 0.6640 | loss: 1.2788\n",
      "epoch:    9 | step:  599 | N:  14 | train Sqc: 0.6625 | loss: 1.2786\n",
      "epoch:    9 | step:  699 | N:  14 | train Sqc: 0.6599 | loss: 1.2782\n",
      "epoch:    9 | step:  799 | N:  14 | train Sqc: 0.6598 | loss: 1.2782\n",
      "epoch:    9 | step:  899 | N:  14 | train Sqc: 0.6594 | loss: 1.2781\n",
      "epoch:    9 | step:  999 | N:  14 | train Sqc: 0.6564 | loss: 1.2776\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  14 | test Sqc: 0.6531 | test Neg: 0.3262\n",
      "epoch:    9 | step:  199 | N:  14 | test Sqc: 0.6609 | test Neg: 0.3235\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    0 | step:   99 | N:  16 | train Sqc: 1.6614 | loss: 1.4747\n",
      "epoch:    0 | step:  199 | N:  16 | train Sqc: 1.4966 | loss: 1.4327\n",
      "epoch:    0 | step:  299 | N:  16 | train Sqc: 1.3892 | loss: 1.4103\n",
      "epoch:    0 | step:  399 | N:  16 | train Sqc: 1.3011 | loss: 1.3939\n",
      "epoch:    0 | step:  499 | N:  16 | train Sqc: 1.2240 | loss: 1.3801\n",
      "epoch:    0 | step:  599 | N:  16 | train Sqc: 1.1556 | loss: 1.3679\n",
      "epoch:    0 | step:  699 | N:  16 | train Sqc: 1.0983 | loss: 1.3575\n",
      "epoch:    0 | step:  799 | N:  16 | train Sqc: 1.0520 | loss: 1.3490\n",
      "epoch:    0 | step:  899 | N:  16 | train Sqc: 1.0120 | loss: 1.3418\n",
      "epoch:    0 | step:  999 | N:  16 | train Sqc: 0.9818 | loss: 1.3364\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    0 | step:   99 | N:  16 | test Sqc: 0.7167 | test Neg: 0.3055\n",
      "epoch:    0 | step:  199 | N:  16 | test Sqc: 0.7171 | test Neg: 0.3051\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 | N:  16 | train Sqc: 0.7215 | loss: 1.2885\n",
      "epoch:    1 | step:  199 | N:  16 | train Sqc: 0.7209 | loss: 1.2878\n",
      "epoch:    1 | step:  299 | N:  16 | train Sqc: 0.7215 | loss: 1.2877\n",
      "epoch:    1 | step:  399 | N:  16 | train Sqc: 0.7162 | loss: 1.2869\n",
      "epoch:    1 | step:  499 | N:  16 | train Sqc: 0.7132 | loss: 1.2866\n",
      "epoch:    1 | step:  599 | N:  16 | train Sqc: 0.7111 | loss: 1.2865\n",
      "epoch:    1 | step:  699 | N:  16 | train Sqc: 0.7085 | loss: 1.2861\n",
      "epoch:    1 | step:  799 | N:  16 | train Sqc: 0.7073 | loss: 1.2858\n",
      "epoch:    1 | step:  899 | N:  16 | train Sqc: 0.7027 | loss: 1.2853\n",
      "epoch:    1 | step:  999 | N:  16 | train Sqc: 0.7020 | loss: 1.2853\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    1 | step:   99 | N:  16 | test Sqc: 0.7022 | test Neg: 0.3068\n",
      "epoch:    1 | step:  199 | N:  16 | test Sqc: 0.7049 | test Neg: 0.3063\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    2 | step:   99 | N:  16 | train Sqc: 0.7120 | loss: 1.2870\n",
      "epoch:    2 | step:  199 | N:  16 | train Sqc: 0.7100 | loss: 1.2864\n",
      "epoch:    2 | step:  299 | N:  16 | train Sqc: 0.7104 | loss: 1.2864\n",
      "epoch:    2 | step:  399 | N:  16 | train Sqc: 0.7060 | loss: 1.2857\n",
      "epoch:    2 | step:  499 | N:  16 | train Sqc: 0.7039 | loss: 1.2855\n",
      "epoch:    2 | step:  599 | N:  16 | train Sqc: 0.7025 | loss: 1.2854\n",
      "epoch:    2 | step:  699 | N:  16 | train Sqc: 0.7005 | loss: 1.2851\n",
      "epoch:    2 | step:  799 | N:  16 | train Sqc: 0.6999 | loss: 1.2849\n",
      "epoch:    2 | step:  899 | N:  16 | train Sqc: 0.6954 | loss: 1.2844\n",
      "epoch:    2 | step:  999 | N:  16 | train Sqc: 0.6952 | loss: 1.2845\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    2 | step:   99 | N:  16 | test Sqc: 0.6988 | test Neg: 0.3072\n",
      "epoch:    2 | step:  199 | N:  16 | test Sqc: 0.7023 | test Neg: 0.3067\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    3 | step:   99 | N:  16 | train Sqc: 0.7096 | loss: 1.2866\n",
      "epoch:    3 | step:  199 | N:  16 | train Sqc: 0.7068 | loss: 1.2860\n",
      "epoch:    3 | step:  299 | N:  16 | train Sqc: 0.7068 | loss: 1.2860\n",
      "epoch:    3 | step:  399 | N:  16 | train Sqc: 0.7027 | loss: 1.2853\n",
      "epoch:    3 | step:  499 | N:  16 | train Sqc: 0.7009 | loss: 1.2852\n",
      "epoch:    3 | step:  599 | N:  16 | train Sqc: 0.6996 | loss: 1.2851\n",
      "epoch:    3 | step:  699 | N:  16 | train Sqc: 0.6978 | loss: 1.2848\n",
      "epoch:    3 | step:  799 | N:  16 | train Sqc: 0.6973 | loss: 1.2846\n",
      "epoch:    3 | step:  899 | N:  16 | train Sqc: 0.6929 | loss: 1.2841\n",
      "epoch:    3 | step:  999 | N:  16 | train Sqc: 0.6928 | loss: 1.2843\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    3 | step:   99 | N:  16 | test Sqc: 0.6975 | test Neg: 0.3073\n",
      "epoch:    3 | step:  199 | N:  16 | test Sqc: 0.7013 | test Neg: 0.3069\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    4 | step:   99 | N:  16 | train Sqc: 0.7084 | loss: 1.2864\n",
      "epoch:    4 | step:  199 | N:  16 | train Sqc: 0.7051 | loss: 1.2858\n",
      "epoch:    4 | step:  299 | N:  16 | train Sqc: 0.7050 | loss: 1.2858\n",
      "epoch:    4 | step:  399 | N:  16 | train Sqc: 0.7009 | loss: 1.2851\n",
      "epoch:    4 | step:  499 | N:  16 | train Sqc: 0.6994 | loss: 1.2850\n",
      "epoch:    4 | step:  599 | N:  16 | train Sqc: 0.6981 | loss: 1.2849\n",
      "epoch:    4 | step:  699 | N:  16 | train Sqc: 0.6964 | loss: 1.2846\n",
      "epoch:    4 | step:  799 | N:  16 | train Sqc: 0.6960 | loss: 1.2845\n",
      "epoch:    4 | step:  899 | N:  16 | train Sqc: 0.6916 | loss: 1.2840\n",
      "epoch:    4 | step:  999 | N:  16 | train Sqc: 0.6916 | loss: 1.2841\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    4 | step:   99 | N:  16 | test Sqc: 0.6969 | test Neg: 0.3074\n",
      "epoch:    4 | step:  199 | N:  16 | test Sqc: 0.7009 | test Neg: 0.3070\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    5 | step:   99 | N:  16 | train Sqc: 0.7075 | loss: 1.2863\n",
      "epoch:    5 | step:  199 | N:  16 | train Sqc: 0.7040 | loss: 1.2857\n",
      "epoch:    5 | step:  299 | N:  16 | train Sqc: 0.7038 | loss: 1.2857\n",
      "epoch:    5 | step:  399 | N:  16 | train Sqc: 0.6999 | loss: 1.2850\n",
      "epoch:    5 | step:  499 | N:  16 | train Sqc: 0.6984 | loss: 1.2849\n",
      "epoch:    5 | step:  599 | N:  16 | train Sqc: 0.6971 | loss: 1.2848\n",
      "epoch:    5 | step:  699 | N:  16 | train Sqc: 0.6955 | loss: 1.2845\n",
      "epoch:    5 | step:  799 | N:  16 | train Sqc: 0.6951 | loss: 1.2844\n",
      "epoch:    5 | step:  899 | N:  16 | train Sqc: 0.6908 | loss: 1.2839\n",
      "epoch:    5 | step:  999 | N:  16 | train Sqc: 0.6908 | loss: 1.2840\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    5 | step:   99 | N:  16 | test Sqc: 0.6966 | test Neg: 0.3075\n",
      "epoch:    5 | step:  199 | N:  16 | test Sqc: 0.7006 | test Neg: 0.3070\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    6 | step:   99 | N:  16 | train Sqc: 0.7068 | loss: 1.2862\n",
      "epoch:    6 | step:  199 | N:  16 | train Sqc: 0.7032 | loss: 1.2856\n",
      "epoch:    6 | step:  299 | N:  16 | train Sqc: 0.7030 | loss: 1.2856\n",
      "epoch:    6 | step:  399 | N:  16 | train Sqc: 0.6991 | loss: 1.2849\n",
      "epoch:    6 | step:  499 | N:  16 | train Sqc: 0.6977 | loss: 1.2848\n",
      "epoch:    6 | step:  599 | N:  16 | train Sqc: 0.6965 | loss: 1.2847\n",
      "epoch:    6 | step:  699 | N:  16 | train Sqc: 0.6948 | loss: 1.2844\n",
      "epoch:    6 | step:  799 | N:  16 | train Sqc: 0.6945 | loss: 1.2843\n",
      "epoch:    6 | step:  899 | N:  16 | train Sqc: 0.6902 | loss: 1.2838\n",
      "epoch:    6 | step:  999 | N:  16 | train Sqc: 0.6902 | loss: 1.2839\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    6 | step:   99 | N:  16 | test Sqc: 0.6964 | test Neg: 0.3076\n",
      "epoch:    6 | step:  199 | N:  16 | test Sqc: 0.7004 | test Neg: 0.3071\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    7 | step:   99 | N:  16 | train Sqc: 0.7063 | loss: 1.2861\n",
      "epoch:    7 | step:  199 | N:  16 | train Sqc: 0.7026 | loss: 1.2855\n",
      "epoch:    7 | step:  299 | N:  16 | train Sqc: 0.7024 | loss: 1.2855\n",
      "epoch:    7 | step:  399 | N:  16 | train Sqc: 0.6986 | loss: 1.2849\n",
      "epoch:    7 | step:  499 | N:  16 | train Sqc: 0.6972 | loss: 1.2847\n",
      "epoch:    7 | step:  599 | N:  16 | train Sqc: 0.6960 | loss: 1.2847\n",
      "epoch:    7 | step:  699 | N:  16 | train Sqc: 0.6944 | loss: 1.2844\n",
      "epoch:    7 | step:  799 | N:  16 | train Sqc: 0.6941 | loss: 1.2842\n",
      "epoch:    7 | step:  899 | N:  16 | train Sqc: 0.6897 | loss: 1.2838\n",
      "epoch:    7 | step:  999 | N:  16 | train Sqc: 0.6898 | loss: 1.2839\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    7 | step:   99 | N:  16 | test Sqc: 0.6963 | test Neg: 0.3076\n",
      "epoch:    7 | step:  199 | N:  16 | test Sqc: 0.7003 | test Neg: 0.3071\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    8 | step:   99 | N:  16 | train Sqc: 0.7058 | loss: 1.2860\n",
      "epoch:    8 | step:  199 | N:  16 | train Sqc: 0.7021 | loss: 1.2854\n",
      "epoch:    8 | step:  299 | N:  16 | train Sqc: 0.7019 | loss: 1.2855\n",
      "epoch:    8 | step:  399 | N:  16 | train Sqc: 0.6981 | loss: 1.2848\n",
      "epoch:    8 | step:  499 | N:  16 | train Sqc: 0.6968 | loss: 1.2847\n",
      "epoch:    8 | step:  599 | N:  16 | train Sqc: 0.6956 | loss: 1.2846\n",
      "epoch:    8 | step:  699 | N:  16 | train Sqc: 0.6940 | loss: 1.2843\n",
      "epoch:    8 | step:  799 | N:  16 | train Sqc: 0.6937 | loss: 1.2842\n",
      "epoch:    8 | step:  899 | N:  16 | train Sqc: 0.6894 | loss: 1.2837\n",
      "epoch:    8 | step:  999 | N:  16 | train Sqc: 0.6895 | loss: 1.2838\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    8 | step:   99 | N:  16 | test Sqc: 0.6963 | test Neg: 0.3076\n",
      "epoch:    8 | step:  199 | N:  16 | test Sqc: 0.7002 | test Neg: 0.3071\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    9 | step:   99 | N:  16 | train Sqc: 0.7054 | loss: 1.2860\n",
      "epoch:    9 | step:  199 | N:  16 | train Sqc: 0.7017 | loss: 1.2854\n",
      "epoch:    9 | step:  299 | N:  16 | train Sqc: 0.7015 | loss: 1.2854\n",
      "epoch:    9 | step:  399 | N:  16 | train Sqc: 0.6978 | loss: 1.2848\n",
      "epoch:    9 | step:  499 | N:  16 | train Sqc: 0.6964 | loss: 1.2846\n",
      "epoch:    9 | step:  599 | N:  16 | train Sqc: 0.6953 | loss: 1.2846\n",
      "epoch:    9 | step:  699 | N:  16 | train Sqc: 0.6937 | loss: 1.2843\n",
      "epoch:    9 | step:  799 | N:  16 | train Sqc: 0.6934 | loss: 1.2841\n",
      "epoch:    9 | step:  899 | N:  16 | train Sqc: 0.6891 | loss: 1.2837\n",
      "epoch:    9 | step:  999 | N:  16 | train Sqc: 0.6892 | loss: 1.2838\n",
      "==================================================   Test   ==================================================\n",
      "epoch:    9 | step:   99 | N:  16 | test Sqc: 0.6962 | test Neg: 0.3076\n",
      "epoch:    9 | step:  199 | N:  16 | test Sqc: 0.7001 | test Neg: 0.3072\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/post_selected/data_18pa.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m----> 3\u001b[0m     prepseq, shadow_state, rhoS \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/post_selected/data_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mN\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mpa.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     prepseq, shadow_state, rhoS \u001b[38;5;241m=\u001b[39m prepseq[:\u001b[38;5;241m600000\u001b[39m], shadow_state[:\u001b[38;5;241m600000\u001b[39m], rhoS[:\u001b[38;5;241m600000\u001b[39m]\n\u001b[1;32m      7\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(prepseq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mtrain_ratio)\n",
      "File \u001b[0;32m~/projects_jupyter/Google collab/tel/N=4-16 with ancilla/training/MPO/utils.py:11\u001b[0m, in \u001b[0;36mtorch_data\u001b[0;34m(filename, shuffle)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_data\u001b[39m(filename, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m out]\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/post_selected/data_18pa.pickle'"
     ]
    }
   ],
   "source": [
    "for N in range(4, 18, 2):\n",
    "    torch.manual_seed(seed)\n",
    "    prepseq, shadow_state, rhoS = torch_data(f'../data/post_selected/data_{N}pa.pickle', shuffle=True)\n",
    "    \n",
    "    prepseq, shadow_state, rhoS = prepseq[:600000], shadow_state[:600000], rhoS[:600000]\n",
    "    \n",
    "    train_size = int(prepseq.shape[0]*train_ratio)\n",
    "    test_size = prepseq.shape[0]-train_size\n",
    "    \n",
    "    prepseq_train, prepseq_test = prepseq[:train_size], prepseq[train_size:]\n",
    "    shadow_state_train, shadow_state_test = shadow_state[:train_size], shadow_state[train_size:]\n",
    "    rhoS_train, rhoS_test = rhoS[:train_size], rhoS[train_size:]\n",
    "    \n",
    "    # split in batches\n",
    "    prepseq_train = prepseq_train.view(-1, batch, N-2)\n",
    "    shadow_state_train = shadow_state_train.view(-1, batch, 4)\n",
    "    rhoS_train = rhoS_train.view(-1, batch, 4, 4)\n",
    "\n",
    "    prepseq_test = prepseq_test.view(-1, batch, N-2)\n",
    "    shadow_state_test = shadow_state_test.view(-1, batch, 4)\n",
    "    rhoS_test = rhoS_test.view(-1, batch, 4, 4)\n",
    "    \n",
    "    # load new model\n",
    "    if N == 4:\n",
    "        mdl = tel_mpo(34, bond=10)\n",
    "    else:\n",
    "        mdl.load_state_dict(torch.load(f'{file}/models/mpo_N={N-2}_pa.pt'))\n",
    "    \n",
    "    # load old model\n",
    "    # mdl.load_state_dict(torch.load(f'{file}/models/mpo_N={N}_na.pt'))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-3) # 0.0001\n",
    "    l = {'train Sqc':[], 'test Sqc':[], 'test Neg':[], 'test Sa':[], 'loss':[]}\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        # Train\n",
    "        if train:\n",
    "            print('='*50+'   Train   '+'='*50)\n",
    "            mdl.train()\n",
    "            for i in range(prepseq_train.shape[0]):\n",
    "                rhoC = mdl(prepseq_train[i])\n",
    "                l['train Sqc'].append(bSqc(rhoS_train[i], rhoC).mean().item())\n",
    "                optimizer.zero_grad()\n",
    "                probs = torch.bmm(torch.bmm(shadow_state_train[i].unsqueeze(1), rhoC), shadow_state_train[i].conj().unsqueeze(-1)).view(-1).real\n",
    "                loss = -probs.log().mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                l['loss'].append(loss.item())\n",
    "                if (i+1)%100 == 0:\n",
    "                    trainS = torch.tensor(l['train Sqc'])[-i:].mean().item()\n",
    "                    loss = torch.tensor(l['loss'])[-i:].mean().item()\n",
    "                    print('epoch:  %3d | step:  %3d | N:  %d | train Sqc: %.4f | loss: %.4f' %(epoch, i, N, trainS, loss))\n",
    "        # Test\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                print('='*50+'   Test   '+'='*50)\n",
    "                mdl.eval()\n",
    "                for i in range(prepseq_test.shape[0]):\n",
    "                    rhoC = mdl(prepseq_test[i])\n",
    "                    l['test Sqc'].append(bSqc(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Neg'].append(Neg(rhoS_test[i], rhoC).mean().item())\n",
    "                    l['test Sa'].append(Sa(rhoS_test[i], rhoC).mean().item())\n",
    "                    if (i+1)%100 == 0:\n",
    "                        testS = torch.tensor(l['test Sqc'])[-i:].mean().item()\n",
    "                        testN = torch.tensor(l['test Neg'])[-i:].mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d | N:  %d | test Sqc: %.4f | test Neg: %.4f' %(epoch, i, N, testS, testN))\n",
    "        torch.save(l, f'{file}/record/mpo_N={N}_pa.pt')\n",
    "        torch.save(mdl.state_dict(), f'{file}/models/mpo_N={N}_pa.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432cd1f-ac10-4812-845e-19e3555c413a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
