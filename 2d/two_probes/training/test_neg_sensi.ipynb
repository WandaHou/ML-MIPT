{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-02 05:35:07.495512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754112907.505493    7108 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754112907.510858    7108 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754112907.517819    7108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754112907.517895    7108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754112907.517897    7108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754112907.517898    7108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from TFM import LlamaPredictor\n",
    "import torch\n",
    "from utils import torch_data, shuffle, blogm, bSqc, Neg, Sa, eps, create_train_test_split, save_checkpoint, load_checkpoint, save_checkpoint_and_test\n",
    "from math import prod\n",
    "\n",
    "dtype = torch.complex128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qubit Layout: 6x6\n",
    "\n",
    "```\n",
    "⊕0── ●1── ●2 ──●3── ⊕4 ──●5\n",
    " │    │    │    │    │    │\n",
    "●6── ●7── ●8── ●9── ●10──●11\n",
    " │    │    │    │    │    │\n",
    "●12──●13──●14──●15──●16──●17\n",
    " │    │    │    │    │    │\n",
    "●18──●19──●20──●21──●22──●23\n",
    " │    │    │    │    │    │\n",
    "●24──●25──●26──●27──●28──●29\n",
    " │    │    │    │    │    │\n",
    "●30──●31──●32──●33──●34──●35\n",
    "```\n",
    "\n",
    "idx = 0 & 4 are probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 78*10**6\n",
    "test_size = 10**6\n",
    "batch = 1000\n",
    "N = 36\n",
    "d = 5\n",
    "file = 'save'\n",
    "\n",
    "checkpoint_epoch_pairs = [(19,19)]\n",
    "site_flip_idx_list = [[2], [8], [14], [20], [26], [32]] # measurement idx to flip\n",
    "\n",
    "with torch.no_grad():\n",
    "    for site_flip_idx in site_flip_idx_list:\n",
    "        for theta_idx in range(11):\n",
    "            # load model\n",
    "            mdl = LlamaPredictor(L_max=N,\n",
    "                                d=d,\n",
    "                                n_embd=96, \n",
    "                                n_layer=36, \n",
    "                                n_head=48,\n",
    "                                vocab_size=3, \n",
    "                                dropout_prob=0.0).to(device)\n",
    "            optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-4)\n",
    "\n",
    "            # load data\n",
    "            prepseq_all = torch.load(f'data/theta{theta_idx}/all_prepseq_theta={theta_idx}.pt',weights_only=True)\n",
    "            shadow_all = torch.load(f'data/theta{theta_idx}/all_shadow_state_theta={theta_idx}.pt',weights_only=True)\n",
    "            rhoS_all = torch.load(f'data/theta{theta_idx}/all_rhoS_theta={theta_idx}.pt',weights_only=True)\n",
    "\n",
    "            # flip the site in prepseq_test\n",
    "            if len(site_flip_idx) > 0:\n",
    "                for idx in site_flip_idx:\n",
    "                    prepseq_all[:,idx] -= 1\n",
    "                    prepseq_all[:,idx] *= -1\n",
    "                print(f'flipped site {site_flip_idx} in prepseq_test for theta_idx={theta_idx}')\n",
    "            \n",
    "            # Preprocess all prepseq data once (add 1 and append zero column)\n",
    "            prepseq_all = torch.cat([prepseq_all+1, torch.zeros(prepseq_all.shape[0], 1, dtype=prepseq_all.dtype)], -1)\n",
    "            \n",
    "            # Create non-overlapping train/test split with batching\n",
    "            train_data, test_data = create_train_test_split(\n",
    "                prepseq_all, shadow_all, rhoS_all, \n",
    "                train_size, test_size, batch\n",
    "            )\n",
    "            \n",
    "            # Extract batched data for convenience\n",
    "            prepseq_train = train_data['prepseq']\n",
    "            shadow_state_train = train_data['shadow_state']\n",
    "            rhoS_train = train_data['rhoS']\n",
    "            \n",
    "            prepseq_test = test_data['prepseq'].to(device)\n",
    "            shadow_state_test = test_data['shadow_state'].to(device)\n",
    "            rhoS_test = test_data['rhoS'].to(device)\n",
    "\n",
    "            torch.save(rhoS_test.view(-1, 4, 4), f'test_records/rhoS_theta={theta_idx}_test.pt')\n",
    "\n",
    "            for pairs in checkpoint_epoch_pairs:\n",
    "                checkpoint, epoch = pairs\n",
    "                # load checkpoint model\n",
    "                checkpoint_info = load_checkpoint(mdl, optimizer, epoch, checkpoint, \n",
    "                                                        save_dir=f'{file}/models', \n",
    "                                                        filename_prefix=f'model_d{d}_theta_idx{theta_idx}')\n",
    "                print(f\"loaded checkpoint {checkpoint} from epoch {epoch}\")\n",
    "                # run test loop\n",
    "                mdl.eval()\n",
    "                rhoC_test = []\n",
    "                test_batches = prepseq_test.shape[0]\n",
    "                for j in range(test_batches):\n",
    "                    prepseq_batch = prepseq_test[j].to(device)\n",
    "                    shadow_state_batch = shadow_state_test[j].to(device) \n",
    "                    rhoS_batch = rhoS_test[j].to(device)\n",
    "                    rhoC = mdl(prepseq_batch, False)\n",
    "                    rhoC_test.append(rhoC)\n",
    "                rhoC_test = torch.stack(rhoC_test).view(-1, 4, 4).to(device)\n",
    "                torch.save(rhoC_test, f'test_records/rhoC_theta={theta_idx}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt')\n",
    "                # calculate mean and second moment of Sqc, Neg, Sa, loss\n",
    "                Sqc = bSqc(rhoS_test.view(-1, 4, 4), rhoC_test)\n",
    "                neg = Neg(rhoS_test.view(-1, 4, 4), rhoC_test)\n",
    "                sa = Sa(rhoS_test.view(-1, 4, 4), rhoC_test)\n",
    "                # save results\n",
    "                torch.save(Sqc, f'test_records/Sqc_theta={theta_idx}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt')\n",
    "                torch.save(neg, f'test_records/neg_theta={theta_idx}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt')\n",
    "                torch.save(sa, f'test_records/sa_theta={theta_idx}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt')\n",
    "                # calculate loss\n",
    "                rhoC_test = []\n",
    "                for j in range(test_batches):\n",
    "                    prepseq_batch = prepseq_test[j].to(device)\n",
    "                    shadow_state_batch = shadow_state_test[j].to(device) \n",
    "                    rhoS_batch = rhoS_test[j].to(device)\n",
    "                    rhoC = mdl(prepseq_batch, True)\n",
    "                    rhoC_test.append(rhoC)\n",
    "                rhoC_test = torch.stack(rhoC_test).view(-1, 4, 4).to(device)\n",
    "                loss = -torch.bmm(torch.bmm(shadow_state_test.view(-1, 4).conj().unsqueeze(1), rhoC_test), shadow_state_test.view(-1, 4).unsqueeze(-1)).view(-1).real.log()\n",
    "                torch.save(loss, f'test_records/loss_theta={theta_idx}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt')\n",
    "                print(f'test at epoch {epoch} checkpoint {checkpoint} site flip {site_flip_idx} complete:')\n",
    "                print(f'Sqc mean: {Sqc.sum().item()/test_size:.4f}, second moment: {(Sqc**2).sum().item()/test_size:.4f}')\n",
    "                print(f'Neg mean: {neg.sum().item()/test_size:.4f}, second moment: {(neg**2).sum().item()/test_size:.4f}')\n",
    "                print(f'Sa mean: {sa.sum().item()/test_size:.4f}, second moment: {(sa**2).sum().item()/test_size:.4f}')\n",
    "                print(f'loss mean: {loss.sum().item()/test_size:.4f}, second moment: {(loss**2).sum().item()/test_size:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sqc, Negativity, and Loss vs theta with all site-flip configs overlaid in one figure\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Site-flip configurations to compare (must match how files were saved)\n",
    "site_flip_idx_list = [[], [2], [8], [14], [20], [26], [32]]\n",
    "\n",
    "# Checkpoints/epochs to plot (ensure these exist with site_flip suffix)\n",
    "checkpoint_epoch_pairs = [(19, 19)]\n",
    "\n",
    "# Theta values for x-axis\n",
    "theta_values = torch.linspace(0, torch.pi/2, 11)\n",
    "\n",
    "for checkpoint, epoch in checkpoint_epoch_pairs:\n",
    "    # One figure per checkpoint/epoch with all flip configs overlaid\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "    for site_flip_idx in site_flip_idx_list:\n",
    "        Sqc_means, Sqc_stds = [], []\n",
    "        Neg_means, Neg_stds = [], []\n",
    "        loss_means, loss_stds = [], []\n",
    "\n",
    "        for theta in range(0, 11):\n",
    "            # Load per-theta tensors saved with site_flip in filename\n",
    "            Sqc_test = torch.load(\n",
    "                f\"test_records/Sqc_theta={theta}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt\"\n",
    "            )\n",
    "            Neg_test = torch.load(\n",
    "                f\"test_records/neg_theta={theta}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt\"\n",
    "            )\n",
    "            loss_test = torch.load(\n",
    "                f\"test_records/loss_theta={theta}_checkpoint={checkpoint}_epoch={epoch}_site_flip={site_flip_idx}.pt\"\n",
    "            )\n",
    "\n",
    "            # Means and standard error\n",
    "            Sqc_mean = Sqc_test.mean().item()\n",
    "            Sqc_std = ((Sqc_test**2).mean() - Sqc_mean**2).sqrt().item() / (Sqc_test.numel()**0.5)\n",
    "\n",
    "            Neg_mean = Neg_test.mean().item()\n",
    "            Neg_std = ((Neg_test**2).mean() - Neg_mean**2).sqrt().item() / (Neg_test.numel()**0.5)\n",
    "\n",
    "            loss_mean = loss_test.mean().item()\n",
    "            loss_std = ((loss_test**2).mean() - loss_mean**2).sqrt().item() / (loss_test.numel()**0.5)\n",
    "\n",
    "            Sqc_means.append(Sqc_mean)\n",
    "            Sqc_stds.append(Sqc_std)\n",
    "            Neg_means.append(Neg_mean)\n",
    "            Neg_stds.append(Neg_std)\n",
    "            loss_means.append(loss_mean)\n",
    "            loss_stds.append(loss_std)\n",
    "\n",
    "        label = f\"flip {'none' if len(site_flip_idx)==0 else site_flip_idx}\"\n",
    "\n",
    "        axes[0].errorbar(theta_values, Sqc_means, yerr=Sqc_stds, marker='o', capsize=4, linewidth=2, label=label)\n",
    "        axes[1].errorbar(theta_values, Neg_means, yerr=Neg_stds, marker='o', capsize=4, linewidth=2, label=label)\n",
    "        axes[2].errorbar(theta_values, loss_means, yerr=loss_stds, marker='o', capsize=4, linewidth=2, label=label)\n",
    "\n",
    "    axes[0].set_xlabel('θ')\n",
    "    axes[0].set_ylabel('Sqc')\n",
    "    axes[0].set_title('Sqc vs θ')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].set_xlabel('θ')\n",
    "    axes[1].set_ylabel('Negativity')\n",
    "    axes[1].set_title('Negativity vs θ')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].set_xlabel('θ')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].set_title('Loss vs θ')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Single legend for all curves\n",
    "    axes[0].legend(frameon=False)\n",
    "\n",
    "    # Put checkpoint/epoch info in the figure title\n",
    "    fig.suptitle(f\"ckpt {checkpoint}, ep {epoch}\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
