{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a7fc8d",
   "metadata": {},
   "source": [
    "**Note: This notebook is not used in the current project.**\n",
    "\n",
    "This notebook was developed for reproducing training dynamics by loading saved model checkpoints and evaluating test metrics, but it is not part of the active workflow for the two-probe quantum error correction experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f65ba4c-9269-4c32-9f93-28d1c951d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-22 21:20:39.486657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753219239.496837    6906 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753219239.500959    6906 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753219239.506137    6906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753219239.506147    6906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753219239.506149    6906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753219239.506151    6906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from TFM import LlamaPredictor\n",
    "import torch\n",
    "from utils import torch_data, shuffle, blogm, bSqc, Neg, Sa, eps, create_train_test_split, save_checkpoint, load_checkpoint, save_checkpoint_and_test\n",
    "from math import prod\n",
    "\n",
    "dtype = torch.complex128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaadd22-2be0-41e4-a274-ffedbfe49d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing d=5, theta_idx=2, train_size=78000000\n",
      "Data/models directory: training_theta=2/\n",
      "Results will be saved to: reproduce/record/\n",
      "Expected checkpoints: 20 epochs × 20 checkpoints = 400 total\n",
      "Data loaded from: training_theta=2/data/theta2/\n",
      "test size=1000000, train size=78000000\n",
      "test indices: [0-999999], train indices: [1000000-78999999]\n",
      "Test data loaded: 1000 batches\n",
      "Looking for checkpoints in: training_theta=2/save/models/\n",
      "Checkpoint pattern: model_d5_theta_idx2_epochXXX_stepXXXX.pt\n",
      "\n",
      "Processing Epoch -1...\n"
     ]
    }
   ],
   "source": [
    "# Code to reproduce training dynamics by loading checkpoints and running test loops\n",
    "import os\n",
    "\n",
    "def run_test_evaluation(model, prepseq_test, shadow_state_test, rhoS_test, device):\n",
    "    \"\"\"\n",
    "    Run test evaluation using the same logic as save_checkpoint_and_test from utils.\n",
    "    This extracts just the test evaluation part without saving checkpoints.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    temp_test_metrics = {'loss':[], 'msk off Sqc':[], 'msk off Neg':[], 'msk off Sa':[]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_batches = prepseq_test.shape[0]\n",
    "        for j in range(test_batches):\n",
    "            prepseq_batch = prepseq_test[j].to(device)\n",
    "            shadow_state_batch = shadow_state_test[j].to(device) \n",
    "            rhoS_batch = rhoS_test[j].to(device)\n",
    "            \n",
    "            # Forward pass (mask off for test metrics)\n",
    "            rhoC = model(prepseq_batch, False)\n",
    "            \n",
    "            # Compute product of partial traces: Tr_A[rho^C] ⊗ Tr_B[rho^C]\n",
    "            # Using same einsum pattern as in utils.py Sa function\n",
    "            rho_reshaped = rhoC.view(-1, 2, 2, 2, 2)  # (batch, i1, i2, j1, j2)\n",
    "            rho_A = torch.einsum('bijkj->bik', rho_reshaped)  # Tr_A: trace over 1st qubit (i1=j1)\n",
    "            rho_B = torch.einsum('bijil->bjl', rho_reshaped)  # Tr_B: trace over 2nd qubit (i2=j2)\n",
    "            rhoC_product = torch.vmap(torch.kron)(rho_A, rho_B)  # Product of marginals Tr_A ⊗ Tr_B\n",
    "            \n",
    "            # Calculate test metrics using utils functions with product of marginals\n",
    "            temp_test_metrics['msk off Sqc'].extend(bSqc(rhoS_batch, rhoC_product).tolist())\n",
    "            temp_test_metrics['msk off Neg'].extend(Neg(rhoS_batch, rhoC_product).tolist())\n",
    "            temp_test_metrics['msk off Sa'].extend(Sa(rhoS_batch, rhoC_product).tolist())\n",
    "            \n",
    "            # Calculate loss (with mask on, same as training loss)\n",
    "            rhoC_masked = model(prepseq_batch, True)\n",
    "            rhoC_masked = rhoC_masked.view(-1, 2, 2, 2, 2)\n",
    "            rhoC_masked_A = torch.einsum('bijkj->bik', rhoC_masked)\n",
    "            rhoC_masked_B = torch.einsum('bijil->bjl', rhoC_masked)\n",
    "            rhoC_masked_product = torch.vmap(torch.kron)(rhoC_masked_A, rhoC_masked_B)\n",
    "\n",
    "            probs_masked = torch.bmm(torch.bmm(shadow_state_batch.conj().unsqueeze(1), rhoC_masked_product), shadow_state_batch.unsqueeze(-1)).view(-1).real\n",
    "            loss_masked = -probs_masked.log().mean()\n",
    "            temp_test_metrics['loss'].append(loss_masked.item())\n",
    "    \n",
    "    # Return mean values using torch tensors (same as utils pattern)\n",
    "    return {\n",
    "        'loss': torch.tensor(temp_test_metrics['loss']).mean().item(),\n",
    "        'msk off Sqc': torch.tensor(temp_test_metrics['msk off Sqc']).mean().item(), \n",
    "        'msk off Neg': torch.tensor(temp_test_metrics['msk off Neg']).mean().item(),\n",
    "        'msk off Sa': torch.tensor(temp_test_metrics['msk off Sa']).mean().item()\n",
    "    }\n",
    "\n",
    "# Configuration matching your training setup\n",
    "seed = 81\n",
    "test_size = 1*10**6\n",
    "N = 36\n",
    "batch = 1000\n",
    "num_check = 20  # Number of checkpoints per epoch (matching your training)\n",
    "max_epochs = 20  # Maximum epochs to process (matching your training)\n",
    "\n",
    "# ===== SPECIFY WHICH THETA VALUES TO PROCESS =====\n",
    "theta_values_to_process = [2,4,6,8,10]  # Change this to process different theta values, e.g. [0, 1, 2, 3, 4]\n",
    "\n",
    "# Expected directory structure for each theta:\n",
    "# training_theta=0/data/theta0/ and training_theta=0/save/models/\n",
    "# training_theta=1/data/theta1/ and training_theta=1/save/models/  \n",
    "# training_theta=2/data/theta2/ and training_theta=2/save/models/\n",
    "# etc.\n",
    "\n",
    "# Create single reproduce directory at notebook level\n",
    "reproduce_file = 'reproduce'  # Single reproduce directory for all results\n",
    "os.makedirs(f'{reproduce_file}/record', exist_ok=True)\n",
    "\n",
    "for d in [5]:\n",
    "    for theta_idx in theta_values_to_process:\n",
    "        for train_size in [78*10**6]:\n",
    "            # Set up directory paths for this theta value\n",
    "            base_dir = f'training_theta={theta_idx}'  # Dynamic base directory for data/models\n",
    "            file = f'{base_dir}/save'  # Original save directory\n",
    "            \n",
    "            print(f\"Processing d={d}, theta_idx={theta_idx}, train_size={train_size}\")\n",
    "            print(f\"Data/models directory: {base_dir}/\")\n",
    "            print(f\"Results will be saved to: {reproduce_file}/record/\")\n",
    "            print(f\"Expected checkpoints: {max_epochs} epochs × {num_check} checkpoints = {max_epochs * num_check} total\")\n",
    "            \n",
    "            # Initialize model (same architecture as training)\n",
    "            torch.manual_seed(seed)\n",
    "            mdl = LlamaPredictor(L_max=N,\n",
    "                                d=d,\n",
    "                                n_embd=96, \n",
    "                                n_layer=36, \n",
    "                                n_head=48,\n",
    "                                vocab_size=3, \n",
    "                                dropout_prob=0.0).to(device)\n",
    "            \n",
    "            # Dummy optimizer for checkpoint loading compatibility\n",
    "            optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-4)\n",
    "            \n",
    "            # Initialize record storage (same structure as training records)\n",
    "            l_test_reproduced = {'loss':[], 'msk off Sqc':[], 'msk off Neg':[],'msk off Sa':[]}\n",
    "            \n",
    "            # Load test data using correct directory structure\n",
    "            data_dir = f'{base_dir}/data/theta{theta_idx}'\n",
    "            try:\n",
    "                prepseq_all = torch.load(f'{data_dir}/all_prepseq_theta={theta_idx}.pt',weights_only=True)\n",
    "                shadow_all = torch.load(f'{data_dir}/all_shadow_state_theta={theta_idx}.pt',weights_only=True)\n",
    "                rhoS_all = torch.load(f'{data_dir}/all_rhoS_theta={theta_idx}.pt',weights_only=True)\n",
    "                print(f\"Data loaded from: {data_dir}/\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"ERROR: Could not load data from {data_dir}/ - {e}\")\n",
    "                print(f\"Please check if directory {base_dir}/ exists and contains the data files\")\n",
    "                continue\n",
    "            \n",
    "            # Preprocess data exactly as in training\n",
    "            prepseq_all = torch.cat([prepseq_all+1, torch.zeros(prepseq_all.shape[0], 1, dtype=prepseq_all.dtype)], -1)\n",
    "            \n",
    "            # Create test split using utils function (we only need test data)\n",
    "            _, test_data = create_train_test_split(\n",
    "                prepseq_all, shadow_all, rhoS_all, \n",
    "                train_size, test_size, batch\n",
    "            )\n",
    "            \n",
    "            prepseq_test = test_data['prepseq']\n",
    "            shadow_state_test = test_data['shadow_state']\n",
    "            rhoS_test = test_data['rhoS']\n",
    "            \n",
    "            print(f\"Test data loaded: {prepseq_test.shape[0]} batches\")\n",
    "            \n",
    "            # Process all available checkpoints (20 per epoch × 20 epochs)\n",
    "            checkpoint_dir = f'{file}/models'\n",
    "            filename_prefix = f'model_d{d}_theta_idx{theta_idx}'\n",
    "            \n",
    "            print(f\"Looking for checkpoints in: {checkpoint_dir}/\")\n",
    "            print(f\"Checkpoint pattern: {filename_prefix}_epochXXX_stepXXXX.pt\")\n",
    "            \n",
    "            checkpoint_count = 0\n",
    "            \n",
    "            # Process epochs from -1 to max_epochs-1 (to match your checkpoint naming)\n",
    "            for epoch in range(-1, max_epochs):\n",
    "                print(f\"\\nProcessing Epoch {epoch}...\")\n",
    "                epoch_checkpoints = 0\n",
    "                \n",
    "                # Process all 20 checkpoints for this epoch\n",
    "                for checkpoint_num in range(num_check + 5):  # +5 to catch any extra final checkpoints\n",
    "                    # Simple checkpoint naming format matching your example: epoch{epoch:03d}_step{checkpoint_num:04d}.pt\n",
    "                    checkpoint_file = f'{checkpoint_dir}/{filename_prefix}_epoch{epoch:04d}_step{checkpoint_num:04d}.pt'\n",
    "                    \n",
    "                    if os.path.exists(checkpoint_file):\n",
    "                        try:\n",
    "                            # Load checkpoint manually since the naming format is different\n",
    "                            checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "                            mdl.load_state_dict(checkpoint['model_state_dict'])\n",
    "                            if 'optimizer_state_dict' in checkpoint:\n",
    "                                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                            \n",
    "                            \n",
    "                            # Run test evaluation using our helper function\n",
    "                            metrics = run_test_evaluation(mdl, prepseq_test, shadow_state_test, rhoS_test, device)\n",
    "                            \n",
    "                            # Store results\n",
    "                            l_test_reproduced['loss'].append(metrics['loss'])\n",
    "                            l_test_reproduced['msk off Sqc'].append(metrics['msk off Sqc'])\n",
    "                            l_test_reproduced['msk off Neg'].append(metrics['msk off Neg'])\n",
    "                            l_test_reproduced['msk off Sa'].append(metrics['msk off Sa'])\n",
    "                            \n",
    "                            checkpoint_count += 1\n",
    "                            epoch_checkpoints += 1\n",
    "                            \n",
    "                            print(f\"  Checkpoint {checkpoint_num}: Loss={metrics['loss']:.4f}, Sqc={metrics['msk off Sqc']:.4f}, Neg={metrics['msk off Neg']:.4f}, Sa={metrics['msk off Sa']:.4f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"  Error loading {checkpoint_file}: {e}\")\n",
    "                            continue\n",
    "                \n",
    "                print(f\"Epoch {epoch} completed: {epoch_checkpoints} checkpoints processed\")\n",
    "            \n",
    "            # Save reproduced results\n",
    "            print(f\"\\n\" + \"=\"*80)\n",
    "            print(f\"SUMMARY:\")\n",
    "            print(f\"Total checkpoints processed: {checkpoint_count}\")\n",
    "            print(f\"Expected: {max_epochs * num_check}, Found: {checkpoint_count}\")\n",
    "            \n",
    "            # Save test records in same format as original training\n",
    "            output_test_file = f'{reproduce_file}/record/reproduced_d{d}_theta_idx{theta_idx}_size{train_size}_test.pt'\n",
    "            torch.save(l_test_reproduced, output_test_file)\n",
    "            \n",
    "            print(f\"Reproduced test records saved to: {output_test_file}\")\n",
    "            print(f\"Records contain {len(l_test_reproduced['loss'])} data points for plotting\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPLETED: Processed {len(theta_values_to_process)} theta value(s): {theta_values_to_process}\")\n",
    "print(f\"All results saved in: {reproduce_file}/record/\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
