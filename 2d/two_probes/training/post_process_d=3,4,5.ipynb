{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Experimental Two-Probe Data\n",
    "\n",
    "This notebook post-processes raw experimental measurement data from two-probe quantum experiments into a format suitable for machine learning training. It performs post-selection based on ancilla-physical qubit pairs for error mitigation, extracts probe qubit measurements, constructs quantum shadow states from different measurement bases, and aggregates data across multiple experimental runs into consolidated training datasets.\n",
    "\n",
    "### Qubit Layout: 6x6 Physical Lattice with Ancilla Perimeter\n",
    "\n",
    "```\n",
    "        ⊕0  ⊕1   ⊕2    ⊕3   ⊕4   ⊕5           ← ANCILLA (top)\n",
    "        │    │    │    │    │    │ \n",
    "   ⊕6 ──●7──●8 ──●9 ──●10──●11──●12──⊕13      \n",
    "        │    │    │    │    │    │    \n",
    "  ⊕14──●15──●16──●17──●18──●19──●20──⊕21\n",
    "        │    │    │    │    │    │    \n",
    "  ⊕22──●23──●24──●25──●26──●27──●28──⊕29      ← 6×6 PHYSICAL\n",
    "        │    │    │    │    │    │            \n",
    "  ⊕30──●31──●32──●33──●34──●35──●36──⊕37       \n",
    "        │    │    │    │    │    │    \n",
    "  ⊕38──●39──●40──●41──●42──●43──●44──⊕45\n",
    "        │    │    │    │    │    │    \n",
    "  ⊕46──●47──●48──●49──●50──●51──●52──⊕53\n",
    "        │    │    │    │    │    │\n",
    "       ⊕54  ⊕55  ⊕56  ⊕57  ⊕58  ⊕59           ← ANCILLA (bottom)\n",
    "```\n",
    "\n",
    "- **Physical qubits** (prep_idx): Interior 6×6 grid used for computation\n",
    "- **Ancilla qubits**: Perimeter qubits used for error correction post-selection\n",
    "- **Post-selection**: Requires ancilla-physical matching (e.g., ancilla 0 ↔ physical 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import os\n",
    "\n",
    "dtype = torch.complex128\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pauli = torch.tensor([[[1,0],[0,1]],[[0,1],[1,0]],[[0,-1j],[1j,0]],[[1,0],[0,-1]]], device=device, dtype=dtype)\n",
    "basis = torch.linalg.eig(pauli)[1][1:].mT # (3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process two probe qubits data\n",
    "def torch_data(filename, d):\n",
    "    data = {}\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            m = torch.load(filename+f'_({i},{j}).pt')\n",
    "            # msk starts as all True, then gets progressively filtered by requiring ancilla-physical qubit pairs to be the same.\n",
    "            # Finally used to keep only measurements that pass all post-selection criteria: m = m[msk]\n",
    "            msk = torch.ones(m.shape[0], device=device, dtype=torch.bool)\n",
    "            # post select on 2-qubit mitigation\n",
    "            for anc, phy in [(0,7),\n",
    "                             (1,8),\n",
    "                             (2,9),\n",
    "                             (3,10),\n",
    "                             (4,11),\n",
    "                             (5,12),\n",
    "                             (54,47),\n",
    "                             (55,48),\n",
    "                             (56,49),\n",
    "                             (57,50),\n",
    "                             (58,51),\n",
    "                             (59,52),\n",
    "                             (6,7),\n",
    "                             (14,15),\n",
    "                             (22,23),\n",
    "                             (30,31),\n",
    "                             (38,39),\n",
    "                             (46,47),\n",
    "                             (13,12),\n",
    "                             (21,20),\n",
    "                             (29,28),\n",
    "                             (37,36),\n",
    "                             (45,44),\n",
    "                             (53,52),\n",
    "                             ]:\n",
    "                msk = msk & (m[:,anc]==m[:,phy])\n",
    "                #print(f'anc={anc}, phy={phy}, {((m[:,anc]==m[:,phy]).float().mean().item()):.4f}')\n",
    "            prep_idx = [7,8,9,10,11,12,\n",
    "                        15,16,17,18,19,20,\n",
    "                        23,24,25,26,27,28,\n",
    "                        31,32,33,34,35,36,\n",
    "                        39,40,41,42,43,44,\n",
    "                        47,48,49,50,51,52]\n",
    "            m = m[msk] # (batch, num_qubits)\n",
    "            if d == 6:\n",
    "                probe_idx = [7,12]\n",
    "            if d == 5:\n",
    "                probe_idx = [7,11]\n",
    "                #probe_idx = [12,44]\n",
    "            if d == 4:\n",
    "                probe_idx = [8,11]\n",
    "            if d == 3:\n",
    "                probe_idx = [8,10]\n",
    "            prep_idx = [p for p in prep_idx if p not in probe_idx]\n",
    "            probe = torch.cat([m[:,probe_idx[0]].view(-1,1), m[:,probe_idx[1]].view(-1,1)], 1)\n",
    "            prep = m[:,prep_idx]\n",
    "            data[(i,j)] = (prep, probe)\n",
    "    prepseq, shadow_state, rhoS = [], [], []\n",
    "    for k in data.keys():\n",
    "        # construct post-measure state\n",
    "        probseq = data[k][1].to(dtype=torch.int64).to(device=device) # (repetition, 2) last 2 outcomes\n",
    "        obs_basis0 = basis[k[0]].unsqueeze(0).expand(probseq.shape[0], -1, -1) # (repetition, 2, 2)\n",
    "        shadow_state0 = obs_basis0.gather(1, probseq[:,0].view(-1, 1, 1).expand(-1, -1, 2)).squeeze(1) # (repetition, 2)\n",
    "        obs_basis1 = basis[k[1]].unsqueeze(0).expand(probseq.shape[0], -1, -1) # (repetition, 2, 2)\n",
    "        shadow_state1 = obs_basis1.gather(1, probseq[:,1].view(-1, 1, 1).expand(-1, -1, 2)).squeeze(1) # (repetition, 2)\n",
    "        shadow_state01 = torch.vmap(torch.kron)(shadow_state0, shadow_state1) # (batch, 4)\n",
    "        # construct rhoS\n",
    "        I = torch.eye(2, 2, device=device)[None,...].expand(shadow_state01.shape[0], -1, -1)\n",
    "        rhoS0 = 3*torch.vmap(torch.outer)(shadow_state0, shadow_state0.conj()) - I\n",
    "        rhoS1 = 3*torch.vmap(torch.outer)(shadow_state1, shadow_state1.conj()) - I\n",
    "        rhoS01 = torch.vmap(torch.kron)(rhoS0, rhoS1)\n",
    "        # collect result\n",
    "        prepseq.append(data[k][0].to(dtype=torch.int64).to(device=device))\n",
    "        shadow_state.append(shadow_state01)\n",
    "        rhoS.append(rhoS01)\n",
    "    prepseq = torch.cat(prepseq, 0).to(torch.int64)\n",
    "    shadow_state = torch.cat(shadow_state, 0)\n",
    "    rhoS = torch.cat(rhoS, 0)\n",
    "    return prepseq, shadow_state, rhoS\n",
    "\n",
    "def shuffle(prepseq, shadow_state, rhoS):\n",
    "    indices = torch.randperm(prepseq.shape[0])\n",
    "    prepseq = prepseq[indices]\n",
    "    shadow_state = shadow_state[indices]\n",
    "    rhoS = rhoS[indices]\n",
    "    return prepseq, shadow_state, rhoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Data Object Descriptions\n",
    "\n",
    "**Key saved objects and their meanings:**\n",
    "\n",
    "- `all_prepseq` - Shape: (N, 34)  \n",
    "  Ancilla measurement outcomes from physical qubits after post-selection filtering. Used as input features for ML models to predict probe states.\n",
    "\n",
    "- `all_shadow_state` - Shape: (N, 4)  \n",
    "  Shadow states of the two probe qubits: |ψ₁⟩⊗|ψ₂⟩ in the 4D Hilbert space. Constructed from probe measurements in X/Y/Z bases.\n",
    "\n",
    "- `all_rhoS` - Shape: (N, 4, 4)  \n",
    "  Tensor product of single-qubit shadow density matrices: ρS = ρS₁⊗ρS₂ where ρSᵢ = 3|ψᵢ⟩⟨ψᵢ| - I. These are the target density matrices for ML training.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Storage Summary\n",
    "\n",
    "**File Organization:**\n",
    "```\n",
    "data/theta{θ_idx}/\n",
    "├── all_prepseq_theta={θ_idx}.pt     # Input: ancilla measurement patterns  \n",
    "├── all_shadow_state_theta={θ_idx}.pt # Target: probe shadow states\n",
    "└── all_rhoS_theta={θ_idx}.pt        # Target: probe shadow density matrices\n",
    "```\n",
    "\n",
    "**Usage:** ML models learn to predict probe quantum states (shadow_state/rhoS) from ancilla measurement outcomes (prepseq) for quantum error correction decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance=5, loop=0, theta_idx=4, portion to keep=0.5838\n",
      "distance=5, loop=1, theta_idx=4, portion to keep=0.5862\n",
      "distance=5, loop=2, theta_idx=4, portion to keep=0.5859\n",
      "distance=5, loop=3, theta_idx=4, portion to keep=0.5833\n",
      "distance=5, loop=4, theta_idx=4, portion to keep=0.5830\n",
      "distance=5, loop=5, theta_idx=4, portion to keep=0.5831\n",
      "distance=5, loop=6, theta_idx=4, portion to keep=0.5810\n",
      "distance=5, loop=7, theta_idx=4, portion to keep=0.5833\n",
      "distance=5, loop=8, theta_idx=4, portion to keep=0.5801\n",
      "distance=5, loop=9, theta_idx=4, portion to keep=0.5811\n",
      "distance=5, loop=10, theta_idx=4, portion to keep=0.5862\n",
      "distance=5, loop=11, theta_idx=4, portion to keep=0.5358\n",
      "distance=5, loop=12, theta_idx=4, portion to keep=0.5414\n",
      "distance=5, loop=13, theta_idx=4, portion to keep=0.5338\n",
      "distance=5, loop=14, theta_idx=4, portion to keep=0.5470\n",
      "distance=5, loop=15, theta_idx=4, portion to keep=0.5313\n",
      "distance=5, loop=16, theta_idx=4, portion to keep=0.5250\n",
      "torch.Size([86679738, 34]) 4\n",
      "torch.Size([86679738, 4]) 4\n",
      "torch.Size([86679738, 4, 4]) 4\n"
     ]
    }
   ],
   "source": [
    "for theta_idx in [4]:\n",
    "    for d in [5]:\n",
    "        all_prepseq = []\n",
    "        all_shadow_state = []\n",
    "        all_rhoS = []\n",
    "        for loop in range(17):\n",
    "            filename = f'data/theta{theta_idx}/loop{loop}/theta={theta_idx}'\n",
    "            torch.manual_seed(loop)\n",
    "            prepseq, shadow_state, rhoS = torch_data(filename, d)\n",
    "            prepseq, shadow_state, rhoS = shuffle(prepseq, shadow_state, rhoS)\n",
    "            all_prepseq.append(prepseq)\n",
    "            all_shadow_state.append(shadow_state)\n",
    "            all_rhoS.append(rhoS)\n",
    "            print(f'distance={d}, loop={loop}, theta_idx={theta_idx}, portion to keep={((prepseq.shape[0]/9000000)):.4f}')\n",
    "        all_prepseq = torch.cat(all_prepseq, 0)\n",
    "        all_shadow_state = torch.cat(all_shadow_state, 0)\n",
    "        all_rhoS = torch.cat(all_rhoS, 0)\n",
    "        torch.save(all_prepseq, f'data/theta{theta_idx}/all_prepseq_theta={theta_idx}.pt')\n",
    "        torch.save(all_shadow_state, f'data/theta{theta_idx}/all_shadow_state_theta={theta_idx}.pt')\n",
    "        torch.save(all_rhoS, f'data/theta{theta_idx}/all_rhoS_theta={theta_idx}.pt')\n",
    "        print(all_prepseq.shape, theta_idx)\n",
    "        print(all_shadow_state.shape, theta_idx)\n",
    "        print(all_rhoS.shape, theta_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for duplicates and save results (run once)\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import create_train_test_split\n",
    "\n",
    "train_size = 78*10**6\n",
    "test_size = 10**6\n",
    "batch = 1000\n",
    "d = 5\n",
    "seed = 81\n",
    "ignore_sites = [28,29,30,31,32]\n",
    "\n",
    "# Efficient duplicate analysis using tensor operations\n",
    "def find_duplicates(data):\n",
    "    # Convert bit strings to unique integers for fast comparison\n",
    "    # Use base-2 (binary) encoding since values are 0,1 after preprocessing\n",
    "    powers = 2 ** torch.arange(data.shape[1] - 1, -1, -1, device=data.device)\n",
    "    hashes = (data * powers).sum(dim=1)\n",
    "    \n",
    "    # Get unique hashes and their counts\n",
    "    unique_hashes, counts = torch.unique(hashes, return_counts=True)\n",
    "    \n",
    "    # Find duplicates (count > 1)\n",
    "    duplicate_mask = counts > 1\n",
    "    duplicate_counts = counts[duplicate_mask]\n",
    "    \n",
    "    return duplicate_counts.cpu()\n",
    "\n",
    "# Store results for all theta values\n",
    "duplicate_results = {}\n",
    "\n",
    "for theta_idx in range(11):\n",
    "    torch.manual_seed(seed)\n",
    "    print(f'Processing theta {theta_idx}...')\n",
    "    \n",
    "    # load train/test data\n",
    "    prepseq_all = torch.load(f'data/theta{theta_idx}/all_prepseq_theta={theta_idx}.pt',weights_only=True)\n",
    "    shadow_all = torch.load(f'data/theta{theta_idx}/all_shadow_state_theta={theta_idx}.pt',weights_only=True)\n",
    "    rhoS_all = torch.load(f'data/theta{theta_idx}/all_rhoS_theta={theta_idx}.pt',weights_only=True)\n",
    "    \n",
    "   \n",
    "    # Create non-overlapping train/test split with batching\n",
    "    train_data, test_data = create_train_test_split(\n",
    "        prepseq_all, shadow_all, rhoS_all, \n",
    "        train_size, test_size, batch\n",
    "    )\n",
    "\n",
    "    prepseq_train = train_data['prepseq'].view(-1, 34) # (train_size, 34-len(ignore_sites))\n",
    "    prepseq_test = test_data['prepseq'].view(-1, 34) # (test_size, 34-len(ignore_sites))\n",
    "\n",
    "    # ignore sites\n",
    "    if len(ignore_sites) > 0:\n",
    "        prepseq_train = prepseq_train[:, ~torch.isin(torch.arange(34), torch.tensor(ignore_sites))]\n",
    "        prepseq_test = prepseq_test[:, ~torch.isin(torch.arange(34), torch.tensor(ignore_sites))]\n",
    "    \n",
    "    # Find duplicates\n",
    "    train_duplicate_counts = find_duplicates(prepseq_train)\n",
    "    test_duplicate_counts = find_duplicates(prepseq_test)\n",
    "    \n",
    "    # Store results\n",
    "    duplicate_results[theta_idx] = {\n",
    "        'train_duplicate_counts': train_duplicate_counts,\n",
    "        'test_duplicate_counts': test_duplicate_counts,\n",
    "        'train_shape': prepseq_train.shape,\n",
    "        'test_shape': prepseq_test.shape\n",
    "    }\n",
    "    \n",
    "    print(f'  Train: {prepseq_train.shape}, duplicates: {len(train_duplicate_counts)}')\n",
    "    print(f'  Test: {prepseq_test.shape}, duplicates: {len(test_duplicate_counts)}')\n",
    "\n",
    "# Save lightweight results file\n",
    "torch.save(duplicate_results, f'duplicate_analysis_results_ignore_sites={ignore_sites}.pt')\n",
    "print(f'Saved duplicate analysis results to duplicate_analysis_results_ignore_sites={ignore_sites}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot duplicate analysis results (run anytime for plotting)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specify sites to ignore (must match what was used in scanning cell)\n",
    "ignore_sites = []\n",
    "#ignore_sites = [28,29,30,31,32]\n",
    "\n",
    "# Plot settings\n",
    "max_frequency_display = 5  # Upper limit for x-axis display\n",
    "\n",
    "# Create title suffix based on ignore sites\n",
    "if ignore_sites:\n",
    "    title_suffix = f' (ignore sites: {ignore_sites})'\n",
    "else:\n",
    "    title_suffix = ' (all sites)'\n",
    "\n",
    "# Load saved results\n",
    "duplicate_results = torch.load(f'duplicate_analysis_results_ignore_sites={ignore_sites}.pt', weights_only=True)\n",
    "\n",
    "thetas = torch.linspace(0, np.pi/2, 11)\n",
    "\n",
    "# Plot histogram of frequency distribution (including singles)\n",
    "# Create one large figure with all thetas\n",
    "valid_thetas = [i for i in range(11) if i in duplicate_results]\n",
    "n_thetas = len(valid_thetas)\n",
    "\n",
    "# Create subplot grid: n_thetas rows, 2 columns (train, test)\n",
    "fig, axes = plt.subplots(n_thetas, 2, figsize=(16, 4*n_thetas))\n",
    "if n_thetas == 1:\n",
    "    axes = axes.reshape(1, -1)  # Ensure 2D array for single row\n",
    "\n",
    "for plot_idx, theta_idx in enumerate(valid_thetas):\n",
    "    data = duplicate_results[theta_idx]\n",
    "    train_duplicate_counts = data['train_duplicate_counts']\n",
    "    test_duplicate_counts = data['test_duplicate_counts']\n",
    "    train_total_shots = data['train_shape'][0]\n",
    "    test_total_shots = data['test_shape'][0]\n",
    "    \n",
    "    # print(f'theta {theta_idx}: train duplicates: {len(train_duplicate_counts)}, test duplicates: {len(test_duplicate_counts)}')\n",
    "    \n",
    "    # Use the current row of axes\n",
    "    current_axes = axes[plot_idx]\n",
    "    \n",
    "    # Train histogram\n",
    "    if len(train_duplicate_counts) > 0:\n",
    "        # Simple calculation: count unique strings at each frequency\n",
    "        total_duplicate_samples = train_duplicate_counts.sum().item()\n",
    "        num_single_strings = train_total_shots - total_duplicate_samples\n",
    "        total_unique_strings = num_single_strings + len(train_duplicate_counts)\n",
    "        \n",
    "        # Count frequency occurrences (always calculate all frequencies)\n",
    "        freq_counts = {}\n",
    "        freq_counts[1] = num_single_strings  # Always include singles in calculation\n",
    "        for freq in train_duplicate_counts:\n",
    "            f = freq.item()\n",
    "            freq_counts[f] = freq_counts.get(f, 0) + 1\n",
    "        \n",
    "        # Convert to ratios (always relative to total unique strings)\n",
    "        all_frequencies = sorted(freq_counts.keys())\n",
    "        all_values = [freq_counts[f] / total_unique_strings for f in all_frequencies]\n",
    "        \n",
    "        # Filter what to display (always show singles)\n",
    "        display_frequencies = all_frequencies\n",
    "        display_values = all_values\n",
    "        \n",
    "        bars = axes[plot_idx, 0].bar(display_frequencies, display_values, alpha=0.7, width=0.8)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar, value in zip(bars, display_values):\n",
    "            height = bar.get_height()\n",
    "            axes[plot_idx, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{value:.8f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[plot_idx, 0].set_xlabel('Frequency')\n",
    "        axes[plot_idx, 0].set_ylabel('Fraction of Unique Strings')\n",
    "        axes[plot_idx, 0].set_title(f'Train Frequency Distribution (θ={thetas[theta_idx]:.2f}){title_suffix}')\n",
    "        axes[plot_idx, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Adaptive y-axis scaling\n",
    "        axes[plot_idx, 0].set_ylim(0, max(display_values) * 1.08)\n",
    "        \n",
    "        \n",
    "        from matplotlib.ticker import MaxNLocator\n",
    "        axes[plot_idx, 0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        axes[plot_idx, 0].set_xlim(0.5, max_frequency_display + 0.5)\n",
    "        axes[plot_idx, 0].set_xticks(range(1, max_frequency_display + 1))\n",
    "    else:\n",
    "        axes[plot_idx, 0].text(0.5, 0.5, 'All singles', ha='center', va='center', transform=axes[plot_idx, 0].transAxes)\n",
    "        axes[plot_idx, 0].set_title(f'Train Frequency Distribution (θ={thetas[theta_idx]:.2f}){title_suffix}')\n",
    "    \n",
    "    # Test histogram  \n",
    "    if len(test_duplicate_counts) > 0:\n",
    "        # Simple calculation: count unique strings at each frequency\n",
    "        total_duplicate_samples = test_duplicate_counts.sum().item()\n",
    "        num_single_strings = test_total_shots - total_duplicate_samples\n",
    "        total_unique_strings = num_single_strings + len(test_duplicate_counts)\n",
    "        \n",
    "        # Count frequency occurrences (always calculate all frequencies)\n",
    "        freq_counts = {}\n",
    "        freq_counts[1] = num_single_strings  # Always include singles in calculation\n",
    "        for freq in test_duplicate_counts:\n",
    "            f = freq.item()\n",
    "            freq_counts[f] = freq_counts.get(f, 0) + 1\n",
    "        \n",
    "        # Convert to ratios (always relative to total unique strings)\n",
    "        all_frequencies = sorted(freq_counts.keys())\n",
    "        all_values = [freq_counts[f] / total_unique_strings for f in all_frequencies]\n",
    "        \n",
    "        # Filter what to display (always show singles)\n",
    "        display_frequencies = [f for f in all_frequencies if f <= max_frequency_display]\n",
    "        display_values = [freq_counts[f] / total_unique_strings for f in display_frequencies]\n",
    "        \n",
    "        bars = axes[plot_idx, 1].bar(display_frequencies, display_values, alpha=0.7, width=0.8)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar, value in zip(bars, display_values):\n",
    "            height = bar.get_height()\n",
    "            axes[plot_idx, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{value:.8f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[plot_idx, 1].set_xlabel('Frequency')\n",
    "        axes[plot_idx, 1].set_ylabel('Fraction of Unique Strings')\n",
    "        axes[plot_idx, 1].set_title(f'Test Frequency Distribution (θ={thetas[theta_idx]:.2f}){title_suffix}')\n",
    "        axes[plot_idx, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Adaptive y-axis scaling\n",
    "        axes[plot_idx, 1].set_ylim(0, max(display_values) * 1.08)\n",
    "        \n",
    "        axes[plot_idx, 1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        axes[plot_idx, 1].set_xlim(0.5, max_frequency_display + 0.5)\n",
    "        axes[plot_idx, 1].set_xticks(range(1, max_frequency_display + 1))\n",
    "    else:\n",
    "        axes[plot_idx, 1].text(0.5, 0.5, 'All singles', ha='center', va='center', transform=axes[plot_idx, 1].transAxes)\n",
    "        axes[plot_idx, 1].set_title(f'Test Frequency Distribution (θ={thetas[theta_idx]:.2f}){title_suffix}')\n",
    "\n",
    "# Show the combined figure with all thetas\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
