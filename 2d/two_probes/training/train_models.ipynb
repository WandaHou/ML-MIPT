{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c250bf-101a-48e0-ac69-95e8984c9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-04 23:21:37.191628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751671297.214319   10643 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751671297.221685   10643 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from TFM import LlamaPredictor\n",
    "import torch\n",
    "from utils import torch_data, shuffle, blogm, bSqc, Neg, Sa, eps, create_train_test_split, save_checkpoint, load_checkpoint, save_checkpoint_and_test\n",
    "from math import prod\n",
    "\n",
    "dtype = torch.complex128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3965d48d-a8b3-4546-bd33-57961e265281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2333120"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LlamaPredictor(L_max=36,\n",
    "                     d=4,\n",
    "                     n_embd=96, \n",
    "                     n_layer=36, \n",
    "                     n_head=48,\n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "paras = 0\n",
    "for p in mdl.parameters():\n",
    "    paras += prod(p.shape)\n",
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6c9a40-c123-4aab-8187-849455b3441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size=1000000, train size=74000000\n",
      "test indices: [0-999999], train indices: [1000000-74999999]\n",
      "Will save checkpoints every 3700 batches (20 times per epoch)\n",
      "Checkpoint loaded: save/models/model_d5_theta_idx4_epoch0000_step0019.pt\n",
      "Resumed from epoch 0, checkpoint 19. Starting epoch 1.\n",
      "==================================================   Train   ==================================================\n",
      "epoch:    1 | step:   99 |  d:    5 | theta_idx:    4 | current Sqc: 0.8271 | current loss: 1.2051\n",
      "epoch:    1 | step:  199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8325 | current loss: 1.2058\n",
      "epoch:    1 | step:  299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8320 | current loss: 1.2053\n",
      "epoch:    1 | step:  399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8299 | current loss: 1.2051\n",
      "epoch:    1 | step:  499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8291 | current loss: 1.2050\n",
      "epoch:    1 | step:  599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8286 | current loss: 1.2048\n",
      "epoch:    1 | step:  699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8296 | current loss: 1.2049\n",
      "epoch:    1 | step:  799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8282 | current loss: 1.2045\n",
      "epoch:    1 | step:  899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8282 | current loss: 1.2045\n",
      "epoch:    1 | step:  999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8278 | current loss: 1.2044\n",
      "epoch:    1 | step:  1099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8286 | current loss: 1.2046\n",
      "epoch:    1 | step:  1199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8276 | current loss: 1.2045\n",
      "epoch:    1 | step:  1299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8279 | current loss: 1.2044\n",
      "epoch:    1 | step:  1399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8270 | current loss: 1.2043\n",
      "epoch:    1 | step:  1499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8260 | current loss: 1.2041\n",
      "epoch:    1 | step:  1599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8262 | current loss: 1.2042\n",
      "epoch:    1 | step:  1699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8267 | current loss: 1.2043\n",
      "epoch:    1 | step:  1799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8264 | current loss: 1.2042\n",
      "epoch:    1 | step:  1899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8264 | current loss: 1.2043\n",
      "epoch:    1 | step:  1999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8263 | current loss: 1.2043\n",
      "epoch:    1 | step:  2099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8262 | current loss: 1.2043\n",
      "epoch:    1 | step:  2199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8264 | current loss: 1.2043\n",
      "epoch:    1 | step:  2299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8259 | current loss: 1.2042\n",
      "epoch:    1 | step:  2399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8263 | current loss: 1.2043\n",
      "epoch:    1 | step:  2499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8263 | current loss: 1.2042\n",
      "epoch:    1 | step:  2599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8262 | current loss: 1.2042\n",
      "epoch:    1 | step:  2699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8263 | current loss: 1.2042\n",
      "epoch:    1 | step:  2799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8262 | current loss: 1.2042\n",
      "epoch:    1 | step:  2899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8266 | current loss: 1.2042\n",
      "epoch:    1 | step:  2999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8269 | current loss: 1.2043\n",
      "epoch:    1 | step:  3099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8271 | current loss: 1.2043\n",
      "epoch:    1 | step:  3199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8276 | current loss: 1.2044\n",
      "epoch:    1 | step:  3299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8278 | current loss: 1.2045\n",
      "epoch:    1 | step:  3399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8275 | current loss: 1.2044\n",
      "epoch:    1 | step:  3499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8274 | current loss: 1.2044\n",
      "epoch:    1 | step:  3599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8271 | current loss: 1.2043\n",
      "Running test at epoch 1, checkpoint 0/20\n",
      "Checkpoint saved: save/models/model_d5_theta_idx4_epoch0001_step0000.pt\n",
      "epoch:   1 | checkpoint:   0 | d:   5 | theta_idx:   4 | train Sqc: 0.8272 | loss: 1.2043 | test Sqc: 0.7540 | test Neg: -0.0002 | test Sa: 0.3114\n",
      "epoch:    1 | step:  3799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8329 | current loss: 1.2043\n",
      "epoch:    1 | step:  3899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8421 | current loss: 1.2059\n",
      "epoch:    1 | step:  3999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8366 | current loss: 1.2051\n",
      "epoch:    1 | step:  4099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8320 | current loss: 1.2044\n",
      "epoch:    1 | step:  4199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8302 | current loss: 1.2041\n",
      "epoch:    1 | step:  4299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8263 | current loss: 1.2034\n",
      "epoch:    1 | step:  4399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8239 | current loss: 1.2031\n",
      "epoch:    1 | step:  4499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8237 | current loss: 1.2029\n",
      "epoch:    1 | step:  4599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8223 | current loss: 1.2025\n",
      "epoch:    1 | step:  4699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8229 | current loss: 1.2027\n",
      "epoch:    1 | step:  4799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8229 | current loss: 1.2027\n",
      "epoch:    1 | step:  4899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8240 | current loss: 1.2029\n",
      "epoch:    1 | step:  4999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8244 | current loss: 1.2030\n",
      "epoch:    1 | step:  5099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8239 | current loss: 1.2029\n",
      "epoch:    1 | step:  5199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8233 | current loss: 1.2027\n",
      "epoch:    1 | step:  5299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8236 | current loss: 1.2028\n",
      "epoch:    1 | step:  5399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8240 | current loss: 1.2029\n",
      "epoch:    1 | step:  5499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8238 | current loss: 1.2029\n",
      "epoch:    1 | step:  5599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8231 | current loss: 1.2028\n",
      "epoch:    1 | step:  5699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8224 | current loss: 1.2027\n",
      "epoch:    1 | step:  5799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8229 | current loss: 1.2028\n",
      "epoch:    1 | step:  5899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8230 | current loss: 1.2028\n",
      "epoch:    1 | step:  5999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8231 | current loss: 1.2028\n",
      "epoch:    1 | step:  6099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8234 | current loss: 1.2028\n",
      "epoch:    1 | step:  6199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8236 | current loss: 1.2029\n",
      "epoch:    1 | step:  6299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8239 | current loss: 1.2030\n",
      "epoch:    1 | step:  6399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8238 | current loss: 1.2029\n",
      "epoch:    1 | step:  6499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8242 | current loss: 1.2030\n",
      "epoch:    1 | step:  6599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8238 | current loss: 1.2030\n",
      "epoch:    1 | step:  6699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8237 | current loss: 1.2029\n",
      "epoch:    1 | step:  6799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8238 | current loss: 1.2030\n",
      "epoch:    1 | step:  6899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8237 | current loss: 1.2030\n",
      "epoch:    1 | step:  6999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8238 | current loss: 1.2030\n",
      "epoch:    1 | step:  7099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8240 | current loss: 1.2030\n",
      "epoch:    1 | step:  7199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8237 | current loss: 1.2029\n",
      "epoch:    1 | step:  7299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8241 | current loss: 1.2030\n",
      "Running test at epoch 1, checkpoint 1/20\n",
      "Checkpoint saved: save/models/model_d5_theta_idx4_epoch0001_step0001.pt\n",
      "epoch:   1 | checkpoint:   1 | d:   5 | theta_idx:   4 | train Sqc: 0.8244 | loss: 1.2031 | test Sqc: 0.7522 | test Neg: -0.0001 | test Sa: 0.3102\n",
      "epoch:    1 | step:  7499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8243 | current loss: 1.2034\n",
      "epoch:    1 | step:  7599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8298 | current loss: 1.2039\n",
      "epoch:    1 | step:  7699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8282 | current loss: 1.2033\n",
      "epoch:    1 | step:  7799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8235 | current loss: 1.2027\n",
      "epoch:    1 | step:  7899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8214 | current loss: 1.2024\n",
      "epoch:    1 | step:  7999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8200 | current loss: 1.2021\n",
      "epoch:    1 | step:  8099 |  d:    5 | theta_idx:    4 | current Sqc: 0.8213 | current loss: 1.2024\n",
      "epoch:    1 | step:  8199 |  d:    5 | theta_idx:    4 | current Sqc: 0.8195 | current loss: 1.2019\n",
      "epoch:    1 | step:  8299 |  d:    5 | theta_idx:    4 | current Sqc: 0.8184 | current loss: 1.2016\n",
      "epoch:    1 | step:  8399 |  d:    5 | theta_idx:    4 | current Sqc: 0.8187 | current loss: 1.2017\n",
      "epoch:    1 | step:  8499 |  d:    5 | theta_idx:    4 | current Sqc: 0.8186 | current loss: 1.2017\n",
      "epoch:    1 | step:  8599 |  d:    5 | theta_idx:    4 | current Sqc: 0.8187 | current loss: 1.2018\n",
      "epoch:    1 | step:  8699 |  d:    5 | theta_idx:    4 | current Sqc: 0.8184 | current loss: 1.2017\n",
      "epoch:    1 | step:  8799 |  d:    5 | theta_idx:    4 | current Sqc: 0.8191 | current loss: 1.2019\n",
      "epoch:    1 | step:  8899 |  d:    5 | theta_idx:    4 | current Sqc: 0.8188 | current loss: 1.2019\n",
      "epoch:    1 | step:  8999 |  d:    5 | theta_idx:    4 | current Sqc: 0.8201 | current loss: 1.2022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10643/3839693540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mrhoS_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrhoS_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0mprepseq_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprepseq_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepseq_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepseq_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                         \u001b[0mrhoC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepseq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, measure, mask)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True  # Test loop now runs automatically during checkpoint saves\n",
    "seed = 81\n",
    "test_size = 1*10**6\n",
    "N = 36\n",
    "#theta = torch.pi*torch.tensor([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5])[theta_idx].item()\n",
    "batch = 1000\n",
    "file = 'save'\n",
    "num_check = 20  # Number of checkpoints to save per epoch \n",
    "start_epoch = 1  # Epoch to start/resume from\n",
    "for d in [5]:\n",
    "    for theta_idx in [4]:\n",
    "        for train_size in [81*10**6]:\n",
    "            torch.manual_seed(seed)\n",
    "            mdl = LlamaPredictor(L_max=N,\n",
    "                                    d=d,\n",
    "                                    n_embd=96, \n",
    "                                    n_layer=36, \n",
    "                                    n_head=48,\n",
    "                                    vocab_size=3, \n",
    "                                    dropout_prob=0.0).to(device)\n",
    "            # mdl = LlamaPredictor(L_max=N,\n",
    "            #                         d=d,\n",
    "            #                         n_embd=12, \n",
    "            #                         n_layer=6, \n",
    "            #                         n_head=2,\n",
    "            #                         vocab_size=3, \n",
    "            #                         dropout_prob=0.0).to(device)\n",
    "            optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-4) # 0.0001\n",
    "            l_train = {'msk on Sqc':[], 'loss':[]} # record mean train metrics at checkpoint saves\n",
    "            l_test = {'msk off Sqc':[], 'msk off Neg':[],'msk off Sa':[]} # record mean test metrics at checkpoint saves\n",
    "            \n",
    "            # Temporary storage for accumulating values between checkpoints\n",
    "            temp_train = {'msk on Sqc':[], 'loss':[]}\n",
    "            temp_test_test = {'msk off Sqc':[], 'msk off Neg':[], 'msk off Sa':[]}\n",
    "            \n",
    "            # load train/test data\n",
    "            \n",
    "            prepseq_all = torch.load(f'data/theta{theta_idx}/all_prepseq_theta={theta_idx}.pt',weights_only=True)\n",
    "            shadow_all = torch.load(f'data/theta{theta_idx}/all_shadow_state_theta={theta_idx}.pt',weights_only=True)\n",
    "            rhoS_all = torch.load(f'data/theta{theta_idx}/all_rhoS_theta={theta_idx}.pt',weights_only=True)\n",
    "            \n",
    "            # Preprocess all prepseq data once (add 1 and append zero column)\n",
    "            prepseq_all = torch.cat([prepseq_all+1, torch.zeros(prepseq_all.shape[0], 1, dtype=prepseq_all.dtype)], -1)\n",
    "            \n",
    "            # Create non-overlapping train/test split with batching\n",
    "            train_data, test_data = create_train_test_split(\n",
    "                prepseq_all, shadow_all, rhoS_all, \n",
    "                train_size, test_size, batch\n",
    "            )\n",
    "            \n",
    "            # Extract batched data for convenience\n",
    "            prepseq_train = train_data['prepseq']\n",
    "            shadow_state_train = train_data['shadow_state']\n",
    "            rhoS_train = train_data['rhoS']\n",
    "            \n",
    "            prepseq_test = test_data['prepseq']\n",
    "            shadow_state_test = test_data['shadow_state']\n",
    "            rhoS_test = test_data['rhoS']\n",
    "\n",
    "\n",
    "            # Calculate checkpoint saving interval\n",
    "            total_batches = prepseq_train.shape[0]\n",
    "            save_interval = max(1, total_batches // num_check)  # Ensure at least 1\n",
    "            print(f'Will save checkpoints every {save_interval} batches ({num_check} times per epoch)')\n",
    "\n",
    "            # load checkpoint (resume from previous epoch's final checkpoint)\n",
    "            if start_epoch > 0:\n",
    "                # Load the final checkpoint from the previous epoch\n",
    "                prev_epoch = start_epoch - 1\n",
    "                final_checkpoint_num = (total_batches - 1) // save_interval  # Last checkpoint of previous epoch\n",
    "                checkpoint_info = load_checkpoint(mdl, optimizer, prev_epoch, final_checkpoint_num, \n",
    "                                                save_dir=f'{file}/models', \n",
    "                                                filename_prefix=f'model_d{d}_theta_idx{theta_idx}')\n",
    "                print(f\"Resumed from epoch {prev_epoch}, checkpoint {final_checkpoint_num}. Starting epoch {start_epoch}.\")\n",
    "\n",
    "            # Save baseline checkpoint for fresh training runs (ensures consistent starting point)\n",
    "            if start_epoch == 0:\n",
    "                print('Saving baseline checkpoint...')\n",
    "                save_checkpoint_and_test(mdl, optimizer, -1, 0,\n",
    "                                        temp_train, temp_test_test,\n",
    "                                        l_train, l_test, \n",
    "                                        prepseq_train, shadow_state_train, rhoS_train,\n",
    "                                        prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                        device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                        d, theta_idx, num_check)\n",
    "\n",
    "            for epoch in range(start_epoch, 10):\n",
    "                # Set manual seed for reproducible training (after checkpoint loading)\n",
    "                # This ensures consistent randomness whether starting fresh or resuming\n",
    "                torch.manual_seed(seed + epoch)  # Offset by start_epoch for consistency\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(seed + epoch)\n",
    "                    torch.cuda.manual_seed_all(seed + epoch)\n",
    "                    # Enable deterministic behavior for CUDA operations (may impact performance)\n",
    "                    torch.backends.cudnn.deterministic = True\n",
    "                    torch.backends.cudnn.benchmark = False\n",
    "                # train loop\n",
    "                if train:\n",
    "                    print('='*50+'   Train   '+'='*50)\n",
    "                    mdl.train()\n",
    "                    \n",
    "                    # Shuffle batch order once at the beginning of each epoch\n",
    "                    prepseq_train_shuffled, shadow_state_train_shuffled, rhoS_train_shuffled = shuffle(prepseq_train, shadow_state_train, rhoS_train)\n",
    "                    \n",
    "                    for i in range(prepseq_train.shape[0]):\n",
    "                        prepseq_batch, shadow_state_batch, rhoS_batch = prepseq_train_shuffled[i].clone(), shadow_state_train_shuffled[i].clone(), rhoS_train_shuffled[i].clone()\n",
    "                        prepseq_batch = prepseq_batch.to(device)\n",
    "                        shadow_state_batch = shadow_state_batch.to(device)\n",
    "                        rhoS_batch = rhoS_batch.to(device)\n",
    "                        rhoC = mdl(prepseq_batch, True)\n",
    "                        # Train\n",
    "                        optimizer.zero_grad()\n",
    "                        probs = torch.bmm(torch.bmm(shadow_state_batch.conj().unsqueeze(1), rhoC), shadow_state_batch.unsqueeze(-1)).view(-1).real\n",
    "                        loss = -probs.log().mean()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        temp_train['loss'].append(loss.item())\n",
    "                        #rhoC = eps(rhoC, 0.1)\n",
    "                        temp_train['msk on Sqc'].extend(bSqc(rhoS_batch, rhoC).tolist())\n",
    "                        \n",
    "                        # Save checkpoint and run test at regular intervals\n",
    "                        if (i+1) % save_interval == 0:\n",
    "                            checkpoint_num = (i+1) // save_interval - 1\n",
    "                            save_checkpoint_and_test(mdl, optimizer, epoch, checkpoint_num, \n",
    "                                                    temp_train, temp_test_test,\n",
    "                                                    l_train, l_test, \n",
    "                                                    prepseq_train, shadow_state_train, rhoS_train,\n",
    "                                                    prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                                    device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                                    d, theta_idx, num_check)\n",
    "                        \n",
    "                        if (i+1)%100 == 0 and temp_train['msk on Sqc'] and temp_train['loss']:\n",
    "                            trainS = torch.tensor(temp_train['msk on Sqc']).mean().item()\n",
    "                            loss_mean = torch.tensor(temp_train['loss']).mean().item()\n",
    "                            print('epoch:  %3d | step:  %3d |  d:  %3d | theta_idx:  %3d | current Sqc: %.4f | current loss: %.4f' %(epoch, i, d, theta_idx, trainS, loss_mean))\n",
    "                # Save final checkpoint at end of epoch (if not already saved)\n",
    "                if total_batches % save_interval != 0 and (temp_train['loss'] or temp_train['msk on Sqc']):\n",
    "                    final_checkpoint_num = total_batches // save_interval\n",
    "                    save_checkpoint_and_test(mdl, optimizer, epoch, final_checkpoint_num, \n",
    "                                            temp_train, temp_test_test,\n",
    "                                            l_train, l_test, \n",
    "                                            prepseq_train, shadow_state_train, rhoS_train,\n",
    "                                            prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                            device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                            d, theta_idx, num_check, is_final=True)\n",
    "                \n",
    "                torch.save(l_train, f'{file}/record/epoch={epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_train.pt')\n",
    "                torch.save(l_test, f'{file}/record/epoch={epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6f3f-3b85-45b1-ab26-4719c24c529c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
