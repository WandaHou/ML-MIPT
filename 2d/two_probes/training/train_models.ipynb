{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76011364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TFM import LlamaPredictor\n",
    "import torch\n",
    "from utils import torch_data, shuffle, blogm, bSqc, Neg, Sa, eps, create_train_test_split, save_checkpoint, load_checkpoint, save_checkpoint_and_test\n",
    "from math import prod\n",
    "\n",
    "dtype = torch.complex128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3965d48d-a8b3-4546-bd33-57961e265281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2333120"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LlamaPredictor(L_max=36,\n",
    "                     d=4,\n",
    "                     n_embd=96, \n",
    "                     n_layer=36, \n",
    "                     n_head=48,\n",
    "                     vocab_size=3, \n",
    "                     dropout_prob=0.0).to(device)\n",
    "paras = 0\n",
    "for p in mdl.parameters():\n",
    "    paras += prod(p.shape)\n",
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6f3f-3b85-45b1-ab26-4719c24c529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 81\n",
    "test_size = 1*10**6\n",
    "N = 36\n",
    "batch = 1000\n",
    "file = 'save'\n",
    "num_check = 20  # Number of checkpoints to save per epoch \n",
    "start_epoch = 0  # Epoch to start/resume from\n",
    "for d in [5]:\n",
    "    for theta_idx in [4]:\n",
    "        for train_size in [81*10**6]:\n",
    "            torch.manual_seed(seed)\n",
    "            mdl = LlamaPredictor(L_max=N,\n",
    "                                    d=d,\n",
    "                                    n_embd=96, \n",
    "                                    n_layer=36, \n",
    "                                    n_head=48,\n",
    "                                    vocab_size=3, \n",
    "                                    dropout_prob=0.0).to(device)\n",
    "            # mdl = LlamaPredictor(L_max=N,\n",
    "            #                         d=d,\n",
    "            #                         n_embd=12, \n",
    "            #                         n_layer=6, \n",
    "            #                         n_head=2,\n",
    "            #                         vocab_size=3, \n",
    "            #                         dropout_prob=0.0).to(device) # light model for quick testing\n",
    "            optimizer = torch.optim.Adam(mdl.parameters(), lr=1e-4) # 0.0001\n",
    "            l_train = {'msk on Sqc':[], 'loss':[]} # record mean train metrics at checkpoint saves\n",
    "            l_test = {'loss':[], 'msk off Sqc':[], 'msk off Neg':[],'msk off Sa':[]} # record mean test metrics at checkpoint saves\n",
    "            \n",
    "            # Temporary storage for accumulating values between checkpoints\n",
    "            temp_train = {'msk on Sqc':[], 'loss':[]}\n",
    "            temp_test_test = {'loss':[], 'msk off Sqc':[], 'msk off Neg':[], 'msk off Sa':[]}\n",
    "            \n",
    "            # load train/test data\n",
    "            prepseq_all = torch.load(f'data/theta{theta_idx}/all_prepseq_theta={theta_idx}.pt',weights_only=True)\n",
    "            shadow_all = torch.load(f'data/theta{theta_idx}/all_shadow_state_theta={theta_idx}.pt',weights_only=True)\n",
    "            rhoS_all = torch.load(f'data/theta{theta_idx}/all_rhoS_theta={theta_idx}.pt',weights_only=True)\n",
    "            \n",
    "            # Preprocess all prepseq data once (add 1 and append zero column)\n",
    "            prepseq_all = torch.cat([prepseq_all+1, torch.zeros(prepseq_all.shape[0], 1, dtype=prepseq_all.dtype)], -1)\n",
    "            \n",
    "            # Create non-overlapping train/test split with batching\n",
    "            train_data, test_data = create_train_test_split(\n",
    "                prepseq_all, shadow_all, rhoS_all, \n",
    "                train_size, test_size, batch\n",
    "            )\n",
    "            \n",
    "            # Extract batched data for convenience\n",
    "            prepseq_train = train_data['prepseq']\n",
    "            shadow_state_train = train_data['shadow_state']\n",
    "            rhoS_train = train_data['rhoS']\n",
    "            \n",
    "            prepseq_test = test_data['prepseq']\n",
    "            shadow_state_test = test_data['shadow_state']\n",
    "            rhoS_test = test_data['rhoS']\n",
    "\n",
    "\n",
    "            # Calculate checkpoint saving interval\n",
    "            total_batches = prepseq_train.shape[0]\n",
    "            save_interval = max(1, total_batches // num_check)  # Ensure at least 1\n",
    "            print(f'Will save checkpoints every {save_interval} batches ({num_check} times per epoch)')\n",
    "\n",
    "            # load checkpoint (resume from previous epoch's final checkpoint)\n",
    "            if start_epoch > 0:\n",
    "                # Load the final checkpoint from the previous epoch\n",
    "                prev_epoch = start_epoch - 1\n",
    "                final_checkpoint_num = (total_batches - 1) // save_interval  # Last checkpoint of previous epoch\n",
    "                checkpoint_info = load_checkpoint(mdl, optimizer, prev_epoch, final_checkpoint_num, \n",
    "                                                save_dir=f'{file}/models', \n",
    "                                                filename_prefix=f'model_d{d}_theta_idx{theta_idx}')\n",
    "                print(f\"Resumed from epoch {prev_epoch}, checkpoint {final_checkpoint_num}. Starting epoch {start_epoch}.\")\n",
    "                \n",
    "                # Load previous training and test records\n",
    "                try:\n",
    "                    l_train = torch.load(f'{file}/record/epoch={prev_epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_train.pt', weights_only=True)\n",
    "                    l_test = torch.load(f'{file}/record/epoch={prev_epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_test.pt', weights_only=True)\n",
    "                    print(f\"Loaded training records up to epoch {prev_epoch}. Train points: {len(l_train['loss'])}, Test points: {len(l_test['msk off Sqc'])}\")\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"Warning: Could not load previous records: {e}\")\n",
    "                    print(\"Starting with empty records.\")\n",
    "\n",
    "            # Save baseline checkpoint for fresh training runs (ensures consistent starting point)\n",
    "            if start_epoch == 0:\n",
    "                print('Saving baseline checkpoint...')\n",
    "                save_checkpoint_and_test(mdl, optimizer, -1, 0,\n",
    "                                        temp_train, temp_test_test,\n",
    "                                        l_train, l_test, \n",
    "                                        prepseq_train, shadow_state_train, rhoS_train,\n",
    "                                        prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                        device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                        d, theta_idx, num_check)\n",
    "            # Enable deterministic behavior for CUDA operations (may impact performance)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            for epoch in range(start_epoch, 10):\n",
    "                # Set manual seed for reproducible training (after checkpoint loading)\n",
    "                # This ensures consistent randomness whether starting fresh or resuming\n",
    "                torch.manual_seed(seed + epoch)  # Offset by start_epoch for consistency\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(seed + epoch)\n",
    "                    torch.cuda.manual_seed_all(seed + epoch)\n",
    "                # train loop\n",
    "                print('='*50+'   Train   '+'='*50)\n",
    "                mdl.train()\n",
    "                \n",
    "                # Shuffle individual samples (not batches) at the beginning of each epoch\n",
    "                # Flatten to individual samples\n",
    "                prepseq_flat = prepseq_train.view(-1, prepseq_train.shape[-1])\n",
    "                shadow_state_flat = shadow_state_train.view(-1, shadow_state_train.shape[-1])\n",
    "                rhoS_flat = rhoS_train.view(-1, rhoS_train.shape[-2], rhoS_train.shape[-1])\n",
    "                \n",
    "                # Use existing shuffle helper function\n",
    "                prepseq_shuffled, shadow_state_shuffled, rhoS_shuffled = shuffle(prepseq_flat, shadow_state_flat, rhoS_flat)\n",
    "                \n",
    "                # Re-batch the shuffled samples\n",
    "                prepseq_train_shuffled = prepseq_shuffled.view(prepseq_train.shape)\n",
    "                shadow_state_train_shuffled = shadow_state_shuffled.view(shadow_state_train.shape)\n",
    "                rhoS_train_shuffled = rhoS_shuffled.view(rhoS_train.shape)\n",
    "                \n",
    "                for i in range(prepseq_train.shape[0]):\n",
    "                    prepseq_batch, shadow_state_batch, rhoS_batch = prepseq_train_shuffled[i].clone(), shadow_state_train_shuffled[i].clone(), rhoS_train_shuffled[i].clone()\n",
    "                    prepseq_batch = prepseq_batch.to(device)\n",
    "                    shadow_state_batch = shadow_state_batch.to(device)\n",
    "                    rhoS_batch = rhoS_batch.to(device)\n",
    "                    rhoC = mdl(prepseq_batch, True)\n",
    "                    # Train\n",
    "                    optimizer.zero_grad()\n",
    "                    probs = torch.bmm(torch.bmm(shadow_state_batch.conj().unsqueeze(1), rhoC), shadow_state_batch.unsqueeze(-1)).view(-1).real\n",
    "                    loss = -probs.log().mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    temp_train['loss'].append(loss.item())\n",
    "                    temp_train['msk on Sqc'].extend(bSqc(rhoS_batch, rhoC).tolist())\n",
    "                    # Save checkpoint and run test at regular intervals\n",
    "                    if (i+1) % save_interval == 0:\n",
    "                        checkpoint_num = (i+1) // save_interval - 1\n",
    "                        save_checkpoint_and_test(mdl, optimizer, epoch, checkpoint_num, \n",
    "                                                temp_train, temp_test_test,\n",
    "                                                l_train, l_test, \n",
    "                                                prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                                device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                                d, theta_idx, num_check)\n",
    "                    \n",
    "                    if (i+1)%100 == 0 and temp_train['msk on Sqc'] and temp_train['loss']:\n",
    "                        trainS = torch.tensor(temp_train['msk on Sqc']).mean().item()\n",
    "                        loss_mean = torch.tensor(temp_train['loss']).mean().item()\n",
    "                        print('epoch:  %3d | step:  %3d |  d:  %3d | theta_idx:  %3d | current Sqc: %.4f | current loss: %.4f' %(epoch, i, d, theta_idx, trainS, loss_mean))\n",
    "                # Save final checkpoint at end of epoch (if not already saved)\n",
    "                if total_batches % save_interval != 0 and (temp_train['loss'] or temp_train['msk on Sqc']):\n",
    "                    final_checkpoint_num = total_batches // save_interval\n",
    "                    save_checkpoint_and_test(mdl, optimizer, epoch, final_checkpoint_num, \n",
    "                                            temp_train, temp_test_test,\n",
    "                                            l_train, l_test, \n",
    "                                            prepseq_test, shadow_state_test, rhoS_test,\n",
    "                                            device, f'{file}/models', f'model_d{d}_theta_idx{theta_idx}', \n",
    "                                            d, theta_idx, num_check, is_final=True)\n",
    "                \n",
    "                torch.save(l_train, f'{file}/record/epoch={epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_train.pt')\n",
    "                torch.save(l_test, f'{file}/record/epoch={epoch}_d={d}_theta_idx={theta_idx}_size{train_size}_test.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
